{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cba679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 03:19:10.947400: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-13 03:19:11.428755: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_dataset_shift\u001b[39m(data, graph, edges, sens_index,priv_group, unpriv_group):\n\u001b[1;32m     29\u001b[0m     dataset_types \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(data[i]\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/ipykernel/kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/ipykernel/kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.append(\"./subjects/\")\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random, math\n",
    "import tensorflow_probability as tfb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import  euclidean_distances\n",
    "from Utils_Functions import KLdivergence\n",
    "from sklearn.feature_selection import SelectKBest, SelectFpr,SelectPercentile \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from aif360.sklearn.metrics import equal_opportunity_difference,average_odds_difference\n",
    "from Utils_Functions import generate_dataset, eod\n",
    "import glob\n",
    "import re\n",
    "input()\n",
    "def generate_dataset_shift(data, graph, edges, sens_index,priv_group, unpriv_group):\n",
    "\n",
    "    dataset_types = [str(data[i].dtype) for i in data.columns]\n",
    "    succ_generated = 0\n",
    "    generation_coef = 1\n",
    "    graph_dic ={}\n",
    "    for i in graph.columns[1:]:\n",
    "        if np.where(graph[i])[0].shape[0]==0:\n",
    "            graph_dic[i]=None\n",
    "        else:\n",
    "            graph_dic[i]= graph['Unnamed: 0'][np.where(graph[i])[0]].values\n",
    "\n",
    "    final_df = pd.DataFrame(columns = data.columns) \n",
    "    trial = 0\n",
    "    not_interesting = False\n",
    "    while final_df.shape[0]<data.shape[0]:\n",
    "\n",
    "        if trial > 20:\n",
    "            not_interesting = True\n",
    "            return None , False\n",
    "        df_new_dic ={}\n",
    "        for edge in graph.sum().index[np.where(graph.sum()==0)[0]]:\n",
    "            df_new_dic[edge] = np.random.choice(np.unique(data[edge]), size = data.shape[0]*generation_coef) \n",
    "\n",
    "        statring_atts = graph.sum().index[np.where(graph.sum()==0)[0]]\n",
    "        \n",
    "        while statring_atts.shape[0] != graph['Unnamed: 0'].shape[0]:\n",
    "            \n",
    "            for att in graph_dic.keys():\n",
    "                if att not in statring_atts:\n",
    "                    if 0 in  [1 if graph_dic[att][i] in statring_atts else 0 for i in range(graph_dic[att].shape[0])]:\n",
    "\n",
    "                        continue\n",
    "                    else:\n",
    "                        edge_logits = 0\n",
    "                        \n",
    "                        for cause in graph_dic[att]:\n",
    "                            edge_logits += (edges[cause+att] * df_new_dic[cause])\n",
    "                        if np.unique(data[att]).shape[0]==2:\n",
    "                            df_new_dic[att] =  tfb.distributions.Bernoulli(logits=edge_logits + edges[att+'0'] ).sample().numpy()\n",
    "                        elif 'float' in dataset_types[np.where(data.columns==att)[0][0]]:\n",
    "\n",
    "                            df_new_dic[att] =  tfb.distributions.Normal(loc=(edge_logits+ edges[att+'0']), scale= edges['sigma_h']).sample().numpy()\n",
    "                        \n",
    "                        else:    \n",
    "                            df_new_dic[att] =  tfb.distributions.Poisson(rate=tf.exp(edge_logits+ edges[att+'0']) ).sample().numpy()  \n",
    "\n",
    "                        statring_atts = np.append(statring_atts,att) \n",
    "\n",
    "        new_df = pd.DataFrame(columns = data.columns)\n",
    "        for col in new_df.columns:\n",
    "            new_df[col] = df_new_dic[col]\n",
    "        \n",
    "        ind_inf = np.unique(np.where(new_df>data.max())[0])\n",
    "        new_df.drop(ind_inf,axis=0,inplace=True)\n",
    "        for col in range(new_df.columns.shape[0]):\n",
    "            new_df[new_df.columns[col]]=new_df[new_df.columns[col]].astype(dataset_types[col])\n",
    "        if new_df.shape[0]<1:\n",
    "            return None , False\n",
    "        X2 = new_df.to_numpy()[:,:-1]\n",
    "        Y2 = new_df.to_numpy()[:,-1]\n",
    "        #dist = euclidean_distances(X2, centroids)\n",
    "        #succ_generated += new_df.iloc[np.where((ave_dist>=dist).sum(1)>0)].shape[0]\n",
    "        final_df = pd.concat([final_df,new_df]).reset_index(drop=True)\n",
    "        final_df = final_df.drop_duplicates()\n",
    "\n",
    "\n",
    "            \n",
    "        trial += 1\n",
    "   \n",
    "    #final_df = final_df.astype(int)\n",
    "    Y2 = final_df.to_numpy()[:,-1]\n",
    "    if (Y2.sum()/Y2.shape[0]< 0.06) or (Y2.sum()/Y2.shape[0]> 0.95):\n",
    "        return None,  False\n",
    "    if priv_group not in final_df[final_df.columns[sens_index]].values or unpriv_group not in final_df[final_df.columns[sens_index]].values:\n",
    "        return None,  False\n",
    "\n",
    "    return final_df, True\n",
    "\n",
    "#dataset ='Bank'\n",
    "for dataset in ['Adult']:\n",
    "    if dataset == 'Adult':\n",
    "        sens_index = 7\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0\n",
    "        data_file_name = 'adult_org-Copy1.csv'\n",
    "        alg_list = ['ges','simy']\n",
    "        \n",
    "    if dataset == 'Compas':\n",
    "        sens_index = 1\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0\n",
    "        data_file_name = 'compas-Copy1'\n",
    "        alg_list = ['ges','pc']\n",
    "        \n",
    "    if dataset == 'Bank':\n",
    "        sens_index = 0\n",
    "        priv_group = 5\n",
    "        unpriv_group = 3\n",
    "        data_file_name = 'bank'\n",
    "        alg_list = ['ges']\n",
    "        \n",
    "    if dataset == 'Heart':\n",
    "        sens_index = 0\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0 \n",
    "        data_file_name = 'heart_processed_1'\n",
    "        alg_list = ['ges']\n",
    "        \n",
    "    if dataset == 'Law':\n",
    "        sens_index = 1\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0\n",
    "        data_file_name = 'law.csv'\n",
    "        alg_list = ['ges','simy']\n",
    "\n",
    "    if dataset == 'Student':\n",
    "        sens_index = 0\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0 \n",
    "        alg_list = ['simy','pc']\n",
    "        data_file_name = 'students-processed_2'\n",
    "        \n",
    "    df = pd.read_csv('./subjects/datasets/'+data_file_name)\n",
    "    df =  df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    X1 = df.to_numpy()[:,:-1]\n",
    "    Y1 = df.to_numpy()[:,-1].astype(int)\n",
    "\n",
    "    \n",
    "    for Algorithm in alg_list:\n",
    "        print('Algorithm', Algorithm)\n",
    "        for edge_list_filename in glob.glob('./'+dataset+'_Analysis/'+Algorithm+'/PP/*.csv'):\n",
    "            print(edge_list_filename)\n",
    "            file_num = int(re.findall(r'\\d+', edge_list_filename.split('/')[-1])[0])\n",
    "            RQ1_res = np.load('./'+dataset+'_Analysis/RQ1/'+dataset+'_'+Algorithm+'_RQ1_results.npy')\n",
    "            if RQ1_res[np.where(RQ1_res[:,1].astype(int)==file_num)[0]][0][2].astype(float)==0.0:\n",
    "                print('No',file_num)\n",
    "                continue\n",
    "            try:\n",
    "                graph_filename = './'+dataset+'_Analysis/'+Algorithm+'/DAGs/'+dataset+'_'+Algorithm+'_DAG_{file_num}.csv'.format(file_num=file_num)\n",
    "                graph = pd.read_csv(graph_filename)\n",
    "                #edge_list_filename = './'+dataset+'_Analysis/'+Algorithm+'/PP/'+dataset+'_'+Algorithm+'_pp_{file_num}.csv'.format(file_num=file_num)\n",
    "                edges_list = pd.read_csv(edge_list_filename)\n",
    "                if 'first_pf0' not in edges_list.columns and 'label0' not in edges_list.columns and 'G30' not in edges_list.columns and 'y0' not in edges_list.columns:\n",
    "                    continue \n",
    "                if dataset=='Bank' and Algorithm=='simy':\n",
    "                    graph.columns = [i.replace('1','') for i in graph.columns]\n",
    "                    graph[graph.columns[0]] = [i.replace('1','') for i in graph[graph.columns[0]]]\n",
    "                    edges_list.columns = [i.replace('1','') for i in edges_list.columns]\n",
    "                edges_list = edges_list[edges_list.columns[1:-1]]\n",
    "\n",
    "            except:\n",
    "                print('Not a DAG! ',file_num)\n",
    "                continue\n",
    "\n",
    "            X1_coef = edges_list.to_numpy()\n",
    "            KMean_coef = KMeans(n_clusters=10)\n",
    "            KMean_coef.fit(X1_coef)\n",
    "            \n",
    "            SelectKBest_final_EOD=[]\n",
    "            SelectFpr_final_EOD=[]\n",
    "            SelectPercentile_final_EOD=[]\n",
    "            None_model_final_EOD = []\n",
    "            SelectKBest_final_AOD=[]\n",
    "            SelectFpr_final_AOD=[]\n",
    "            SelectPercentile_final_AOD=[]\n",
    "            None_model_final_AOD = []\n",
    "            drop_final_EOD = []\n",
    "            drop_final_AOD = []\n",
    "\n",
    "            for i in range(KMean_coef.n_clusters):\n",
    "                #print('Coef ',i)\n",
    "                weights_ind = np.random.choice(np.where(KMean_coef.labels_==i)[0])\n",
    "                edges = edges_list.iloc[weights_ind]\n",
    "\n",
    "                if dataset=='Law':\n",
    "                    edges['first_pf0']+=1\n",
    "                elif dataset=='Bank':\n",
    "                    edges['label0']+=1\n",
    "                elif dataset=='Heart':\n",
    "                    edges['label0']+=1\n",
    "                elif dataset=='Student':\n",
    "                    edges['G30']+=1\n",
    "                else:\n",
    "                    edges['y0']+=1\n",
    "                #print(file_num,weights_ind)\n",
    "                SelectKBest_EOD=[]\n",
    "                SelectFpr_EOD=[]\n",
    "                SelectPercentile_EOD=[]\n",
    "                SelectKBest_AOD=[]\n",
    "                SelectFpr_AOD=[]\n",
    "                SelectPercentile_AOD=[]\n",
    "                drop_EOD = []\n",
    "                drop_AOD = []\n",
    "                None_model = []\n",
    "                for j in range(10):\n",
    "                    final_df, status = generate_dataset_shift(df, graph, edges,sens_index, priv_group, unpriv_group)\n",
    "\n",
    "                    if status==False:\n",
    "                        continue\n",
    "\n",
    "                    X2 = final_df.to_numpy()[:,:-1]\n",
    "                    Y2 = final_df.to_numpy()[:,-1].astype(int)\n",
    "                    A2 = X2[:,sens_index]\n",
    "\n",
    "                    if priv_group not in A2 or unpriv_group not in A2:\n",
    "                        print('No sens group')\n",
    "                        continue\n",
    "                    model = LogisticRegression()\n",
    "                    model.fit(X2,Y2)\n",
    "                    \n",
    "                    preds_None = model.predict(X2)\n",
    "                    acc_None = accuracy_score(Y2,preds_None)\n",
    "                    f1_None = f1_score(Y2,preds_None)\n",
    "                    AOD_None = round(average_odds_difference(Y2, preds_None,prot_attr=A2,priv_group=priv_group ),3)\n",
    "                    EOD_None = eod(Y2, preds_None,sens=A2, priv=priv_group, unpriv=unpriv_group)        \n",
    "                    None_model.append([EOD_None,acc_None,f1_None])\n",
    "                    \n",
    "\n",
    "                    #print('Success rate DAG', file_num, succ_rate)\n",
    "                    for transformer in ['SelectKBest', 'SelectFpr','SelectPercentile' ,'drop']:\n",
    "\n",
    "\n",
    "                        if transformer == 'SelectKBest':\n",
    "                            Kbest = SelectKBest( k=int(X2.shape[1]/2))\n",
    "                            X_new = Kbest.fit_transform(X2, Y2)\n",
    "                        elif transformer == 'SelectFpr':\n",
    "                            sfpr =  SelectFpr(alpha=0.01)\n",
    "                            X_new = sfpr.fit_transform(X2, Y2)\n",
    "\n",
    "                        elif transformer == 'SelectPercentile':\n",
    "                            percentile =  SelectPercentile(percentile=10)\n",
    "                            X_new = percentile.fit_transform(X2, Y2)\n",
    "                        elif transformer == 'drop':                        \n",
    "                            X_new =  np.delete(X2,sens_index,axis=1)\n",
    "\n",
    "\n",
    "                        model = LogisticRegression()\n",
    "                        model.fit(X_new,Y2)\n",
    "                        preds_new = model.predict(X_new)\n",
    "                        acc_temp = accuracy_score(Y2,preds_new)\n",
    "                        f1_temp = f1_score(Y2,preds_new)\n",
    "                        AOD = round(average_odds_difference(Y2, preds_new,prot_attr=A2,priv_group=priv_group ),3)\n",
    "                        EOD = eod(Y2, preds_new,sens=A2, priv=priv_group, unpriv=unpriv_group)\n",
    "                        acc_diff = round(acc_temp - acc_None,2)\n",
    "                        f1_diff = round(f1_temp- f1_None,2)\n",
    "                        EOD_diff = EOD - EOD_None\n",
    "                        AOD_diff = AOD - AOD_None\n",
    "                        EOD_diff_abs = abs(EOD) - abs(EOD_None)\n",
    "                        AOD_diff_abs = abs(AOD) - abs(AOD_None)\n",
    "\n",
    "                        if transformer == 'SelectKBest':\n",
    "                            SelectKBest_EOD.append([ EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                            SelectKBest_AOD.append([ AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "\n",
    "                        elif transformer == 'SelectFpr':\n",
    "\n",
    "                            SelectFpr_EOD.append([EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                            SelectFpr_AOD.append([AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "\n",
    "                        elif transformer == 'SelectPercentile':\n",
    "                            SelectPercentile_EOD.append([EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                            SelectPercentile_AOD.append([AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "\n",
    "                        elif transformer == 'drop':\n",
    "                            drop_EOD.append([EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                            drop_AOD.append([AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "                \n",
    "                if len(SelectKBest_EOD)>0:\n",
    "                    SelectKBest_final_EOD.append(np.concatenate([[file_num,weights_ind], np.mean(SelectKBest_EOD,axis=0)]))\n",
    "                if len(SelectKBest_AOD)>0:\n",
    "                    SelectKBest_final_AOD.append(np.concatenate([[file_num,weights_ind], np.mean(SelectKBest_AOD,axis=0)])) \n",
    "\n",
    "                if len(SelectFpr_EOD)>0:\n",
    "                    SelectFpr_final_EOD.append(np.concatenate([[file_num,weights_ind], np.mean(SelectFpr_EOD,axis=0)]))\n",
    "                if len(SelectFpr_AOD)>0:\n",
    "                    SelectFpr_final_AOD.append(np.concatenate([[file_num,weights_ind], np.mean(SelectFpr_AOD,axis=0)]))\n",
    "\n",
    "                if len(SelectPercentile_EOD)>0:    \n",
    "                    SelectPercentile_final_EOD.append(np.concatenate([[file_num,weights_ind], np.mean(SelectPercentile_EOD,axis=0)]))\n",
    "                if len(SelectPercentile_AOD)>0:    \n",
    "                    SelectPercentile_final_AOD.append(np.concatenate([[file_num,weights_ind], np.mean(SelectPercentile_AOD,axis=0)]))\n",
    "\n",
    "                if len(drop_EOD)>0:    \n",
    "                    drop_final_EOD.append(np.concatenate([[file_num,weights_ind], np.mean(drop_EOD,axis=0)]))\n",
    "                if len(drop_AOD)>0:    \n",
    "                    drop_final_AOD.append(np.concatenate([[file_num,weights_ind], np.mean(drop_AOD,axis=0)]))\n",
    "\n",
    "    #         print(np.array(SelectKBest_final_EOD).min(),np.array(SelectKBest_final_EOD).max())\n",
    "    #         print(np.array(SelectKBest_final_AOD).min(),np.array(SelectKBest_final_AOD).max())\n",
    "\n",
    "    #         print(np.array(SelectFpr_final_EOD).min(),np.array(SelectFpr_final_EOD).max())\n",
    "    #         print(np.array(SelectFpr_final_AOD).min(),np.array(SelectFpr_final_AOD).max())\n",
    "\n",
    "    #         print(np.array(SelectPercentile_final_EOD).min(),np.array(SelectPercentile_final_EOD).max())\n",
    "    #         print(np.array(SelectPercentile_final_AOD).min(),np.array(SelectPercentile_final_AOD).max())\n",
    "            np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_SelectKbest_EOD_' + str(file_num)+'.npy',SelectKBest_final_EOD)\n",
    "            np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_SelectKbest_AOD_' + str(file_num)+'.npy',SelectKBest_final_AOD)\n",
    "\n",
    "            np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_SelectFpr_EOD_' + str(file_num)+'.npy',SelectFpr_final_EOD)\n",
    "            np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_SelectFpr_AOD_' + str(file_num)+'.npy',SelectFpr_final_AOD)\n",
    "\n",
    "            np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_SelectPercentile_EOD_' + str(file_num)+'.npy',SelectPercentile_final_EOD)\n",
    "            np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_SelectPercentile_AOD_' + str(file_num)+'.npy',SelectPercentile_final_AOD)\n",
    "\n",
    "            np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_drop_EOD_' + str(file_num)+'.npy',drop_final_EOD)\n",
    "            np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_drop_AOD_' + str(file_num)+'.npy',drop_final_AOD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "549efad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfb.distributions.Bernoulli(logits=8 ).sample(1000).numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07884b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf73018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fded6cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f40643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f35e190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 03:14:21.531686: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'shape' with dtype int32 and shape [1]\n",
      "\t [[{{node shape}}]]\n",
      "2024-01-18 03:14:21.534978: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'shape' with dtype int32 and shape [1]\n",
      "\t [[{{node shape}}]]\n",
      "2024-01-18 03:14:21.552596: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat' with dtype int32 and shape [2]\n",
      "\t [[{{node poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat}}]]\n",
      "2024-01-18 03:14:21.552655: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat' with dtype int32 and shape [2]\n",
      "\t [[{{node poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat}}]]\n",
      "2024-01-18 03:14:21.555673: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat' with dtype int32 and shape [2]\n",
      "\t [[{{node poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat}}]]\n",
      "2024-01-18 03:14:21.555726: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat' with dtype int32 and shape [2]\n",
      "\t [[{{node poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat}}]]\n",
      "2024-01-18 03:14:21.578797: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'shape' with dtype int32 and shape [1]\n",
      "\t [[{{node shape}}]]\n",
      "2024-01-18 03:14:21.579494: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'shape' with dtype int32 and shape [1]\n",
      "\t [[{{node shape}}]]\n",
      "2024-01-18 03:14:21.580172: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'shape' with dtype int32 and shape [1]\n",
      "\t [[{{node shape}}]]\n",
      "2024-01-18 03:14:21.587102: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'poisson_noncpu/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat' with dtype int32 and shape [2]\n",
      "\t [[{{node poisson_noncpu/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat}}]]\n",
      "2024-01-18 03:14:21.587157: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'poisson_noncpu/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat' with dtype int32 and shape [2]\n",
      "\t [[{{node poisson_noncpu/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat}}]]\n",
      "2024-01-18 03:14:21.604435: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'shape' with dtype int32 and shape [1]\n",
      "\t [[{{node shape}}]]\n",
      "2024-01-18 03:14:21.606668: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'shape' with dtype int32 and shape [1]\n",
      "\t [[{{node shape}}]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 336\u001b[0m\n\u001b[1;32m    333\u001b[0m None_model \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m--> 336\u001b[0m     final_df, status \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_dataset_shift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\u001b[43msens_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriv_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpriv_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 86\u001b[0m, in \u001b[0;36mgenerate_dataset_shift\u001b[0;34m(data, graph, edges, sens_index, priv_group, unpriv_group)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m , \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m X2 \u001b[38;5;241m=\u001b[39m \u001b[43mnew_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     87\u001b[0m Y2 \u001b[38;5;241m=\u001b[39m new_df\u001b[38;5;241m.\u001b[39mto_numpy()[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#dist = euclidean_distances(X2, centroids)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m#succ_generated += new_df.iloc[np.where((ave_dist>=dist).sum(1)>0)].shape[0]\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/pandas/core/frame.py:1840\u001b[0m, in \u001b[0;36mDataFrame.to_numpy\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_numpy\u001b[39m(\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1782\u001b[0m     dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1783\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1784\u001b[0m     na_value: \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mno_default,\n\u001b[1;32m   1785\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;124;03m    Convert the DataFrame to a NumPy array.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1838\u001b[0m \u001b[38;5;124;03m           [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object)\u001b[39;00m\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1840\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1842\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(dtype)\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/pandas/core/generic.py:5980\u001b[0m, in \u001b[0;36mNDFrame._consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m():\n\u001b[1;32m   5978\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mconsolidate()\n\u001b[0;32m-> 5980\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_protect_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/pandas/core/generic.py:5968\u001b[0m, in \u001b[0;36mNDFrame._protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   5966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f()\n\u001b[1;32m   5967\u001b[0m blocks_before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mblocks)\n\u001b[0;32m-> 5968\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mblocks) \u001b[38;5;241m!=\u001b[39m blocks_before:\n\u001b[1;32m   5970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/pandas/core/generic.py:5978\u001b[0m, in \u001b[0;36mNDFrame._consolidate_inplace.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m():\n\u001b[0;32m-> 5978\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/pandas/core/internals/managers.py:686\u001b[0m, in \u001b[0;36mBaseBlockManager.consolidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    684\u001b[0m bm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs, verify_integrity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    685\u001b[0m bm\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m \u001b[43mbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bm\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/pandas/core/internals/managers.py:1871\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m   1870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1871\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1872\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1873\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs \u001b[38;5;241m=\u001b[39m _consolidate_with_refs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs)\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/pandas/core/internals/managers.py:2329\u001b[0m, in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   2327\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[0;32m-> 2329\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2332\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[1;32m   2333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/pandas/core/internals/managers.py:2388\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2387\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[0;32m-> 2388\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2389\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[1;32m   2391\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.append(\"./subjects/\")\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random, math\n",
    "import tensorflow_probability as tfb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import  euclidean_distances\n",
    "from Utils_Functions import KLdivergence\n",
    "from sklearn.feature_selection import SelectKBest, SelectFpr,SelectPercentile \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from aif360.sklearn.metrics import equal_opportunity_difference,average_odds_difference\n",
    "from Utils_Functions import generate_dataset, eod\n",
    "import glob\n",
    "import re\n",
    "\n",
    "def generate_dataset_shift(data, graph, edges, sens_index,priv_group, unpriv_group):\n",
    "\n",
    "    dataset_types = [str(data[i].dtype) for i in data.columns]\n",
    "    succ_generated = 0\n",
    "    generation_coef = 1\n",
    "    graph_dic ={}\n",
    "    for i in graph.columns[1:]:\n",
    "        if np.where(graph[i])[0].shape[0]==0:\n",
    "            graph_dic[i]=None\n",
    "        else:\n",
    "            graph_dic[i]= graph['Unnamed: 0'][np.where(graph[i])[0]].values\n",
    "\n",
    "    final_df = pd.DataFrame(columns = data.columns) \n",
    "    trial = 0\n",
    "    not_interesting = False\n",
    "    while final_df.shape[0]<data.shape[0]:\n",
    "\n",
    "        if trial > 20:\n",
    "            not_interesting = True\n",
    "            return None , False\n",
    "        df_new_dic ={}\n",
    "        for edge in graph.sum().index[np.where(graph.sum()==0)[0]]:\n",
    "            df_new_dic[edge] = np.random.choice(np.unique(data[edge]), size = data.shape[0]*generation_coef) \n",
    "\n",
    "        statring_atts = graph.sum().index[np.where(graph.sum()==0)[0]]\n",
    "        \n",
    "        while statring_atts.shape[0] != graph['Unnamed: 0'].shape[0]:\n",
    "            \n",
    "            for att in graph_dic.keys():\n",
    "                if att not in statring_atts:\n",
    "                    if 0 in  [1 if graph_dic[att][i] in statring_atts else 0 for i in range(graph_dic[att].shape[0])]:\n",
    "\n",
    "                        continue\n",
    "                    else:\n",
    "                        edge_logits = 0\n",
    "                        \n",
    "                        for cause in graph_dic[att]:\n",
    "                            edge_logits += (edges[cause+att] * df_new_dic[cause])\n",
    "                        if np.unique(data[att]).shape[0]==2:\n",
    "                            df_new_dic[att] =  tfb.distributions.Bernoulli(logits=edge_logits + edges[att+'0'] ).sample().numpy()\n",
    "                        elif 'float' in dataset_types[np.where(data.columns==att)[0][0]]:\n",
    "\n",
    "                            df_new_dic[att] =  tfb.distributions.Normal(loc=(edge_logits+ edges[att+'0']), scale= edges['sigma_h']).sample().numpy()\n",
    "                        \n",
    "                        else:    \n",
    "                            df_new_dic[att] =  tfb.distributions.Poisson(rate=tf.exp(edge_logits+ edges[att+'0']) ).sample().numpy()  \n",
    "\n",
    "                        statring_atts = np.append(statring_atts,att) \n",
    "\n",
    "        new_df = pd.DataFrame(columns = data.columns)\n",
    "        for col in new_df.columns:\n",
    "            new_df[col] = df_new_dic[col]\n",
    "        \n",
    "        ind_inf = np.unique(np.where(new_df>data.max())[0])\n",
    "        new_df.drop(ind_inf,axis=0,inplace=True)\n",
    "        for col in range(new_df.columns.shape[0]):\n",
    "            new_df[new_df.columns[col]]=new_df[new_df.columns[col]].astype(dataset_types[col])\n",
    "        if new_df.shape[0]<1:\n",
    "            return None , False\n",
    "        X2 = new_df.to_numpy()[:,:-1]\n",
    "        Y2 = new_df.to_numpy()[:,-1]\n",
    "        #dist = euclidean_distances(X2, centroids)\n",
    "        #succ_generated += new_df.iloc[np.where((ave_dist>=dist).sum(1)>0)].shape[0]\n",
    "        final_df = pd.concat([final_df,new_df]).reset_index(drop=True)\n",
    "        final_df = final_df.drop_duplicates()\n",
    "\n",
    "\n",
    "            \n",
    "        trial += 1\n",
    "   \n",
    "    #final_df = final_df.astype(int)\n",
    "    Y2 = final_df.to_numpy()[:,-1]\n",
    "    if (Y2.sum()/Y2.shape[0]< 0.06) or (Y2.sum()/Y2.shape[0]> 0.95):\n",
    "        return None,  False\n",
    "    if priv_group not in final_df[final_df.columns[sens_index]].values or unpriv_group not in final_df[final_df.columns[sens_index]].values:\n",
    "        return None,  False\n",
    "\n",
    "    return final_df, True\n",
    "\n",
    "#dataset ='Bank'\n",
    "for dataset in ['Compas']:\n",
    "    if dataset == 'Adult':\n",
    "        sens_index = 7\n",
    "        sensitive_name = 'gender'\n",
    "        priv_group = 1  #male\n",
    "        unpriv_group = 0#female\n",
    "        favorable_label  = 1.0\n",
    "        unfavorable_label = 0.0\n",
    "        data_file_name = 'adult_org-Copy1.csv'\n",
    "        graph_base_filename ='./Adult_Analysis/ges/DAGs/Adult_ges_DAG_1.csv'\n",
    "        edge_list_base_filename = './Adult_Analysis/ges/PP/Adult_ges_pp_1.csv'\n",
    "        alg_list = ['ges','simy']\n",
    "        hp_ind = 37\n",
    "    if dataset == 'Compas':\n",
    "        sens_index = 1\n",
    "        sensitive_name = 'race'\n",
    "        priv_group = 1  #male\n",
    "        unpriv_group = 0#female\n",
    "        favorable_label  = 1.0\n",
    "        unfavorable_label = 0.0\n",
    "        data_file_name = 'compas-Copy1'\n",
    "        graph_base_filename ='./Compas_Analysis/pc/DAGs/Compas_pc_DAG_13.csv'\n",
    "        edge_list_base_filename = './Compas_Analysis/pc/PP/Compas_pc_pp_13.csv'\n",
    "        alg_list = ['ges','pc']\n",
    "        hp_ind = 60\n",
    "    if dataset == 'Bank':\n",
    "        sens_index = 0\n",
    "        sensitive_name = 'age'\n",
    "        priv_group = 5  #male\n",
    "        unpriv_group = 3#female\n",
    "        favorable_label  = 1.0\n",
    "        unfavorable_label = 0.0\n",
    "        data_file_name = 'bank'\n",
    "        graph_base_filename ='./Bank_Analysis/ges/DAGs/Bank_ges_DAG_8.csv'\n",
    "        edge_list_base_filename = './Bank_Analysis/ges/PP/Bank_ges_pp_8.csv'\n",
    "        alg_list = ['ges']\n",
    "        hp_ind = 22\n",
    "    if dataset == 'Law':\n",
    "        sens_index = 0\n",
    "        sensitive_name = 'sex'\n",
    "        priv_group = 1  #male\n",
    "        unpriv_group = 0#female\n",
    "        favorable_label  = 1.0\n",
    "        unfavorable_label = 0.0\n",
    "        data_file_name = 'law.csv'\n",
    "        graph_base_filename ='./Law_Analysis/ges/DAGs/Law_ges_DAG_15.csv'\n",
    "        edge_list_base_filename = './Law_Analysis/ges/PP/Law_ges_pp_15.csv'\n",
    "        alg_list = ['ges','simy']\n",
    "        hp_ind = 22\n",
    "    if dataset == 'Student':\n",
    "        sens_index = 0\n",
    "        sensitive_name = 'sex'\n",
    "        priv_group = 1  #male\n",
    "        unpriv_group = 0#female\n",
    "        favorable_label  = 1.0\n",
    "        unfavorable_label = 0.0\n",
    "        data_file_name = 'students-processed_2'\n",
    "        graph_base_filename ='./Student_Analysis/pc/DAGs/Student_pc_DAG_1.csv'\n",
    "        edge_list_base_filename = './Student_Analysis/pc/PP/Student_pc_pp_1.csv'\n",
    "        alg_list = ['simy','pc']\n",
    "        hp_ind = 9\n",
    "    if dataset == 'Heart':\n",
    "        sens_index = 0\n",
    "        sensitive_name = 'sex'\n",
    "        priv_group = 1  #male\n",
    "        unpriv_group = 0#female\n",
    "        favorable_label  = 1.0\n",
    "        unfavorable_label = 0.0\n",
    "        data_file_name = 'heart_processed_1'\n",
    "        graph_base_filename ='./Heart_Analysis/ges/DAGs/Heart_ges_DAG_4.csv'\n",
    "        edge_list_base_filename = './Heart_Analysis/ges/PP/Heart_ges_pp_4.csv'\n",
    "        alg_list = ['ges']\n",
    "        hp_ind = 10\n",
    "        \n",
    "    df = pd.read_csv('./subjects/datasets/'+data_file_name)\n",
    "    df =  df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    X1 = df.to_numpy()[:,:-1]\n",
    "    Y1 = df.to_numpy()[:,-1].astype(int)\n",
    "    \n",
    "    drop_Eod_diff_abs_max = [0,0,0,0,0,0]\n",
    "    drop_Eod_diff_abs_min = [0,0,0,1,0,0]\n",
    "    drop_Eod_diff_max = [0,0,0,0,0,0]\n",
    "    drop_Eod_diff_min = [0,0,0,1,0,0]\n",
    "\n",
    "    SelectFpr_Eod_diff_abs_max = [0,0,0,0,0,0]\n",
    "    SelectFpr_Eod_diff_abs_min = [0,0,0,1,0,0]\n",
    "    SelectFpr_Eod_diff_max = [0,0,0,0,0,0]\n",
    "    SelectFpr_Eod_diff_min = [0,0,0,1,0,0]\n",
    "\n",
    "    SelectKbest_Eod_diff_abs_max = [0,0,0,0,0,0]\n",
    "    SelectKbest_Eod_diff_abs_min = [0,0,0,1,0,0]\n",
    "    SelectKbest_Eod_diff_max = [0,0,0,0,0,0]\n",
    "    SelectKbest_Eod_diff_min = [0,0,0,1,0,0]\n",
    "\n",
    "    SelectPercentile_Eod_diff_abs_max = [0,0,0,0,0,0]\n",
    "    SelectPercentile_Eod_diff_abs_min = [0,0,0,1,0,0]\n",
    "    SelectPercentile_Eod_diff_max = [0,0,0,0,0,0]\n",
    "    SelectPercentile_Eod_diff_min = [0,0,0,1,0,0]\n",
    "    for Algorithm in alg_list:\n",
    "\n",
    "        for filename in glob.glob('./'+dataset+'_Analysis/RQ2/'+Algorithm+'*.npy'):\n",
    "\n",
    "\n",
    "            file_num = int(re.findall(r'\\d+', filename.split('/')[-1])[0])\n",
    "\n",
    "\n",
    "    #         print(file_num)\n",
    "            res = np.load(filename)\n",
    "\n",
    "            if 'drop' in filename and 'EOD' in filename:\n",
    "\n",
    "                if res[:,5].max()> drop_Eod_diff_abs_max[2]:\n",
    "                    drop_Eod_diff_abs_max = [Algorithm,file_num,res[res[:,5].argmax(),1],res[:,5].max(), res[res[:,5].argmax(),6],res[res[:,5].argmax(),7] ]\n",
    "\n",
    "                if res[:,4].max()> drop_Eod_diff_max[2]:\n",
    "                    drop_Eod_diff_max = [Algorithm,file_num,res[res[:,4].argmax(),1],res[:,4].max() , res[res[:,4].argmax(),5],res[res[:,4].argmax(),7]]\n",
    "\n",
    "                if res[:,5].min()< drop_Eod_diff_abs_min[2]:\n",
    "                    drop_Eod_diff_abs_min = [Algorithm,file_num, res[res[:,5].argmin(),1],res[:,5].min(), res[res[:,5].argmin(),6],res[res[:,5].argmin(),7 ]]\n",
    "\n",
    "                if res[:,4].min()< drop_Eod_diff_min[2]:\n",
    "                    drop_Eod_diff_min = [Algorithm,file_num,res[res[:,4].argmin(),1],res[:,4].min(), res[res[:,4].argmin(),6],res[res[:,4].argmin(),7  ]  ]  \n",
    "\n",
    "            if 'SelectFpr' in filename and 'EOD' in filename:\n",
    "\n",
    "                if res[:,5].max()> SelectFpr_Eod_diff_abs_max[2]:\n",
    "                    SelectFpr_Eod_diff_abs_max = [Algorithm,file_num,res[res[:,5].argmax(),1],res[:,5].max() , res[res[:,5].argmax(),6],res[res[:,5].argmax(),7]]\n",
    "\n",
    "                if res[:,4].max()> SelectFpr_Eod_diff_max[2]:\n",
    "                    SelectFpr_Eod_diff_max = [Algorithm,file_num,res[res[:,4].argmax(),1],res[:,4].max() , res[res[:,4].argmax(),6],res[res[:,4].argmax(),7 ]]\n",
    "\n",
    "                if res[:,5].min()< SelectFpr_Eod_diff_abs_min[2]:\n",
    "                    SelectFpr_Eod_diff_abs_min = [Algorithm,file_num,res[res[:,5].argmin(),1],res[:,5].min(), res[res[:,5].argmin(),6],res[res[:,5].argmin(),7 ]]\n",
    "\n",
    "                if res[:,4].min()< SelectFpr_Eod_diff_min[2]:\n",
    "                    SelectFpr_Eod_diff_min = [Algorithm,file_num,res[res[:,4].argmin(),1],res[:,4].min() , res[res[:,4].argmin(),6],res[res[:,4].argmin(),7 ] ]   \n",
    "\n",
    "            if 'SelectKbest' in filename and 'EOD' in filename:\n",
    "\n",
    "                if res[:,5].max()> SelectKbest_Eod_diff_abs_max[2]:\n",
    "                    SelectKbest_Eod_diff_abs_max = [Algorithm,file_num,res[res[:,5].argmax(),1],res[:,5].max() , res[res[:,5].argmax(),6],res[res[:,5].argmax(),7]]\n",
    "\n",
    "                if res[:,4].max()> SelectKbest_Eod_diff_max[2]:\n",
    "                    SelectKbest_Eod_diff_max = [Algorithm,file_num,res[res[:,4].argmax(),1],res[:,4].max(), res[res[:,4].argmax(),6],res[res[:,4].argmax(),7  ]]\n",
    "\n",
    "                if res[:,5].min()< SelectKbest_Eod_diff_abs_min[2]:\n",
    "                    SelectKbest_Eod_diff_abs_min = [Algorithm,file_num,res[res[:,5].argmin(),1],res[:,5].min() , res[res[:,5].argmin(),6],res[res[:,5].argmin(),7]]\n",
    "\n",
    "                if res[:,4].min()< SelectKbest_Eod_diff_min[2]:\n",
    "                    SelectKbest_Eod_diff_min = [Algorithm,file_num,res[res[:,4].argmin(),1],res[:,4].min(), res[res[:,4].argmin(),6],res[res[:,4].argmin(),7 ]]    \n",
    "\n",
    "            if 'SelectPercentile' in filename and 'EOD' in filename:\n",
    "\n",
    "                if res[:,5].max()> SelectPercentile_Eod_diff_abs_max[2]:\n",
    "                    SelectPercentile_Eod_diff_abs_max = [Algorithm,file_num,res[res[:,5].argmax(),1],res[:,5].max(), res[res[:,5].argmax(),6],res[res[:,5].argmax(),7 ]]\n",
    "\n",
    "                if res[:,4].max()> SelectPercentile_Eod_diff_max[2]:\n",
    "                    SelectPercentile_Eod_diff_max = [Algorithm,file_num,res[res[:,4].argmax(),1],res[:,4].max(), res[res[:,4].argmax(),6],res[res[:,4].argmax(),7  ]]\n",
    "\n",
    "                if res[:,5].min()< SelectPercentile_Eod_diff_abs_min[2]:\n",
    "                    SelectPercentile_Eod_diff_abs_min = [Algorithm,file_num,res[res[:,5].argmin(),1],res[:,5].min() , res[res[:,5].argmin(),6],res[res[:,5].argmin(),7]]\n",
    "\n",
    "                if res[:,4].min()< SelectPercentile_Eod_diff_min[2]:\n",
    "                    SelectPercentile_Eod_diff_min = [Algorithm,file_num,res[res[:,4].argmin(),1],res[:,4].min(), res[res[:,4].argmin(),6],res[res[:,4].argmin(),7 ]]  \n",
    "    #Base dag\n",
    "    RQ_res =[] \n",
    "    for filename in glob.glob('./'+dataset+'_Analysis/RQ1/*.npy'):\n",
    "            RQ_res += np.load(filename).tolist()\n",
    "\n",
    "    max_ind = np.array(RQ_res)[:,2].astype(float).argmax()\n",
    "    dag_base_filename = './'+dataset+'_Analysis/'+RQ_res[max_ind][0]+'/DAGs/'+dataset+'_'+RQ_res[max_ind][0]+'_DAG_'+RQ_res[max_ind][1]+'.csv'\n",
    "\n",
    "    # drop\n",
    "    pert_alg_max  = drop_Eod_diff_max[0]\n",
    "    pert_file_num_max = int(drop_Eod_diff_max[1])\n",
    "    pert_weight_ind_max = int(drop_Eod_diff_max[2])\n",
    "    dag_pert_filname_max = './'+dataset+'_Analysis/'+pert_alg_max+'/DAGs/'+dataset+'_'+pert_alg_max+'_DAG_'+str(pert_file_num_max)+'.csv'\n",
    "\n",
    "\n",
    "    pert_alg_min  = drop_Eod_diff_min[0]\n",
    "    pert_file_num_min = int(drop_Eod_diff_min[1])\n",
    "    pert_weight_ind_min = int(drop_Eod_diff_min[2])\n",
    "    dag_pert_filname_min = './'+dataset+'_Analysis/'+pert_alg_min+'/DAGs/'+dataset+'_'+pert_alg_min+'_DAG_'+str(pert_file_num_min)+'.csv'\n",
    "\n",
    "\n",
    "\n",
    "    SelectKBest_final_EOD=[]\n",
    "    SelectFpr_final_EOD=[]\n",
    "    SelectPercentile_final_EOD=[]\n",
    "    None_model_final_EOD = []\n",
    "    SelectKBest_final_AOD=[]\n",
    "    SelectFpr_final_AOD=[]\n",
    "    SelectPercentile_final_AOD=[]\n",
    "    None_model_final_AOD = []\n",
    "    drop_final_EOD = []\n",
    "    drop_final_AOD = []\n",
    "    dag_space = [[dag_pert_filname_min,pert_file_num_min,pert_weight_ind_min,pert_alg_min],[dag_pert_filname_max,pert_file_num_max,pert_weight_ind_max,pert_alg_max]]\n",
    "    for dag in dag_space:\n",
    "        graph = pd.read_csv(dag[0])\n",
    "        edge_list_filename =  './'+dataset+'_Analysis/'+dag[3]+'/PP/'+dataset+'_'+dag[3]+'_pp_'+str(dag[1])+'.csv'\n",
    "        edges_list = pd.read_csv(edge_list_filename)\n",
    "        if dataset=='Bank' and Algorithm=='simy':\n",
    "            graph.columns = [i.replace('1','') for i in graph.columns]\n",
    "            graph[graph.columns[0]] = [i.replace('1','') for i in graph[graph.columns[0]]]\n",
    "            edges_list.columns = [i.replace('1','') for i in edges_list.columns]\n",
    "        \n",
    "        if dataset=='Bank':\n",
    "            pert_edge = [['label0' , 1]]\n",
    "        elif dataset=='Heart':\n",
    "            pert_edge = [['label0' , 1]]\n",
    "        elif dataset=='Student':\n",
    "            pert_edge = [['G30' , 1]]\n",
    "        else:\n",
    "            pert_edge = [['y0' , 1]]\n",
    "        for perturbation in pert_edge:#,['y0' , 2],['y0' , 3]]:\n",
    "            edges = edges_list[edges_list.columns[1:-1]].iloc[dag[2]]\n",
    "            edges[perturbation[0]]+=perturbation[1]\n",
    "            SelectKBest_EOD=[]\n",
    "            SelectFpr_EOD=[]\n",
    "            SelectPercentile_EOD=[]\n",
    "            SelectKBest_AOD=[]\n",
    "            SelectFpr_AOD=[]\n",
    "            SelectPercentile_AOD=[]\n",
    "            drop_EOD = []\n",
    "            drop_AOD = []\n",
    "            None_model = []\n",
    "\n",
    "            for j in range(10):\n",
    "                final_df, status = generate_dataset_shift(df, graph, edges,sens_index, priv_group, unpriv_group)\n",
    "\n",
    "                if status==False:\n",
    "                    continue\n",
    "\n",
    "                X2 = final_df.to_numpy()[:,:-1]\n",
    "                Y2 = final_df.to_numpy()[:,-1].astype(int)\n",
    "                A2 = X2[:,sens_index]\n",
    "\n",
    "                model = LogisticRegression()\n",
    "                model.fit(X2,Y2)\n",
    "\n",
    "                preds_None = model.predict(X2)\n",
    "                acc_None = accuracy_score(Y2,preds_None)\n",
    "                f1_None = f1_score(Y2,preds_None)\n",
    "                AOD_None = round(average_odds_difference(Y2, preds_None,prot_attr=A2,priv_group=priv_group ),3)\n",
    "                EOD_None = eod(Y2, preds_None,sens=A2, priv=priv_group, unpriv=unpriv_group)        \n",
    "                None_model.append([EOD_None,acc_None,f1_None])\n",
    "\n",
    "                #print('Success rate DAG', file_num, succ_rate)\n",
    "                for transformer in ['SelectKBest', 'SelectFpr','SelectPercentile' ,'drop']:\n",
    "\n",
    "                    if transformer == 'SelectKBest':\n",
    "                        Kbest = SelectKBest(k=int(X2.shape[1]/2))\n",
    "                        X_new = Kbest.fit_transform(X2, Y2)\n",
    "                    elif transformer == 'SelectFpr':\n",
    "                        sfpr =  SelectFpr(alpha=0.01)\n",
    "                        X_new = sfpr.fit_transform(X2, Y2)\n",
    "\n",
    "                    elif transformer == 'SelectPercentile':\n",
    "                        percentile =  SelectPercentile(percentile=10)\n",
    "                        X_new = percentile.fit_transform(X2, Y2)\n",
    "                    elif transformer == 'drop':                        \n",
    "                        X_new =  np.delete(X2,sens_index,axis=1)\n",
    "\n",
    "\n",
    "                    model = LogisticRegression()\n",
    "                    model.fit(X_new,Y2)\n",
    "                    preds_new = model.predict(X_new)\n",
    "                    acc_temp = accuracy_score(Y2,preds_new)\n",
    "                    f1_temp = f1_score(Y2,preds_new)\n",
    "                    AOD = round(average_odds_difference(Y2, preds_new,prot_attr=A2,priv_group=priv_group ),3)\n",
    "                    EOD = eod(Y2, preds_new,sens=A2, priv=priv_group, unpriv=unpriv_group)\n",
    "                    acc_diff = round(acc_temp - acc_None,2)\n",
    "                    f1_diff = round(f1_temp- f1_None,2)\n",
    "                    EOD_diff = EOD - EOD_None\n",
    "                    AOD_diff = AOD - AOD_None\n",
    "                    EOD_diff_abs = abs(EOD) - abs(EOD_None)\n",
    "                    AOD_diff_abs = abs(AOD) - abs(AOD_None)\n",
    "\n",
    "                    if transformer == 'SelectKBest':\n",
    "                        SelectKBest_EOD.append([ EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                        SelectKBest_AOD.append([ AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "\n",
    "                    elif transformer == 'SelectFpr':\n",
    "                        SelectFpr_EOD.append([EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                        SelectFpr_AOD.append([AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "\n",
    "                    elif transformer == 'SelectPercentile':\n",
    "                        SelectPercentile_EOD.append([EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                        SelectPercentile_AOD.append([AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "\n",
    "                    elif transformer == 'drop':\n",
    "                        drop_EOD.append([EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                        drop_AOD.append([AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "\n",
    "            if len(SelectKBest_EOD)>0:\n",
    "                SelectKBest_final_EOD.append(np.concatenate([[perturbation[0],str(perturbation[1])], np.mean(SelectKBest_EOD,axis=0)]))\n",
    "            if len(SelectKBest_AOD)>0:\n",
    "                SelectKBest_final_AOD.append(np.concatenate([[perturbation[0],str(perturbation[1])], np.mean(SelectKBest_AOD,axis=0)])) \n",
    "\n",
    "            if len(SelectFpr_EOD)>0:\n",
    "                SelectFpr_final_EOD.append(np.concatenate([[perturbation[0],str(perturbation[1])], np.mean(SelectFpr_EOD,axis=0)]))\n",
    "            if len(SelectFpr_AOD)>0:\n",
    "                SelectFpr_final_AOD.append(np.concatenate([[perturbation[0],str(perturbation[1])], np.mean(SelectFpr_AOD,axis=0)]))\n",
    "\n",
    "            if len(SelectPercentile_EOD)>0:    \n",
    "                SelectPercentile_final_EOD.append(np.concatenate([[perturbation[0],str(perturbation[1])], np.mean(SelectPercentile_EOD,axis=0)]))\n",
    "            if len(SelectPercentile_AOD)>0:    \n",
    "                SelectPercentile_final_AOD.append(np.concatenate([[perturbation[0],str(perturbation[1])], np.mean(SelectPercentile_AOD,axis=0)]))\n",
    "\n",
    "            if len(drop_EOD)>0:    \n",
    "                drop_final_EOD.append(np.concatenate([[perturbation[0],str(perturbation[1])], np.mean(drop_EOD,axis=0)]))\n",
    "            if len(drop_AOD)>0:    \n",
    "                drop_final_AOD.append(np.concatenate([[perturbation[0],str(perturbation[1])], np.mean(drop_AOD,axis=0)]))\n",
    "\n",
    "\n",
    "    #     np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_SelectKbest_EOD_' + str(file_num)+'.npy',SelectKBest_final_EOD)\n",
    "    #     np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_SelectKbest_AOD_' + str(file_num)+'.npy',SelectKBest_final_AOD)\n",
    "\n",
    "    #     np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_SelectFpr_EOD_' + str(file_num)+'.npy',SelectFpr_final_EOD)\n",
    "    #     np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_SelectFpr_AOD_' + str(file_num)+'.npy',SelectFpr_final_AOD)\n",
    "\n",
    "    #     np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_SelectPercentile_EOD_' + str(file_num)+'.npy',SelectPercentile_final_EOD)\n",
    "    #     np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_SelectPercentile_AOD_' + str(file_num)+'.npy',SelectPercentile_final_AOD)\n",
    "\n",
    "    #     np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_drop_EOD_' + str(file_num)+'.npy',drop_final_EOD)\n",
    "    #     np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_drop_AOD_' + str(file_num)+'.npy',drop_final_AOD)\n",
    "    print(dataset)\n",
    "    print('Drop',round(float(drop_final_EOD[0][4]),3),round(float(drop_final_EOD[1][4]),3))\n",
    "    print('SelectKBest',round(float(SelectKBest_final_EOD[0][4]),3),round(float(SelectKBest_final_EOD[1][4]),3))\n",
    "    print('SelectFpr',round(float(SelectFpr_final_EOD[0][4]),3),round(float(SelectFpr_final_EOD[1][4]),3))\n",
    "    print('SelectPercentile',round(float(SelectPercentile_final_EOD[0][4]),3),round(float(SelectPercentile_final_EOD[1][4]),3))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80bcc842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>p</th>\n",
       "      <th>j</th>\n",
       "      <th>d</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sex</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>race</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>y</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  sex  race  age  p  j  d  y\n",
       "0        sex    0     0    0  0  0  0  0\n",
       "1       race    1     0    0  1  1  0  1\n",
       "2        age    0     0    0  1  1  0  1\n",
       "3          p    1     0    0  0  1  0  0\n",
       "4          j    0     0    0  0  0  0  0\n",
       "5          d    1     0    0  1  1  0  0\n",
       "6          y    0     0    0  1  0  1  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c4668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
