{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c555ec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm pc\n",
      "\n",
      "\n",
      "Failed to import TensorFlow. Please note that TensorFlow is not installed by default when you install TensorFlow Probability. This is so that users can decide whether to install the GPU-enabled TensorFlow package. To use TensorFlow Probability, please install the most recent version of TensorFlow, by following instructions at https://tensorflow.org/install.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 202\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;66;03m#ave_dist.append(euclidean_distances(X1[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).max())\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m final_df, succ_rate \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mave_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKMean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_centers_\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43msens_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriv_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpriv_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m succ_rate \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Major_Revision\\Fairness_Practices_Robustness_Testing\\subjects\\Utils_Functions.py:98\u001b[0m, in \u001b[0;36mgenerate_dataset\u001b[1;34m(data, graph, edges, ave_dist, centroids, sens_index, priv_group, unpriv_group)\u001b[0m\n\u001b[0;32m     95\u001b[0m                     df_new_dic[att] \u001b[38;5;241m=\u001b[39m  tfb\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(loc\u001b[38;5;241m=\u001b[39m(edge_logits\u001b[38;5;241m+\u001b[39m edges[att\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m]), scale\u001b[38;5;241m=\u001b[39m edges[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma_h\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     97\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:    \n\u001b[1;32m---> 98\u001b[0m                     df_new_dic[att] \u001b[38;5;241m=\u001b[39m  \u001b[43mtfb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPoisson\u001b[49m(rate\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mexp(edge_logits\u001b[38;5;241m+\u001b[39m edges[att\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m]) )\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mnumpy()  \n\u001b[0;32m    100\u001b[0m                 statring_atts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(statring_atts,att) \n\u001b[0;32m    102\u001b[0m new_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py:56\u001b[0m, in \u001b[0;36mLazyLoader.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[1;32m---> 56\u001b[0m   module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, item)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py:40\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_first_access):\n\u001b[1;32m---> 40\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_first_access\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_first_access \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:38\u001b[0m, in \u001b[0;36m_validate_tf_environment\u001b[1;34m(package)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check TF version and (depending on package) warn about TensorFloat32.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    inadequate.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m   \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m):\n\u001b[0;32m     40\u001b[0m   \u001b[38;5;66;03m# Print more informative error message, then reraise.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to import TensorFlow. Please note that TensorFlow is not \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstalled by default when you install TensorFlow Probability. This \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis so that users can decide whether to install the GPU-enabled \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensorFlow package. To use TensorFlow Probability, please install \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe most recent version of TensorFlow, by following instructions at \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://tensorflow.org/install.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.append(\"./subjects/\")\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random, math\n",
    "import tensorflow_probability as tfb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import  euclidean_distances\n",
    "import glob\n",
    "import re\n",
    "from sklearn.neighbors import KDTree\n",
    "from subjects.Utils_Functions import generate_dataset, eod\n",
    "import mahalanobis\n",
    "def generate_dataset_I(data, graph, edges, ave_dist, centroids,sens_index, priv_group, unpriv_group):\n",
    "\n",
    "    dataset_types = [str(data[i].dtype) for i in data.columns]\n",
    "    succ_generated = 0\n",
    "    generation_coef = 10\n",
    "    graph_dic ={}\n",
    "    for i in graph.columns[1:]:\n",
    "        if np.where(graph[i])[0].shape[0]==0:\n",
    "            graph_dic[i]=None\n",
    "        else:\n",
    "            graph_dic[i]= graph['Unnamed: 0'][np.where(graph[i])[0]].values\n",
    "\n",
    "    final_df = pd.DataFrame(columns = data.columns) \n",
    "    trial = 0\n",
    "    not_interesting = False\n",
    "    while final_df.shape[0]<data.shape[0]:\n",
    "\n",
    "        if trial > 20:\n",
    "            not_interesting = True\n",
    "            return None , 0.0#succ_generated/( trial *  data.shape[0]*generation_coef) \n",
    "        df_new_dic ={}\n",
    "        for edge in graph.sum().index[np.where(graph.sum()==0)[0]]:\n",
    "            df_new_dic[edge] = np.random.choice(np.unique(data[edge]), size = data.shape[0]*generation_coef) \n",
    "\n",
    "        statring_atts = graph.sum().index[np.where(graph.sum()==0)[0]]\n",
    "        \n",
    "        while statring_atts.shape[0] != graph['Unnamed: 0'].shape[0]:\n",
    "            \n",
    "            for att in graph_dic.keys():\n",
    "                if att not in statring_atts:\n",
    "                    if 0 in  [1 if graph_dic[att][i] in statring_atts else 0 for i in range(graph_dic[att].shape[0])]:\n",
    "\n",
    "                        continue\n",
    "                    else:\n",
    "                        edge_logits = 0\n",
    "                        \n",
    "                        for cause in graph_dic[att]:\n",
    "                            edge_logits += (edges[cause+att] * df_new_dic[cause])\n",
    "                        if np.unique(data[att]).shape[0]==2:\n",
    "                            df_new_dic[att] =  tfb.distributions.Bernoulli(logits=edge_logits + edges[att+'0'] ).sample().numpy()\n",
    "                        elif 'float' in dataset_types[np.where(data.columns==att)[0][0]]:\n",
    "\n",
    "                            df_new_dic[att] =  tfb.distributions.Normal(loc=(edge_logits+ edges[att+'0']), scale= edges['sigma_h']).sample().numpy()\n",
    "                        \n",
    "                        else:    \n",
    "                            df_new_dic[att] =  tfb.distributions.Poisson(rate=tf.exp(edge_logits+ edges[att+'0']) ).sample().numpy()  \n",
    "\n",
    "                        statring_atts = np.append(statring_atts,att) \n",
    "\n",
    "        new_df = pd.DataFrame(columns = data.columns)\n",
    "        for col in new_df.columns:\n",
    "            new_df[col] = df_new_dic[col]\n",
    "        \n",
    "        ind_inf = np.unique(np.where(new_df>data.max())[0])\n",
    "        new_df.drop(ind_inf,axis=0,inplace=True)\n",
    "        for col in range(new_df.columns.shape[0]):\n",
    "            new_df[new_df.columns[col]]=new_df[new_df.columns[col]].astype(dataset_types[col])\n",
    "        if new_df.shape[0]<1:\n",
    "            return None , 0.0\n",
    "        X2 = new_df.to_numpy()[:,:-1]\n",
    "        Y2 = new_df.to_numpy()[:,-1]\n",
    "        dist = euclidean_distances(X2, centroids)\n",
    "        succ_generated += new_df.iloc[np.where((ave_dist>=dist).sum(1)>0)].shape[0]\n",
    "        final_df = pd.concat([final_df,new_df.iloc[np.where((ave_dist>=dist).sum(1)>0)[0]]]).reset_index(drop=True)\n",
    "        final_df = final_df.drop_duplicates()\n",
    "\n",
    "        #print(succ_generated,trial)\n",
    "        if succ_generated<10:\n",
    "            return None, 0.0\n",
    "            \n",
    "        trial += 1\n",
    "   \n",
    "    #final_df = final_df.astype(int)\n",
    "    succ_rate = succ_generated/( trial *  data.shape[0]*generation_coef) \n",
    "    final_df = final_df.sample(n= data.shape[0])\n",
    "    Y2 = final_df.to_numpy()[:,-1]\n",
    "    if (Y2.sum()/Y2.shape[0]< 0.06) or (Y2.sum()/Y2.shape[0]> 0.95):\n",
    "        return None, 0.0#succ_generated/( trial *  data.shape[0]*generation_coef)   \n",
    "    if priv_group not in final_df[final_df.columns[sens_index]].values or unpriv_group not in final_df[final_df.columns[sens_index]].values:\n",
    "        return None, 0.0\n",
    "\n",
    "    return final_df, succ_rate\n",
    "dataset ='Bank'\n",
    "\n",
    "if dataset == 'Adult':\n",
    "    sens_index = 7\n",
    "    priv_group = 1\n",
    "    unpriv_group = 0\n",
    "    data_file_name = 'adult_org-Copy1.csv'\n",
    "if dataset == 'Compas':\n",
    "    sens_index = 1\n",
    "    priv_group = 1\n",
    "    unpriv_group = 0\n",
    "    data_file_name = 'compas-Copy1'\n",
    "if dataset == 'Bank':\n",
    "    sens_index = 0\n",
    "    priv_group = 5\n",
    "    unpriv_group = 3\n",
    "    data_file_name = 'bank'\n",
    "if dataset == 'Heart':\n",
    "    sens_index = 0\n",
    "    priv_group = 1\n",
    "    unpriv_group = 0 \n",
    "    data_file_name = 'heart_processed_1'\n",
    "if dataset == 'Law':\n",
    "    sens_index = 1\n",
    "    priv_group = 1\n",
    "    unpriv_group = 0\n",
    "    data_file_name = 'law.csv'\n",
    "\n",
    "if dataset == 'Student':\n",
    "    sens_index = 0\n",
    "    priv_group = 1\n",
    "    unpriv_group = 0  \n",
    "    data_file_name = 'students-processed_2'\n",
    "df = pd.read_csv('./subjects/datasets/'+data_file_name)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "X1 = df.to_numpy()[:,:-1]\n",
    "Y1 = df.to_numpy()[:,-1]\n",
    "mahND = mahalanobis.MahalanobisND(X1,10 )\n",
    "num_cluster = 100\n",
    "try :\n",
    "    with open('./'+dataset+'_Analysis/Kmean/KMean_{clus}.pkl'.format(clus=num_cluster), 'rb') as f:\n",
    "        KMean = pickle.load(f)\n",
    "except:\n",
    "    KMean = KMeans(n_clusters=num_cluster)\n",
    "    KMean.fit(X1)\n",
    "    with open('./'+dataset+'_Analysis/Kmean/KMean_{clus}.pkl'.format(clus=num_cluster),'wb') as f:\n",
    "        pickle.dump(KMean,f)\n",
    "\n",
    "ave_dist =[] \n",
    "for i in range(KMean.n_clusters):\n",
    "    mean_dist = euclidean_distances(X1[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).mean()\n",
    "    std_dist = euclidean_distances(X1[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).std()\n",
    "    if dataset == 'Heart':\n",
    "        ave_dist.append(mean_dist+ (3 * std_dist))\n",
    "    elif dataset == 'Compas':\n",
    "        ave_dist.append(mean_dist+ (2 * std_dist))\n",
    "    elif dataset == 'Student':\n",
    "        ave_dist.append(mean_dist+ (3 * std_dist))\n",
    "    elif dataset == 'Bank':\n",
    "        ave_dist.append(mean_dist+ (3 * std_dist))\n",
    "    else:\n",
    "        ave_dist.append(mean_dist+ (2 * std_dist))\n",
    "    \n",
    "for Algorithm in ['pc','ges','simy']:\n",
    "    print('Algorithm', Algorithm)\n",
    "    succ_rate_list =[]\n",
    "    metrics_temp=[]\n",
    "    for edge_list_filename in glob.glob('./'+dataset+'_Analysis/'+Algorithm+'/PP/*.csv'):\n",
    "#         print(edge_list_filename)\n",
    "        file_num = int(re.findall(r'\\d+', edge_list_filename)[0])\n",
    "        \n",
    "    #     if file_num in [5,7,10,12]:\n",
    "    #         continue\n",
    "\n",
    "        try:\n",
    "            graph_filename = './'+dataset+'_Analysis/'+Algorithm+'/DAGs/'+dataset+'_'+Algorithm+'_DAG_{file_num}.csv'.format(file_num=file_num)\n",
    "            graph = pd.read_csv(graph_filename)\n",
    "            \n",
    "\n",
    "            #edge_list_filename = './'+dataset+'_Analysis/'+Algorithm+'/PP/'+dataset+'_'+Algorithm+'_pp_{file_num}.csv'.format(file_num=file_num)\n",
    "            edges_list = pd.read_csv(edge_list_filename)\n",
    "\n",
    "            if dataset=='Bank' and Algorithm=='simy':\n",
    "                graph.columns = [i.replace('1','') for i in graph.columns]\n",
    "                graph[graph.columns[0]] = [i.replace('1','') for i in graph[graph.columns[0]]]\n",
    "                edges_list.columns = [i.replace('1','') for i in edges_list.columns]\n",
    "            edges_list = edges_list[edges_list.columns[1:-1]].mean()\n",
    "\n",
    "        except:\n",
    "            print('Not a DAG! ',file_num)\n",
    "            continue\n",
    "        \n",
    "\n",
    "  \n",
    "            #ave_dist.append(euclidean_distances(X1[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).max())\n",
    "        \n",
    "\n",
    "        final_df, succ_rate = generate_dataset(df, graph, edges_list, ave_dist, KMean.cluster_centers_ ,sens_index, priv_group, unpriv_group)\n",
    "\n",
    "\n",
    "        if succ_rate == 0.0:\n",
    "            continue\n",
    "\n",
    "\n",
    "#             metrics_temp.append(np.array([Algorithm,file_num,round(succ_rate,3),distance_avg]))\n",
    "#             print('Success rate DAG ', file_num,'-> Succ = ', round(succ_rate,3),' Dis avg= ', distance_avg,' Dis std= ', distance_std)\n",
    "        else:\n",
    "            X_org = df.to_numpy()[:,:-1]\n",
    "            X_gen = final_df.to_numpy()[:,:-1]\n",
    "            tree = KDTree(X_org)\n",
    "            distance_list = tree.query(X_gen, k=1)[0]\n",
    "            distance_avg = round(distance_list.mean(),3)\n",
    "            mah_dist  = mahND.calc_distances(final_df.to_numpy().astype(float)[:,:-1]).mean()\n",
    "        succ_rate_list.append(np.array([Algorithm,file_num,succ_rate,distance_avg, mah_dist]))\n",
    "    \n",
    "    print(f'{dataset}-{Algorithm} -> ${round(np.mean(np.array(succ_rate_list)[:,2:].astype(float), axis=0)[0],2)}$ & ${round(np.std(np.array(succ_rate_list)[:,2:].astype(float), axis=0)[0],2)}$ & ${round(np.min(np.array(succ_rate_list)[:,2:].astype(float), axis=0)[0],2)}$ & ${round(np.max(np.array(succ_rate_list)[:,2:].astype(float), axis=0)[0],2)}$ & ${round(np.mean(np.array(succ_rate_list)[:,2:].astype(float), axis=0)[1],1)}$&${round(np.mean(np.array(succ_rate_list)[:,-1].astype(float), axis=0),1)}$')\n",
    "    np.save('./'+dataset+'_Analysis/RQ1/'+dataset+'_'+Algorithm+'_RQ1_results_mahdist.npy',metrics_temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b2edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subjects.Utils_Functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
