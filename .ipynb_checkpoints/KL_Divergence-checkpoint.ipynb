{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe1bf263",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm pc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 21:28:41.963134: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'shape' with dtype int32 and shape [1]\n",
      "\t [[{{node shape}}]]\n",
      "2024-01-25 21:28:41.966508: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'shape' with dtype int32 and shape [1]\n",
      "\t [[{{node shape}}]]\n",
      "2024-01-25 21:28:41.983768: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat' with dtype int32 and shape [2]\n",
      "\t [[{{node poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat}}]]\n",
      "2024-01-25 21:28:41.983823: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat' with dtype int32 and shape [2]\n",
      "\t [[{{node poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat}}]]\n",
      "2024-01-25 21:28:41.986892: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat' with dtype int32 and shape [2]\n",
      "\t [[{{node poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat}}]]\n",
      "2024-01-25 21:28:41.986942: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat' with dtype int32 and shape [2]\n",
      "\t [[{{node poisson_noncpu/batched_las_vegas_algorithm/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat}}]]\n",
      "2024-01-25 21:28:42.010656: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'shape' with dtype int32 and shape [1]\n",
      "\t [[{{node shape}}]]\n",
      "2024-01-25 21:28:42.011361: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'shape' with dtype int32 and shape [1]\n",
      "\t [[{{node shape}}]]\n",
      "2024-01-25 21:28:42.012016: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'shape' with dtype int32 and shape [1]\n",
      "\t [[{{node shape}}]]\n",
      "2024-01-25 21:28:42.019040: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'poisson_noncpu/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat' with dtype int32 and shape [2]\n",
      "\t [[{{node poisson_noncpu/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat}}]]\n",
      "2024-01-25 21:28:42.019097: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'poisson_noncpu/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat' with dtype int32 and shape [2]\n",
      "\t [[{{node poisson_noncpu/while/uniform/stateless_random_uniform/StatelessRandomUniformV2/poisson_noncpu/concat}}]]\n",
      "2024-01-25 21:28:42.036357: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'shape' with dtype int32 and shape [1]\n",
      "\t [[{{node shape}}]]\n",
      "2024-01-25 21:28:42.038685: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'shape' with dtype int32 and shape [1]\n",
      "\t [[{{node shape}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38602878109215666\n",
      "./Adult_Analysis/pc/PP/Adult_pc_pp_64.csv 0.06894784011825217\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inpot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 229\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mprint\u001b[39m(edge_list_filename, kd\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m#     re = rel_entr(pdf,pdf1)\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m#     kl2 = entropy(pdf, qk=pdf1)\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[43minpot\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inpot' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.special import kl_div,rel_entr\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.append(\"./subjects/\")\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random, math\n",
    "import tensorflow_probability as tfb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import  euclidean_distances\n",
    "from Utils_Functions import KLdivergence\n",
    "from sklearn.feature_selection import SelectKBest, SelectFpr,SelectPercentile \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from aif360.sklearn.metrics import equal_opportunity_difference,average_odds_difference\n",
    "from Utils_Functions import generate_dataset, eod\n",
    "import glob\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "def generate_dataset(data, graph, edges, ave_dist, centroids,sens_index, priv_group, unpriv_group):\n",
    "\n",
    "    dataset_types = [str(data[i].dtype) for i in data.columns]\n",
    "    succ_generated = 0\n",
    "    generation_coef = 10\n",
    "    graph_dic ={}\n",
    "    for i in graph.columns[1:]:\n",
    "        if np.where(graph[i])[0].shape[0]==0:\n",
    "            graph_dic[i]=None\n",
    "        else:\n",
    "            graph_dic[i]= graph['Unnamed: 0'][np.where(graph[i])[0]].values\n",
    "\n",
    "    final_df = pd.DataFrame(columns = data.columns) \n",
    "    trial = 0\n",
    "    not_interesting = False\n",
    "    while final_df.shape[0]<data.shape[0]:\n",
    "\n",
    "        if trial > 20:\n",
    "            not_interesting = True\n",
    "            return None , 0.0#succ_generated/( trial *  data.shape[0]*generation_coef) \n",
    "        df_new_dic ={}\n",
    "        for edge in graph.sum().index[np.where(graph.sum()==0)[0]]:\n",
    "            df_new_dic[edge] = np.random.choice(np.unique(data[edge]), size = data.shape[0]*generation_coef) \n",
    "#             df_new_dic[edge] = df[edge]\n",
    "        statring_atts = graph.sum().index[np.where(graph.sum()==0)[0]]\n",
    "        \n",
    "        while statring_atts.shape[0] != graph['Unnamed: 0'].shape[0]:\n",
    "            \n",
    "            for att in graph_dic.keys():\n",
    "                if att not in statring_atts:\n",
    "                    if 0 in  [1 if graph_dic[att][i] in statring_atts else 0 for i in range(graph_dic[att].shape[0])]:\n",
    "\n",
    "                        continue\n",
    "                    else:\n",
    "                        edge_logits = 0\n",
    "                        \n",
    "                        for cause in graph_dic[att]:\n",
    "                            edge_logits += (edges[cause+att] * df_new_dic[cause])\n",
    "                        if np.unique(data[att]).shape[0]==2:\n",
    "                            df_new_dic[att] =  tfb.distributions.Bernoulli(logits=edge_logits + edges[att+'0'] ).sample().numpy()\n",
    "                        elif 'float' in dataset_types[np.where(data.columns==att)[0][0]]:\n",
    "\n",
    "                            df_new_dic[att] =  tfb.distributions.Normal(loc=(edge_logits+ edges[att+'0']), scale= edges['sigma_h']).sample().numpy()\n",
    "                        \n",
    "                        else:    \n",
    "                            df_new_dic[att] =  tfb.distributions.Poisson(rate=tf.exp(edge_logits+ edges[att+'0']) ).sample().numpy()  \n",
    "\n",
    "                        statring_atts = np.append(statring_atts,att) \n",
    "\n",
    "        new_df = pd.DataFrame(columns = data.columns)\n",
    "        for col in new_df.columns:\n",
    "            new_df[col] = df_new_dic[col]\n",
    "        \n",
    "        ind_inf = np.unique(np.where(new_df>data.max())[0])\n",
    "        new_df.drop(ind_inf,axis=0,inplace=True)\n",
    "        for col in range(new_df.columns.shape[0]):\n",
    "            new_df[new_df.columns[col]]=new_df[new_df.columns[col]].astype(dataset_types[col])\n",
    "        if new_df.shape[0]<1:\n",
    "            return None , 0.0\n",
    "        X2 = new_df.to_numpy()[:,:-1]\n",
    "        Y2 = new_df.to_numpy()[:,-1]\n",
    "        dist = euclidean_distances(X2, centroids)\n",
    "        succ_generated += new_df.iloc[np.where((ave_dist>=dist).sum(1)>0)].shape[0]\n",
    "        final_df = pd.concat([final_df,new_df.iloc[np.where((ave_dist>=dist).sum(1)>0)[0]]]).reset_index(drop=True)\n",
    "        final_df = final_df.drop_duplicates()\n",
    "\n",
    "        #print(succ_generated,trial)\n",
    "        if succ_generated<10:\n",
    "            return None, 0.0\n",
    "            \n",
    "        trial += 1\n",
    "   \n",
    "    #final_df = final_df.astype(int)\n",
    "    succ_rate = succ_generated/( trial *  data.shape[0]*generation_coef) \n",
    "    final_df = final_df.sample(n= data.shape[0])\n",
    "    Y2 = final_df.to_numpy()[:,-1]\n",
    "    if (Y2.sum()/Y2.shape[0]< 0.06) or (Y2.sum()/Y2.shape[0]> 0.95):\n",
    "        return None, 0.0#succ_generated/( trial *  data.shape[0]*generation_coef)   \n",
    "    if priv_group not in final_df[final_df.columns[sens_index]].values or unpriv_group not in final_df[final_df.columns[sens_index]].values:\n",
    "        return None, 0.0\n",
    "\n",
    "    return final_df, succ_rate\n",
    "\n",
    "for dataset in ['Adult']:\n",
    "    if dataset == 'Adult':\n",
    "        sens_index = 7\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0\n",
    "        data_file_name = 'adult_org-Copy1.csv'\n",
    "        alg_list = ['pc','ges','simy']\n",
    "        \n",
    "    if dataset == 'Compas':\n",
    "        sens_index = 1\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0\n",
    "        data_file_name = 'compas-Copy1'\n",
    "        alg_list = ['ges','pc']\n",
    "        \n",
    "    if dataset == 'Bank':\n",
    "        sens_index = 0\n",
    "        priv_group = 5\n",
    "        unpriv_group = 3\n",
    "        data_file_name = 'bank'\n",
    "        alg_list = ['ges']\n",
    "        \n",
    "    if dataset == 'Heart':\n",
    "        sens_index = 0\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0 \n",
    "        data_file_name = 'heart_processed_1'\n",
    "        alg_list = ['ges']\n",
    "        \n",
    "    if dataset == 'Law':\n",
    "        sens_index = 1\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0\n",
    "        data_file_name = 'law.csv'\n",
    "        alg_list = ['ges','simy']\n",
    "\n",
    "    if dataset == 'Student':\n",
    "        sens_index = 0\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0 \n",
    "        alg_list = ['simy','pc']\n",
    "        data_file_name = 'students-processed_2'\n",
    "        \n",
    "    df = pd.read_csv('./subjects/datasets/'+data_file_name)\n",
    "    df =  df.drop_duplicates().reset_index(drop=True)\n",
    "    KD = KernelDensity(bandwidth=0.2,kernel='gaussian')\n",
    "    data = df.to_numpy()\n",
    "    KD.fit(data)\n",
    "    prob = KD.score_samples(data)\n",
    "    pdf = np.exp(prob)/np.sum(np.exp(prob))\n",
    "    X1 = df.to_numpy()[:,:-1]\n",
    "    Y1 = df.to_numpy()[:,-1].astype(int)\n",
    "    num_cluster = 100\n",
    "    try :\n",
    "        with open('./'+dataset+'_Analysis/Kmean/KMean_{clus}.pkl'.format(clus=num_cluster), 'rb') as f:\n",
    "            KMean = pickle.load(f)\n",
    "    except:\n",
    "        KMean = KMeans(n_clusters=num_cluster)\n",
    "        KMean.fit(X1)\n",
    "        with open('./'+dataset+'_Analysis/Kmean/KMean_{clus}.pkl'.format(clus=num_cluster),'wb') as f:\n",
    "            pickle.dump(KMean,f)\n",
    "\n",
    "    ave_dist =[] \n",
    "    for i in range(KMean.n_clusters):\n",
    "        mean_dist = euclidean_distances(X1[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).mean()\n",
    "        std_dist = euclidean_distances(X1[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).std()\n",
    "        ave_dist.append(mean_dist+2 * std_dist )\n",
    "        #ave_dist.append(euclidean_distances(X1[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).max())\n",
    "    final_dic = {}\n",
    "    for Algorithm in alg_list:\n",
    "        print('Algorithm', Algorithm)\n",
    "        for edge_list_filename in glob.glob('./'+dataset+'_Analysis/'+Algorithm+'/PP/*.csv'):\n",
    "            #print(edge_list_filename)\n",
    "            file_num = int(re.findall(r'\\d+', edge_list_filename.split('/')[-1])[0])\n",
    "            RQ1_res = np.load('./'+dataset+'_Analysis/RQ1/'+dataset+'_'+Algorithm+'_RQ1_results.npy')\n",
    "            if RQ1_res[np.where(RQ1_res[:,1].astype(int)==file_num)[0]][0][2].astype(float)==0.0:\n",
    "                print('No',file_num)\n",
    "                continue\n",
    "            try:\n",
    "                graph_filename = './'+dataset+'_Analysis/'+Algorithm+'/DAGs/'+dataset+'_'+Algorithm+'_DAG_{file_num}.csv'.format(file_num=file_num)\n",
    "                graph = pd.read_csv(graph_filename)\n",
    "                #edge_list_filename = './'+dataset+'_Analysis/'+Algorithm+'/PP/'+dataset+'_'+Algorithm+'_pp_{file_num}.csv'.format(file_num=file_num)\n",
    "                edges_list = pd.read_csv(edge_list_filename)\n",
    "                \n",
    "                if dataset=='Bank' and Algorithm=='simy':\n",
    "                    graph.columns = [i.replace('1','') for i in graph.columns]\n",
    "                    graph[graph.columns[0]] = [i.replace('1','') for i in graph[graph.columns[0]]]\n",
    "                    edges_list.columns = [i.replace('1','') for i in edges_list.columns]\n",
    "                edges_list = edges_list[edges_list.columns[1:-1]]\n",
    "\n",
    "            except:\n",
    "                print('Not a DAG! ',file_num)\n",
    "                continue\n",
    "            \n",
    "            edges = edges_list.mean()    \n",
    "\n",
    "            final_df, succ_rate = generate_dataset(df, graph, edges, ave_dist, KMean.cluster_centers_ ,sens_index, priv_group, unpriv_group)\n",
    "\n",
    "            if succ_rate==0.0 :\n",
    "                    continue\n",
    "            print(succ_rate)\n",
    "\n",
    "            KD = KernelDensity(bandwidth=0.2,kernel='gaussian')\n",
    "            data1 = final_df.to_numpy()\n",
    "            KD.fit(data1)\n",
    "            prob1 = KD.score_samples(data1)\n",
    "\n",
    "\n",
    "            pdf1 = np.exp(prob1)/np.sum(np.exp(prob1))\n",
    "\n",
    "            kd =kl_div(pdf,pdf1)\n",
    "            print(edge_list_filename, kd.sum())\n",
    "            #     re = rel_entr(pdf,pdf1)\n",
    "            #     kl2 = entropy(pdf, qk=pdf1)\n",
    "            inpot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a998dc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.879064917524047e-14"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "kernel = stats.gaussian_kde(data.T)\n",
    "kernel1 = stats.gaussian_kde(data1.astype(float).T)\n",
    "kde = kernel.evaluate(data.T)\n",
    "kde1 = kernel1.evaluate(data1.astype(float).T)\n",
    "pdf_1 = np.exp(kde)/np.sum(np.exp(kde))\n",
    "pdf_2 = np.exp(kde1)/np.sum(np.exp(kde1))\n",
    "kd =kl_div(pdf_1,pdf_2)\n",
    "kd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6510dee0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't get common type for non-numeric array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mkernel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/scipy/stats/_kde.py:613\u001b[0m, in \u001b[0;36mgaussian_kde.pdf\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpdf\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    604\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03m    Evaluate the estimated pdf on a provided set of points.\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/scipy/stats/_kde.py:267\u001b[0m, in \u001b[0;36mgaussian_kde.evaluate\u001b[0;34m(self, points)\u001b[0m\n\u001b[1;32m    263\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoints have dimension \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, dataset has dimension \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (d,\n\u001b[1;32m    264\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md)\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m--> 267\u001b[0m output_dtype, spec \u001b[38;5;241m=\u001b[39m \u001b[43m_get_output_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m result \u001b[38;5;241m=\u001b[39m gaussian_kernel_estimate[spec](\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[:, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m    270\u001b[0m     points\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcho_cov, output_dtype)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/scipy/stats/_kde.py:712\u001b[0m, in \u001b[0;36m_get_output_dtype\u001b[0;34m(covariance, points)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_output_dtype\u001b[39m(covariance, points):\n\u001b[1;32m    706\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03m    Calculates the output dtype and the \"spec\" (=C type name).\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m    This was necessary in order to deal with the fused types in the Cython\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    routine `gaussian_kernel_estimate`. See gh-10824 for details.\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 712\u001b[0m     output_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommon_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovariance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m     itemsize \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(output_dtype)\u001b[38;5;241m.\u001b[39mitemsize\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m itemsize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mcommon_type\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/numpy/lib/type_check.py:730\u001b[0m, in \u001b[0;36mcommon_type\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    728\u001b[0m         p \u001b[38;5;241m=\u001b[39m array_precision\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    729\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 730\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt get common type for non-numeric array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    731\u001b[0m     precision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(precision, p)\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_complex:\n",
      "\u001b[0;31mTypeError\u001b[0m: can't get common type for non-numeric array"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "758d5d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.57148022e-07, 1.20237139e-07, 2.52211340e-07, ...,\n",
       "       1.54018465e-07, 1.15675690e-07, 1.21864029e-07])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kde1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9718204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "bandwidths = 10 ** np.linspace(-1, 1, 100)\n",
    "grid = GridSearchCV(KernelDensity(kernel='gaussian'),\n",
    "                    {'bandwidth': bandwidths},\n",
    "                    cv=LeaveOneOut())\n",
    "grid.fit(data);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
