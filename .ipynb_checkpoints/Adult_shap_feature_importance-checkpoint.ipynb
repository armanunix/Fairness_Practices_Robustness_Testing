{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df26b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(\"./subjects/\")\n",
    "import time, random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from adf_data.adult import adult_data\n",
    "from adf_utils.config import adult\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from Search_utils import DT_tree\n",
    "from Search_utils import causal_dataset_adult_I\n",
    "from Search_utils import causal_pertubation_adult_I\n",
    "from Search_utils import nodes_analysis\n",
    "from Search_utils import tree_distance\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "def test_cases(X_train, X_test, y_train, y_test, A_train, A_test,dataset, program_name, max_iter, \n",
    "               sensitive_param, unpriviliged_group, priviliged_group, sensitive_name):\n",
    "    global df_inp, df_input_org, shap_values,fig,X, feat_imoprtance\n",
    "    EOD_tresh = 0.05\n",
    "    filename0 ='ShAP_NI_new'\n",
    "    le =  LabelEncoder()\n",
    "    #global X_train, y_train, X_test, y_test\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    acc_None = round(accuracy_score(y_test,preds),2)\n",
    "    f1_None = round(f1_score(y_test,preds),2)\n",
    "    print(acc_None,f1_None )\n",
    "    pred_bal_None = preds.sum()/preds.shape[0]\n",
    "    if(program_name == \"LogisticRegression\"):\n",
    "        import Logistic_Regression_Mitigation\n",
    "        original_program = Logistic_Regression_Mitigation.logisticRegressionOriginal\n",
    "        input_program_tree = 'logistic_regression_Params.xml'\n",
    "        num_args = 15\n",
    "    coef_data=pd.read_csv('./Results/adult_coef1.csv')\n",
    "    df_input_org = pd.read_csv('Results/'+program_name+'/'+dataset+'/'+sensitive_name+'/Inputs.csv').drop(columns=['Unnamed: 0'])\n",
    "    df_input_org.columns = ['solver', 'penalty', 'dual', 'tol', 'C', 'fit_intercept', 'intercept_scaling', \n",
    "                        'max_iteration', 'multi_class', 'l1_ratio', 'class_weight', 'random_state', 'verbose',\n",
    "                        'warm_start', 'n_jobs','EOD', 'time','score','intresting']\n",
    "   \n",
    "    cat_columns = ['solver', 'penalty', 'dual','fit_intercept', 'multi_class', 'warm_start']\n",
    "\n",
    "    default_acc = acc_None - 0.05\n",
    "    epsilon = 0.05\n",
    "#-------------------------------------------------------------\n",
    "    coef_list = ['e0', 'z1e', 'z2e', 'xe', 'ne', 'h0', 'z1h', 'z2h', 'xh', 'nh', 'eh',\n",
    "       'w0', 'z2w', 'ew', 'nw', 'hw', 'm0', 'z1m', 'z2m', 'wm', 'hm', 'nm',\n",
    "       'xm', 'o0', 'z1o', 'z2o', 'eo', 'wo', 'mo', 'xo', 'r0', 'mr', 'er',\n",
    "       'z2r', 'nr', 'xr', 'y0', 'z1y', 'z2y', 'ey', 'oy', 'wy', 'my', 'hy',\n",
    "       'ry', 'ny', 'xy', 'sigma_h']\n",
    "#     Stratified Approach\n",
    "# ----------------------------------------\n",
    "\n",
    "#     bins = [i*0.01 for i in range(int(df_input_org['EOD'].min()*100),int(df_input_org['EOD'].max()*100)-2,1)]\n",
    "#     biniazed =np.digitize(df_input_org['EOD'], bins, right=False)\n",
    "#     for bin0 in bins:\n",
    "#         if np.where(biniazed==bin0*100)[0].shape[0] < 5 :\n",
    "#             bins = np.delete(bins,np.where(np.array(bins)==bin0)[0])\n",
    "#     df_input_org['bins'] = np.digitize(df_input_org['EOD'], bins, right=False)  \n",
    "#     X_train, df_inp_temp, y_train, y_test = train_test_split(df_input_org[df_input_org.columns[:15]], df_input_org['EOD'],stratify=df_input_org['bins'],test_size=0.005)\n",
    "#     df_inp_temp['EOD'] = y_test\n",
    "#     df_inp_temp.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# -------------------------------------------\n",
    "# #       Same Sample size of each bin\n",
    "    bins = [i*0.01 for i in range(int(df_input_org['EOD'].min()*100),int(df_input_org['EOD'].max()*100)-2,1)]\n",
    "    biniazed =np.digitize(df_input_org['EOD'], bins, right=False)\n",
    "    for bin0 in bins:\n",
    "        if np.where(biniazed==bin0*100)[0].shape[0] < 10 :\n",
    "            bins = np.delete(bins,np.where(np.array(bins)==bin0)[0])\n",
    "    df_input_org['bins'] = np.digitize(df_input_org['EOD'], bins, right=False)  \n",
    "    num_samples_st = 300\n",
    "    num_bins = df_input_org['bins'].unique().shape[0]\n",
    "    size_bins = int(num_samples_st / num_bins) + 1\n",
    "    random_indices = []\n",
    "    for bin0 in df_input_org['bins'].unique():\n",
    "        random_indices = np.append(random_indices,np.random.choice(np.where(df_input_org['bins']==bin0)[0], size =size_bins ))\n",
    "    \n",
    "#         Top Down Approach        \n",
    "#------------------------------------------------------\n",
    "#     n_samples = 200\n",
    "#     top_indices  = df_input_org.sort_values(by=['EOD']).tail(n_samples).index\n",
    "#     down_indices = df_input_org.sort_values(by=['EOD']).head(n_samples).index  \n",
    "#     print('label ratio' , np.where(df_input_org.iloc[np.concatenate([top_indices, down_indices])]['EOD'] >= EOD_tresh, 1, 0 ).sum()/(2*n_samples))\n",
    "\n",
    "#------------------------------------------------------\n",
    "#       dataset perturbation\n",
    "    feat_imoprtance = {}\n",
    "    rand_st = 0\n",
    "    successful_none = False\n",
    "    while successful_none != True:\n",
    "        print('RS',rand_st)\n",
    "\n",
    "        #     Stratified Approach\n",
    "        #df_inp = df_inp_temp.copy(deep=True)\n",
    "\n",
    "\n",
    "        #         Top Down Approach\n",
    "        #df_inp = df_input_org.iloc[np.concatenate([top_indices, down_indices])].reset_index(drop=True)\n",
    "\n",
    "        #       Same Sample size of each bin\n",
    "        \n",
    "        df_inp = df_input_org.iloc[random_indices].reset_index(drop=True)\n",
    "        None_class_ratio = np.where(df_inp['EOD'] >= EOD_tresh, 1, 0 ).sum()/df_inp.shape[0]\n",
    "        \n",
    "        status_perturb, df_inp = causal_pertubation_adult_I(df,original_program, df_inp, coef_data, \n",
    "                                                           coef_list, None, 0.0 ,\n",
    "                                                           False,  sensitive_name, \n",
    "                                                           default_acc=None, default_f1=None, \n",
    "                                                           random_state=rand_st, drop_feat = None)\n",
    "        \n",
    "        if status_perturb== False:\n",
    "            print('Unable to generate dataset with this perturbation!')\n",
    "            rand_st = random.randint(0,100)\n",
    "            \n",
    "            continue\n",
    "        else:\n",
    "            successful_none = True\n",
    "            print('EOD of perturbing',round(df_inp['EOD2'].mean(),3) )\n",
    "            labels = df_inp['EOD2']\n",
    "            \n",
    "            # Preprocessing the hyperparameters\n",
    "            for col in cat_columns:\n",
    "                df_inp[col] = le.fit_transform(df_inp[col])\n",
    "            columns = []\n",
    "            for col in df_inp.columns[:num_args]:\n",
    "                if df_inp[col].unique().shape[0] > 1 :\n",
    "                    columns.append(col)           \n",
    "\n",
    "            df_inp = df_inp[columns]\n",
    "            for ind in np.where(df_inp.isna().sum()>0):\n",
    "                df_inp[df_inp.columns[ind]+'_isna']=0\n",
    "                df_inp.loc[np.where(df_inp[df_inp.columns[ind]].isna()==True)[0],df_inp.columns[ind]+'_isna'] = 1\n",
    "                df_inp.loc[np.where(df_inp[df_inp.columns[ind]].isna()==True)[0], df_inp.columns[ind]] = 0\n",
    "\n",
    "            df_inp['label'] = labels\n",
    "\n",
    "            if not os.path.exists('./Results/'):\n",
    "                    os.makedirs('./Results/')\n",
    "            if not os.path.exists('./Results/' + str(program_name) + '/'):\n",
    "                os.makedirs('./Results/' + str(program_name) + '/')\n",
    "            if not os.path.exists('./Results/' + str(program_name) + '/'+ str(dataset) + '/'):\n",
    "                os.makedirs('./Results/' + str(program_name) + '/'+ str(dataset) + '/')\n",
    "            if not os.path.exists('./Results/' + str(program_name) + '/'+ str(dataset) + '/' + str(sensitive_name) + '/'):\n",
    "                os.makedirs('./Results/' + str(program_name) + '/' + str(dataset) + '/' + str(sensitive_name) + '/')\n",
    "            if not os.path.exists('./Results/' + str(program_name) + '/'+ str(dataset) + '/' + str(sensitive_name) + '/'+filename0+'/'):\n",
    "                os.makedirs('./Results/' + str(program_name) + '/' + str(dataset) + '/' + str(sensitive_name) + '/'+filename0+'/')\n",
    "    #             if not os.path.exists('./Results/' + str(program_name) + '/'+ str(dataset) + '/' + str(sensitive_name) + '/'+filename0+'/'+ str(edge0) + '/'):\n",
    "    #                 os.makedirs('./Results/' + str(program_name) + '/' + str(dataset) + '/' + str(sensitive_name) + '/'+filename0+'/'+ str(edge0) + '/')    \n",
    "\n",
    "            \n",
    "            dir_res = 'Results/'+program_name+'/'+dataset+'/'+sensitive_name+'/'+filename0+'/' + 'None' \n",
    "#             out = dir_res + '.dot'\n",
    "#             graph = DT_tree(df_inp)\n",
    "#             graph.write_png(dir_res + \".png\")\n",
    "#             condition = nodes_analysis(graph)\n",
    "#             None_condition = condition\n",
    "                        #print('Time to', causal_edge, time.time() - time1)\n",
    "            X = df_inp[df_inp.columns[:-1]]\n",
    "            y = df_inp['label']\n",
    "            features = X.columns\n",
    "            cat_features = []\n",
    "            for cat in X.select_dtypes(exclude=\"number\"):\n",
    "                cat_features.append(cat)\n",
    "                X[cat] = X[cat].astype(\"category\").cat.codes.astype(\"category\")\n",
    "            clf =  ElasticNetCV(cv=10).fit(X,y,) \n",
    "            explainer = shap.LinearExplainer(clf, X,algorithm='auto', feature_perturbation=\"correlation_dependent\")\n",
    "            shap_values = explainer.shap_values(X)\n",
    "            feat_imoprtance['None'] = X.columns[np.argsort(np.abs(shap_values).mean(0))]\n",
    "\n",
    "            fig  = shap.summary_plot(shap_values, X, show=False)         \n",
    "            plt.savefig(dir_res + \"_shap.png\")   \n",
    "            fig = plt.figure()\n",
    "            plt.figure().clear()\n",
    "            plt.close()\n",
    "            plt.cla()\n",
    "            plt.clf()\n",
    "            \n",
    "    distance_list=[]\n",
    "#     for step in range(res.shape[0]):\n",
    "        \n",
    "    \n",
    "#         causal_edge = res['edge'][step]\n",
    "#         step_size = float(res['step_size'][step])\n",
    "\n",
    "    for stp in range(-9,9):#[0.01, -0.01]:\n",
    "        if stp==0:continue\n",
    "        step_size = stp/100\n",
    "        for causal_edge in coef_list:\n",
    "            if 'sigma' in causal_edge or '0' in causal_edge:\n",
    "                continue\n",
    "            time1 = time.time()\n",
    "\n",
    "            #     Stratified Approach\n",
    "            #df_inp = df_inp_temp.copy(deep=True)\n",
    "\n",
    "            #         Top Down Approach\n",
    "            #df_inp = df_input_org.iloc[np.concatenate([top_indices, down_indices])].reset_index(drop=True)\n",
    "\n",
    "            #       Same Sample size of each bin\n",
    "            df_inp = df_input_org.iloc[random_indices].reset_index(drop=True)               \n",
    "\n",
    "            status_perturb, df_inp = causal_pertubation_adult_I(df,original_program, df_inp, coef_data, \n",
    "                                                                   coef_list, causal_edge, step_size ,\n",
    "                                                                   False,  sensitive_name, \n",
    "                                                                   default_acc=None, default_f1=None, \n",
    "                                                                   random_state=rand_st, \n",
    "                                                                  drop_feat = None)\n",
    "\n",
    "            if status_perturb== False:\n",
    "                print('Unable to generate dataset with perturbing', causal_edge)\n",
    "                continue\n",
    "            else:\n",
    "                if pd.isna(df_inp['EOD2'].mean()) == True:\n",
    "                    print('Perturbation leaded low accuracy!')\n",
    "                    continue\n",
    "                else:\n",
    "                    print('EOD of perturbing', causal_edge, df_inp['EOD2'].mean())\n",
    "        #--------------------------------------------------------           \n",
    "\n",
    "                labels = df_inp['EOD2']      \n",
    "                # Preprocessing the hyperparameters\n",
    "                for col in cat_columns:\n",
    "                    df_inp[col] = le.fit_transform(df_inp[col])\n",
    "                columns = []\n",
    "                for col in df_inp.columns[:num_args]:\n",
    "                    if df_inp[col].unique().shape[0] > 1 :\n",
    "                        columns.append(col)           \n",
    "\n",
    "                df_inp = df_inp[columns]\n",
    "                for ind in np.where(df_inp.isna().sum()>0):\n",
    "                    df_inp[df_inp.columns[ind]+'_isna']=0\n",
    "                    df_inp.loc[np.where(df_inp[df_inp.columns[ind]].isna()==True)[0],df_inp.columns[ind]+'_isna'] = 1\n",
    "                    df_inp.loc[np.where(df_inp[df_inp.columns[ind]].isna()==True)[0], df_inp.columns[ind]] = 0\n",
    "\n",
    "                df_inp['label'] = labels\n",
    "    #                 print('class ratio',labels.sum()/labels.shape[0])\n",
    "    #                 if labels.sum() < 3 or labels.sum() > labels.shape[0] - 3 :\n",
    "    #                     print('Only one class detected on ', causal_edge)\n",
    "    #                     if causal_edge== None:\n",
    "    #                         successful_none = True\n",
    "    #                     continue\n",
    "\n",
    "                if not os.path.exists('./Results/'):\n",
    "                        os.makedirs('./Results/')\n",
    "                if not os.path.exists('./Results/' + str(program_name) + '/'):\n",
    "                    os.makedirs('./Results/' + str(program_name) + '/')\n",
    "                if not os.path.exists('./Results/' + str(program_name) + '/'+ str(dataset) + '/'):\n",
    "                    os.makedirs('./Results/' + str(program_name) + '/'+ str(dataset) + '/')\n",
    "                if not os.path.exists('./Results/' + str(program_name) + '/'+ str(dataset) + '/' + str(sensitive_name) + '/'):\n",
    "                    os.makedirs('./Results/' + str(program_name) + '/' + str(dataset) + '/' + str(sensitive_name) + '/')\n",
    "                if not os.path.exists('./Results/' + str(program_name) + '/'+ str(dataset) + '/' + str(sensitive_name) + '/'+filename0+'/'):\n",
    "                    os.makedirs('./Results/' + str(program_name) + '/' + str(dataset) + '/' + str(sensitive_name) + '/'+filename0+'/')\n",
    "        #             if not os.path.exists('./Results/' + str(program_name) + '/'+ str(dataset) + '/' + str(sensitive_name) + '/'+filename0+'/'+ str(edge0) + '/'):\n",
    "        #                 os.makedirs('./Results/' + str(program_name) + '/' + str(dataset) + '/' + str(sensitive_name) + '/'+filename0+'/'+ str(edge0) + '/')    \n",
    "\n",
    "                dir_res = 'Results/'+program_name+'/'+dataset+'/'+sensitive_name+'/'+filename0+'/' + str(causal_edge)  + ('_pos_' if step_size >0 else '_neg_') + str(abs(step_size))\n",
    "    #                 out = dir_res + '.dot'\n",
    "    #                 graph = DT_tree(df_inp)\n",
    "    #                 graph.write_png(dir_res + \".png\")\n",
    "    #                 condition = nodes_analysis(graph)\n",
    "    #                 if step_size > 0 : \n",
    "    #                     distance_pos[causal_edge] = tree_distance(None_condition, condition)\n",
    "    #                 else :\n",
    "    #                     distance_neg[causal_edge] = tree_distance(None_condition, condition)\n",
    "                    #print('Time to this edge',time.time() - time1)\n",
    "                X = df_inp[df_inp.columns[:-1]]\n",
    "                y = df_inp[\"label\"]\n",
    "                features = X.columns\n",
    "                cat_features = []\n",
    "                for cat in X.select_dtypes(exclude=\"number\"):\n",
    "                    cat_features.append(cat)\n",
    "                    X[cat] = X[cat].astype(\"category\").cat.codes.astype(\"category\")\n",
    "                clf =  ElasticNetCV(cv=10).fit(X,y,) \n",
    "                explainer = shap.LinearExplainer(clf, X,algorithm='auto', feature_perturbation=\"correlation_dependent\")\n",
    "                shap_values = explainer.shap_values(X)\n",
    "                feat_imoprtance[causal_edge+'_'+ str(step_size)] = X.columns[np.argsort(np.abs(shap_values).mean(0))]\n",
    "                distance = 0 \n",
    "                for feat in  feat_imoprtance[causal_edge+'_'+ str(step_size)][-4:]:\n",
    "                    if feat not in feat_imoprtance['None'][-4:]:\n",
    "                        distance+=1\n",
    "                distance_list.append(distance)      \n",
    "                print(causal_edge,' feature importance')\n",
    "                fig  = shap.summary_plot(shap_values, X, show=False)\n",
    "                plt.savefig(dir_res +\"_shap.png\")   \n",
    "                fig = plt.figure()\n",
    "                plt.figure().clear()\n",
    "                plt.close()\n",
    "                plt.cla()\n",
    "                plt.clf()\n",
    "    #res['dis'] = distance_list\n",
    "    #res.to_csv('Results/'+program_name+'/'+dataset+'/'+sensitive_name+'/distances.csv')\n",
    "#     np.save('Results/'+program_name+'/'+dataset+'/'+sensitive_name+'/'+filename0+'/png/' + str(causal_edge) + '_pos.npy' ,distance_pos)\n",
    "#     np.save('Results/'+program_name+'/'+dataset+'/'+sensitive_name+'/'+filename0+'/png/' + str(causal_edge) + '_neg.npy' ,distance_neg)\n",
    "#     print('Negative' ,distance_neg)\n",
    "#     print('Postive' ,distance_pos)\n",
    "if __name__ == '__main__':\n",
    "    num_iteration =  100000\n",
    "    program_name=\"LogisticRegression\"\n",
    "    data = {'adult':adult_data}\n",
    "    data_config = {\"adult\":adult}\n",
    "    for dataset in data_config.keys(): \n",
    "        if dataset == \"adult\":\n",
    "            sens_list =[8]\n",
    "        for sensitive_param in sens_list:\n",
    "            if dataset == \"adult\" and sensitive_param == 8:\n",
    "                sensitive_name = \"gender\"\n",
    "                priviliged_group = [{sensitive_name: 1.0}]  #male\n",
    "                unpriviliged_group = [{sensitive_name: 0.0}]  #female\n",
    "                favorable_label  = 1.0\n",
    "                unfavorable_label = 0.0\n",
    "            X, Y, input_shape, nb_classes = data[dataset]()\n",
    "\n",
    "            df = pd.DataFrame(X)\n",
    "            df.columns = data_config[dataset]().feature_name\n",
    "            A = df[sensitive_name].to_numpy()\n",
    "            X = df.to_numpy()\n",
    "\n",
    "            df['label'] = Y\n",
    "\n",
    "            X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(X, Y, A, random_state=0,stratify=Y)\n",
    "            \n",
    "            test_cases(X_train, X_test, y_train, y_test, A_train, A_test, dataset, program_name, num_iteration, \n",
    "                       sensitive_param, unpriviliged_group, priviliged_group, sensitive_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087001fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
