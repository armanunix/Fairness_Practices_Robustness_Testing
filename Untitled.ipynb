{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a162b9b-982d-4014-a4e0-a0a53500c586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm ges\n",
      "./Adult_Analysis/ges/PP\\Adult_ges_pp_1.csv\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 563\u001b[0m\n\u001b[0;32m    561\u001b[0m None_model \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m--> 563\u001b[0m     final_df, status \u001b[38;5;241m=\u001b[39m generate_dataset_shift(df, graph, edges,sens_index, priv_group, unpriv_group)\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    566\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset', type=str, help='Select the dataset')\n",
    "# parser.add_argument('--practice', type=str, help='Select the dataset')\n",
    "# args = parser.parse_args()\n",
    "# dataset=args.dataset\n",
    "# Practice=args.practice\n",
    "dataset='Adult'\n",
    "Practice = 'TO'\n",
    "\n",
    "if Practice in ['SelectKBest', 'SelectFpr','SelectPercentile' ,'drop']:\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import sys,os\n",
    "    sys.path.append(\"./subjects/\")\n",
    "    \n",
    "    import pickle\n",
    "    import matplotlib.pyplot as plt\n",
    "    import itertools\n",
    "    from sklearn.cluster import KMeans\n",
    "    import tensorflow as tf\n",
    "    import time\n",
    "    import random, math\n",
    "    import tensorflow_probability as tfb\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import f1_score, accuracy_score\n",
    "    from sklearn.metrics.pairwise import  euclidean_distances\n",
    "    from Utils_Functions import KLdivergence\n",
    "    from sklearn.feature_selection import SelectKBest, SelectFpr,SelectPercentile \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import f1_score, accuracy_score\n",
    "    from aif360.sklearn.metrics import equal_opportunity_difference,average_odds_difference\n",
    "    import glob\n",
    "    import re\n",
    "    import argparse\n",
    "        \n",
    "    def generate_dataset_I(data, graph, edges, ave_dist, centroids,sens_index, priv_group, unpriv_group):\n",
    "    \n",
    "        dataset_types = [str(data[i].dtype) for i in data.columns]\n",
    "        succ_generated = 0\n",
    "        generation_coef = 10\n",
    "        graph_dic ={}\n",
    "        for i in graph.columns[1:]:\n",
    "            if np.where(graph[i])[0].shape[0]==0:\n",
    "                graph_dic[i]=None\n",
    "            else:\n",
    "                graph_dic[i]= graph['Unnamed: 0'][np.where(graph[i])[0]].values\n",
    "    \n",
    "        final_df = pd.DataFrame(columns = data.columns) \n",
    "        trial = 0\n",
    "        not_interesting = False\n",
    "        while final_df.shape[0]<data.shape[0]:\n",
    "    \n",
    "            if trial > 20:\n",
    "                not_interesting = True\n",
    "                return None , 0.0#succ_generated/( trial *  data.shape[0]*generation_coef) \n",
    "            df_new_dic ={}\n",
    "            for edge in graph.sum().index[np.where(graph.sum()==0)[0]]:\n",
    "                df_new_dic[edge] = np.random.choice(np.unique(data[edge]), size = data.shape[0]*generation_coef) \n",
    "    \n",
    "            statring_atts = graph.sum().index[np.where(graph.sum()==0)[0]]\n",
    "            \n",
    "            while statring_atts.shape[0] != graph['Unnamed: 0'].shape[0]:\n",
    "                \n",
    "                for att in graph_dic.keys():\n",
    "                    if att not in statring_atts:\n",
    "                        if 0 in  [1 if graph_dic[att][i] in statring_atts else 0 for i in range(graph_dic[att].shape[0])]:\n",
    "    \n",
    "                            continue\n",
    "                        else:\n",
    "                            edge_logits = 0\n",
    "                            \n",
    "                            for cause in graph_dic[att]:\n",
    "                                edge_logits += (edges[cause+att] * df_new_dic[cause])\n",
    "                            if np.unique(data[att]).shape[0]==2:\n",
    "                                df_new_dic[att] =  tfb.distributions.Bernoulli(logits=edge_logits + edges[att+'0'] ).sample().numpy()\n",
    "                            elif 'float' in dataset_types[np.where(data.columns==att)[0][0]]:\n",
    "    \n",
    "                                df_new_dic[att] =  tfb.distributions.Normal(loc=(edge_logits+ edges[att+'0']), scale= edges['sigma_h']).sample().numpy()\n",
    "                            \n",
    "                            else:    \n",
    "                                df_new_dic[att] =  tfb.distributions.Poisson(rate=tf.exp(edge_logits+ edges[att+'0']) ).sample().numpy()  \n",
    "    \n",
    "                            statring_atts = np.append(statring_atts,att) \n",
    "    \n",
    "            new_df = pd.DataFrame(columns = data.columns)\n",
    "            for col in new_df.columns:\n",
    "                new_df[col] = df_new_dic[col]\n",
    "            \n",
    "            ind_inf = np.unique(np.where(new_df>data.max())[0])\n",
    "            new_df.drop(ind_inf,axis=0,inplace=True)\n",
    "            for col in range(new_df.columns.shape[0]):\n",
    "                new_df[new_df.columns[col]]=new_df[new_df.columns[col]].astype(dataset_types[col])\n",
    "            if new_df.shape[0]<1:\n",
    "                return None , 0.0\n",
    "            X2 = new_df.to_numpy()[:,:-1]\n",
    "            Y2 = new_df.to_numpy()[:,-1]\n",
    "            dist = euclidean_distances(X2, centroids)\n",
    "            succ_generated += new_df.iloc[np.where((ave_dist>=dist).sum(1)>0)].shape[0]\n",
    "            final_df = pd.concat([final_df,new_df.iloc[np.where((ave_dist>=dist).sum(1)>0)[0]]]).reset_index(drop=True)\n",
    "            final_df = final_df.drop_duplicates()\n",
    "    \n",
    "            #print(succ_generated,trial)\n",
    "            if succ_generated<10:\n",
    "                return None, 0.0\n",
    "                \n",
    "            trial += 1\n",
    "       \n",
    "        #final_df = final_df.astype(int)\n",
    "        succ_rate = succ_generated/( trial *  data.shape[0]*generation_coef) \n",
    "        final_df = final_df.sample(n= data.shape[0])\n",
    "        Y2 = final_df.to_numpy()[:,-1]\n",
    "        if (Y2.sum()/Y2.shape[0]< 0.06) or (Y2.sum()/Y2.shape[0]> 0.95):\n",
    "            return None, 0.0#succ_generated/( trial *  data.shape[0]*generation_coef)   \n",
    "        if priv_group not in final_df[final_df.columns[sens_index]].values or unpriv_group not in final_df[final_df.columns[sens_index]].values:\n",
    "            return None, 0.0\n",
    "    \n",
    "        return final_df, succ_rate\n",
    "    \n",
    "    def eod(y_true, y_pred, sens, priv, unpriv):\n",
    "        \n",
    "        ind_priv = np.where(sens==priv)[0]\n",
    "        ytru_rate_priv = np.where(y_true[ind_priv]==1)[0].shape[0]\n",
    "        pred_rate_priv = np.where(y_pred[ind_priv]==1)[0].shape[0]\n",
    "        tpr_priv = ytru_rate_priv/ind_priv.shape[0] - pred_rate_priv/ind_priv.shape[0]\n",
    "        ind_unpriv = np.where(sens==unpriv)[0]\n",
    "        ytru_rate_unpriv = np.where(y_true[ind_unpriv]==1)[0].shape[0]\n",
    "        pred_rate_unpriv = np.where(y_pred[ind_unpriv]==1)[0].shape[0]\n",
    "        tpr_unpriv = ytru_rate_unpriv/ind_unpriv.shape[0] - pred_rate_unpriv/ind_unpriv.shape[0]\n",
    "        return round(tpr_priv - tpr_unpriv , 3)\n",
    "\n",
    "\n",
    "    for dataset in [dataset]:\n",
    "        if dataset == 'Adult':\n",
    "            sens_index = 7\n",
    "            priv_group = 1\n",
    "            unpriv_group = 0\n",
    "            data_file_name = 'adult_org-Copy1.csv'\n",
    "        if dataset == 'Compas':\n",
    "            sens_index = 1\n",
    "            priv_group = 1\n",
    "            unpriv_group = 0\n",
    "            data_file_name = 'compas-Copy1'\n",
    "        if dataset == 'Bank':\n",
    "            sens_index = 0\n",
    "            priv_group = 5\n",
    "            unpriv_group = 3\n",
    "            data_file_name = 'bank'\n",
    "        if dataset == 'Heart':\n",
    "            sens_index = 0\n",
    "            priv_group = 1\n",
    "            unpriv_group = 0 \n",
    "            data_file_name = 'heart_processed_1'\n",
    "        if dataset == 'Law':\n",
    "            sens_index = 1\n",
    "            priv_group = 1\n",
    "            unpriv_group = 0\n",
    "            data_file_name = 'law.csv'\n",
    "    \n",
    "        if dataset == 'Student':\n",
    "            sens_index = 0\n",
    "            priv_group = 1\n",
    "            unpriv_group = 0  \n",
    "            data_file_name = 'students-processed_2'\n",
    "        df = pd.read_csv('./subjects/datasets/'+data_file_name)\n",
    "        df = df.drop_duplicates()\n",
    "    \n",
    "        X1 = df.to_numpy()[:,:-1]\n",
    "        Y1 = df.to_numpy()[:,-1].astype(int)\n",
    "        num_cluster = 100\n",
    "        try :\n",
    "            with open('./'+dataset+'_Analysis/Kmean/KMean_{clus}.pkl'.format(clus=num_cluster), 'rb') as f:\n",
    "                KMean = pickle.load(f)\n",
    "        except:\n",
    "            KMean = KMeans(n_clusters=num_cluster)\n",
    "            KMean.fit(X1)\n",
    "            with open('./'+dataset+'_Analysis/Kmean/KMean_{clus}.pkl'.format(clus=num_cluster),'wb') as f:\n",
    "                pickle.dump(KMean,f)\n",
    "    \n",
    "        ave_dist =[] \n",
    "        for i in range(KMean.n_clusters):\n",
    "            mean_dist = euclidean_distances(X1[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).mean()\n",
    "            std_dist = euclidean_distances(X1[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).std()\n",
    "            ave_dist.append(mean_dist + (2 * std_dist))\n",
    "            #ave_dist.append(euclidean_distances(X1[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).max())\n",
    "    \n",
    "        for Algorithm in ['pc','ges','simy']:\n",
    "            print('Algorithm', Algorithm)\n",
    "            for edge_list_filename in glob.glob('./'+dataset+'_Analysis/'+Algorithm+'/PP/*.csv'):\n",
    "                print(edge_list_filename)\n",
    "                file_num = int(re.findall(r'\\d+', edge_list_filename)[0])\n",
    "                RQ1_res = np.load('./'+dataset+'_Analysis/RQ1/'+dataset+'_'+Algorithm+'_RQ1_results.npy')\n",
    "                if RQ1_res[np.where(RQ1_res[:,1].astype(int)==file_num)[0]][0][2].astype(float)==0.0:\n",
    "                    continue\n",
    "                try:\n",
    "                    graph_filename = './'+dataset+'_Analysis/'+Algorithm+'/DAGs/'+dataset+'_'+Algorithm+'_DAG_{file_num}.csv'.format(file_num=file_num)\n",
    "                    graph = pd.read_csv(graph_filename)\n",
    "                    #edge_list_filename = './'+dataset+'_Analysis/'+Algorithm+'/PP/'+dataset+'_'+Algorithm+'_pp_{file_num}.csv'.format(file_num=file_num)\n",
    "                    edges_list = pd.read_csv(edge_list_filename)\n",
    "                    if dataset=='Bank' and Algorithm=='simy':\n",
    "                        graph.columns = [i.replace('1','') for i in graph.columns]\n",
    "                        graph[graph.columns[0]] = [i.replace('1','') for i in graph[graph.columns[0]]]\n",
    "                        edges_list.columns = [i.replace('1','') for i in edges_list.columns]\n",
    "                    edges_list = edges_list[edges_list.columns[1:-1]]\n",
    "    \n",
    "                except:\n",
    "                    print('Not a DAG! ',file_num)\n",
    "                    continue\n",
    "    \n",
    "                X1_coef = edges_list.to_numpy()\n",
    "                KMean_coef = KMeans(n_clusters=10)\n",
    "                KMean_coef.fit(X1_coef)\n",
    "                SelectKBest_final_EOD=[]\n",
    "                SelectFpr_final_EOD=[]\n",
    "                SelectPercentile_final_EOD=[]\n",
    "                None_model_final_EOD = []\n",
    "                SelectKBest_final_AOD=[]\n",
    "                SelectFpr_final_AOD=[]\n",
    "                SelectPercentile_final_AOD=[]\n",
    "                None_model_final_AOD = []\n",
    "                drop_final_EOD = []\n",
    "                drop_final_AOD = []\n",
    "    \n",
    "                for i in range(KMean_coef.n_clusters):\n",
    "                    #print('Coef ',i)\n",
    "                    edges = edges_list.iloc[np.random.choice(np.where(KMean_coef.labels_==i)[0])]\n",
    "                    SelectKBest_EOD=[]\n",
    "                    SelectFpr_EOD=[]\n",
    "                    SelectPercentile_EOD=[]\n",
    "                    SelectKBest_AOD=[]\n",
    "                    SelectFpr_AOD=[]\n",
    "                    SelectPercentile_AOD=[]\n",
    "                    drop_EOD = []\n",
    "                    drop_AOD = []\n",
    "                    None_model = []\n",
    "                    for j in range(10):\n",
    "                        final_df, succ_rate = generate_dataset_I(df, graph, edges, ave_dist, KMean.cluster_centers_ ,sens_index, priv_group, unpriv_group)\n",
    "                        if succ_rate==0.0 :\n",
    "                            continue\n",
    "    \n",
    "                        X2 = final_df.to_numpy()[:,:-1]\n",
    "                        Y2 = final_df.to_numpy()[:,-1].astype(int)\n",
    "                        A2 = X2[:,sens_index]\n",
    "    \n",
    "                        if priv_group not in A2 or unpriv_group not in A2:\n",
    "                            print('No sens group')\n",
    "                            continue\n",
    "                        model = LogisticRegression()\n",
    "                        model.fit(X2,Y2)\n",
    "                        \n",
    "                        preds_None = model.predict(X2)\n",
    "                        acc_None = accuracy_score(Y2,preds_None)\n",
    "                        f1_None = f1_score(Y2,preds_None)\n",
    "                        AOD_None = round(average_odds_difference(Y2, preds_None,prot_attr=A2,priv_group=priv_group ),3)\n",
    "                        EOD_None = eod(Y2, preds_None,sens=A2, priv=priv_group, unpriv=unpriv_group)        \n",
    "                        None_model.append([EOD_None,acc_None,f1_None])\n",
    "                        \n",
    "    \n",
    "                        #print('Success rate DAG', file_num, succ_rate)\n",
    "                        for transformer in [Practice]:\n",
    "    \n",
    "    \n",
    "                            if transformer == 'SelectKBest':\n",
    "                                Kbest = SelectKBest( k=int(X2.shape[1]/2))\n",
    "                                X_new = Kbest.fit_transform(X2, Y2)\n",
    "                            elif transformer == 'SelectFpr':\n",
    "                                sfpr =  SelectFpr(alpha=0.01)\n",
    "                                X_new = sfpr.fit_transform(X2, Y2)\n",
    "    \n",
    "                            elif transformer == 'SelectPercentile':\n",
    "                                percentile =  SelectPercentile(percentile=10)\n",
    "                                X_new = percentile.fit_transform(X2, Y2)\n",
    "                            elif transformer == 'drop':                        \n",
    "                                X_new =  np.delete(X2,sens_index,axis=1)\n",
    "    \n",
    "    \n",
    "                            model = LogisticRegression()\n",
    "                            model.fit(X_new,Y2)\n",
    "                            preds_new = model.predict(X_new)\n",
    "                            acc_temp = accuracy_score(Y2,preds_new)\n",
    "                            f1_temp = f1_score(Y2,preds_new)\n",
    "                            AOD = round(average_odds_difference(Y2, preds_new,prot_attr=A2,priv_group=priv_group ),3)\n",
    "                            EOD = eod(Y2, preds_new,sens=A2, priv=priv_group, unpriv=unpriv_group)\n",
    "                            acc_diff = round(acc_temp - acc_None,2)\n",
    "                            f1_diff = round(f1_temp- f1_None,2)\n",
    "                            EOD_diff = EOD - EOD_None\n",
    "                            AOD_diff = AOD - AOD_None\n",
    "                            EOD_diff_abs = abs(EOD) - abs(EOD_None)\n",
    "                            AOD_diff_abs = abs(AOD) - abs(AOD_None)\n",
    "                            #print([ EOD_diff, acc_diff, f1_diff])\n",
    "                            if transformer == 'SelectKBest':\n",
    "                                SelectKBest_EOD.append([ EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                                SelectKBest_AOD.append([ AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "    \n",
    "                            elif transformer == 'SelectFpr':\n",
    "                                SelectFpr_EOD.append([EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                                SelectFpr_AOD.append([AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "    \n",
    "                            elif transformer == 'SelectPercentile':\n",
    "                                SelectPercentile_EOD.append([EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                                SelectPercentile_AOD.append([AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "    \n",
    "                            elif transformer == 'drop':\n",
    "                                drop_EOD.append([EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                                drop_AOD.append([AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "    \n",
    "                    if len(SelectKBest_EOD)>0:\n",
    "                        SelectKBest_final_EOD.append(np.mean(SelectKBest_EOD,axis=0))\n",
    "                    if len(SelectKBest_AOD)>0:\n",
    "                        SelectKBest_final_AOD.append(np.mean(SelectKBest_AOD,axis=0)) \n",
    "    \n",
    "                    if len(SelectFpr_EOD)>0:\n",
    "                        SelectFpr_final_EOD.append(np.mean(SelectFpr_EOD,axis=0))\n",
    "                    if len(SelectFpr_AOD)>0:\n",
    "                        SelectFpr_final_AOD.append(np.mean(SelectFpr_AOD,axis=0))\n",
    "    \n",
    "                    if len(SelectPercentile_EOD)>0:    \n",
    "                        SelectPercentile_final_EOD.append(np.mean(SelectPercentile_EOD,axis=0))\n",
    "                    if len(SelectPercentile_AOD)>0:    \n",
    "                        SelectPercentile_final_AOD.append(np.mean(SelectPercentile_AOD,axis=0))\n",
    "    \n",
    "                    if len(drop_EOD)>0:    \n",
    "                        drop_final_EOD.append(np.mean(drop_EOD,axis=0))\n",
    "                    if len(drop_AOD)>0:    \n",
    "                        drop_final_AOD.append(np.mean(drop_AOD,axis=0))\n",
    "    \n",
    "        #         print(np.array(SelectKBest_final_EOD).min(),np.array(SelectKBest_final_EOD).max())\n",
    "        #         print(np.array(SelectKBest_final_AOD).min(),np.array(SelectKBest_final_AOD).max())\n",
    "    \n",
    "        #         print(np.array(SelectFpr_final_EOD).min(),np.array(SelectFpr_final_EOD).max())\n",
    "        #         print(np.array(SelectFpr_final_AOD).min(),np.array(SelectFpr_final_AOD).max())\n",
    "    \n",
    "        #         print(np.array(SelectPercentile_final_EOD).min(),np.array(SelectPercentile_final_EOD).max())\n",
    "        #         print(np.array(SelectPercentile_final_AOD).min(),np.array(SelectPercentile_final_AOD).max())\n",
    "                np.save('./'+dataset+'_Analysis/RQ2/'+Algorithm+'_SelectKbest_EOD_' + str(file_num)+'.npy',SelectKBest_final_EOD)\n",
    "                np.save('./'+dataset+'_Analysis/RQ2/'+Algorithm+'_SelectKbest_AOD_' + str(file_num)+'.npy',SelectKBest_final_AOD)\n",
    "    \n",
    "                np.save('./'+dataset+'_Analysis/RQ2/'+Algorithm+'_SelectFpr_EOD_' + str(file_num)+'.npy',SelectFpr_final_EOD)\n",
    "                np.save('./'+dataset+'_Analysis/RQ2/'+Algorithm+'_SelectFpr_AOD_' + str(file_num)+'.npy',SelectFpr_final_AOD)\n",
    "    \n",
    "                np.save('./'+dataset+'_Analysis/RQ2/'+Algorithm+'_SelectPercentile_EOD_' + str(file_num)+'.npy',SelectPercentile_final_EOD)\n",
    "                np.save('./'+dataset+'_Analysis/RQ2/'+Algorithm+'_SelectPercentile_AOD_' + str(file_num)+'.npy',SelectPercentile_final_AOD)\n",
    "    \n",
    "                np.save('./'+dataset+'_Analysis/RQ2/'+Algorithm+'_drop_EOD_' + str(file_num)+'.npy',drop_final_EOD)\n",
    "                np.save('./'+dataset+'_Analysis/RQ2/'+Algorithm+'_drop_AOD_' + str(file_num)+'.npy',drop_final_AOD)\n",
    "\n",
    "if Practice in ['TO','CEO']:\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import sys,os\n",
    "    sys.path.append(\"./subjects/\")\n",
    "\n",
    "    import pickle\n",
    "    import matplotlib.pyplot as plt\n",
    "    import itertools\n",
    "    from sklearn.cluster import KMeans\n",
    "    import tensorflow as tf\n",
    "    import time\n",
    "    import random, math\n",
    "    import tensorflow_probability as tfb\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import f1_score, accuracy_score\n",
    "    from sklearn.metrics.pairwise import  euclidean_distances\n",
    "    from Utils_Functions import KLdivergence\n",
    "    from sklearn.feature_selection import SelectKBest, SelectFpr,SelectPercentile \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import f1_score, accuracy_score\n",
    "    from aif360.sklearn.metrics import equal_opportunity_difference,average_odds_difference\n",
    "    from Utils_Functions import generate_dataset, eod\n",
    "    import glob\n",
    "    import re\n",
    "    from fairlearn.postprocessing import ThresholdOptimizer\n",
    "    from aif360.sklearn.postprocessing import CalibratedEqualizedOdds\n",
    "    import argparse\n",
    "    def generate_dataset_shift(data, graph, edges, sens_index,priv_group, unpriv_group):\n",
    "    \n",
    "        dataset_types = [str(data[i].dtype) for i in data.columns]\n",
    "        succ_generated = 0\n",
    "        generation_coef = 1\n",
    "        graph_dic ={}\n",
    "        for i in graph.columns[1:]:\n",
    "            if np.where(graph[i])[0].shape[0]==0:\n",
    "                graph_dic[i]=None\n",
    "            else:\n",
    "                graph_dic[i]= graph['Unnamed: 0'][np.where(graph[i])[0]].values\n",
    "    \n",
    "        final_df = pd.DataFrame(columns = data.columns) \n",
    "        trial = 0\n",
    "        not_interesting = False\n",
    "        while final_df.shape[0]<data.shape[0]:\n",
    "    \n",
    "            if trial > 20:\n",
    "                not_interesting = True\n",
    "                return None , False\n",
    "            df_new_dic ={}\n",
    "            for edge in graph.sum().index[np.where(graph.sum()==0)[0]]:\n",
    "                df_new_dic[edge] = np.random.choice(np.unique(data[edge]), size = data.shape[0]*generation_coef) \n",
    "    \n",
    "            statring_atts = graph.sum().index[np.where(graph.sum()==0)[0]]\n",
    "            \n",
    "            while statring_atts.shape[0] != graph['Unnamed: 0'].shape[0]:\n",
    "                \n",
    "                for att in graph_dic.keys():\n",
    "                    if att not in statring_atts:\n",
    "                        if 0 in  [1 if graph_dic[att][i] in statring_atts else 0 for i in range(graph_dic[att].shape[0])]:\n",
    "    \n",
    "                            continue\n",
    "                        else:\n",
    "                            edge_logits = 0\n",
    "                            \n",
    "                            for cause in graph_dic[att]:\n",
    "                                edge_logits += (edges[cause+att] * df_new_dic[cause])\n",
    "                            if np.unique(data[att]).shape[0]==2:\n",
    "                                df_new_dic[att] =  tfb.distributions.Bernoulli(logits=edge_logits + edges[att+'0'] ).sample().numpy()\n",
    "                            elif 'float' in dataset_types[np.where(data.columns==att)[0][0]]:\n",
    "    \n",
    "                                df_new_dic[att] =  tfb.distributions.Normal(loc=(edge_logits+ edges[att+'0']), scale= edges['sigma_h']).sample().numpy()\n",
    "                            \n",
    "                            else:    \n",
    "                                df_new_dic[att] =  tfb.distributions.Poisson(rate=tf.exp(edge_logits+ edges[att+'0']) ).sample().numpy()  \n",
    "    \n",
    "                            statring_atts = np.append(statring_atts,att) \n",
    "    \n",
    "            new_df = pd.DataFrame(columns = data.columns)\n",
    "            for col in new_df.columns:\n",
    "                new_df[col] = df_new_dic[col]\n",
    "            \n",
    "            ind_inf = np.unique(np.where(new_df>data.max())[0])\n",
    "            new_df.drop(ind_inf,axis=0,inplace=True)\n",
    "            for col in range(new_df.columns.shape[0]):\n",
    "                new_df[new_df.columns[col]]=new_df[new_df.columns[col]].astype(dataset_types[col])\n",
    "            if new_df.shape[0]<1:\n",
    "                return None , False\n",
    "            X2 = new_df.to_numpy()[:,:-1]\n",
    "            Y2 = new_df.to_numpy()[:,-1]\n",
    "            #dist = euclidean_distances(X2, centroids)\n",
    "            #succ_generated += new_df.iloc[np.where((ave_dist>=dist).sum(1)>0)].shape[0]\n",
    "            final_df = pd.concat([final_df,new_df]).reset_index(drop=True)\n",
    "            final_df = final_df.drop_duplicates()\n",
    "    \n",
    "    \n",
    "                \n",
    "            trial += 1\n",
    "       \n",
    "        #final_df = final_df.astype(int)\n",
    "        Y2 = final_df.to_numpy()[:,-1]\n",
    "        if (Y2.sum()/Y2.shape[0]< 0.06) or (Y2.sum()/Y2.shape[0]> 0.95):\n",
    "            return None,  False\n",
    "        if priv_group not in final_df[final_df.columns[sens_index]].values or unpriv_group not in final_df[final_df.columns[sens_index]].values:\n",
    "            return None,  False\n",
    "    \n",
    "    \n",
    "    #dataset ='Bank'\n",
    "\n",
    "    for dataset in [dataset]:\n",
    "        if dataset == 'Adult':\n",
    "            sens_index = 7\n",
    "            priv_group = 1\n",
    "            unpriv_group = 0\n",
    "            data_file_name = 'adult_org-Copy1.csv'\n",
    "            alg_list = ['ges','simy']\n",
    "            \n",
    "        if dataset == 'Compas':\n",
    "            sens_index = 1\n",
    "            priv_group = 1\n",
    "            unpriv_group = 0\n",
    "            data_file_name = 'compas-Copy1'\n",
    "            alg_list = ['ges','pc']\n",
    "            \n",
    "        if dataset == 'Bank':\n",
    "            sens_index = 0\n",
    "            priv_group = 5\n",
    "            unpriv_group = 3\n",
    "            data_file_name = 'bank'\n",
    "            alg_list = ['ges']\n",
    "            \n",
    "        if dataset == 'Heart':\n",
    "            sens_index = 0\n",
    "            priv_group = 1\n",
    "            unpriv_group = 0 \n",
    "            data_file_name = 'heart_processed_1'\n",
    "            alg_list = ['ges']\n",
    "            \n",
    "        if dataset == 'Law':\n",
    "            sens_index = 1\n",
    "            priv_group = 1\n",
    "            unpriv_group = 0\n",
    "            data_file_name = 'law.csv'\n",
    "            alg_list = ['ges','simy']\n",
    "    \n",
    "        if dataset == 'Student':\n",
    "            sens_index = 0\n",
    "            priv_group = 1\n",
    "            unpriv_group = 0 \n",
    "            alg_list = ['simy','pc']\n",
    "            data_file_name = 'students-processed_2'\n",
    "            \n",
    "        df = pd.read_csv('./subjects/datasets/'+data_file_name)\n",
    "        df =  df.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "        X1 = df.to_numpy()[:,:-1]\n",
    "        Y1 = df.to_numpy()[:,-1].astype(int)\n",
    "    \n",
    "        \n",
    "        for Algorithm in alg_list:\n",
    "            print('Algorithm', Algorithm)\n",
    "            for edge_list_filename in glob.glob('./'+dataset+'_Analysis/'+Algorithm+'/PP/*.csv'):\n",
    "                print(edge_list_filename)\n",
    "                file_num = int(re.findall(r'\\d+', edge_list_filename.split('/')[-1])[0])\n",
    "                RQ1_res = np.load('./'+dataset+'_Analysis/RQ1/'+dataset+'_'+Algorithm+'_RQ1_results.npy')\n",
    "                if RQ1_res[np.where(RQ1_res[:,1].astype(int)==file_num)[0]][0][2].astype(float)==0.0:\n",
    "                    print('No',file_num)\n",
    "                    continue\n",
    "                try:\n",
    "                    graph_filename = './'+dataset+'_Analysis/'+Algorithm+'/DAGs/'+dataset+'_'+Algorithm+'_DAG_{file_num}.csv'.format(file_num=file_num)\n",
    "                    graph = pd.read_csv(graph_filename)\n",
    "                    #edge_list_filename = './'+dataset+'_Analysis/'+Algorithm+'/PP/'+dataset+'_'+Algorithm+'_pp_{file_num}.csv'.format(file_num=file_num)\n",
    "                    edges_list = pd.read_csv(edge_list_filename)\n",
    "                    if 'first_pf0' not in edges_list.columns and 'label0' not in edges_list.columns and 'G30' not in edges_list.columns and 'y0' not in edges_list.columns:\n",
    "                        continue \n",
    "                    if dataset=='Bank' and Algorithm=='simy':\n",
    "                        graph.columns = [i.replace('1','') for i in graph.columns]\n",
    "                        graph[graph.columns[0]] = [i.replace('1','') for i in graph[graph.columns[0]]]\n",
    "                        edges_list.columns = [i.replace('1','') for i in edges_list.columns]\n",
    "                    edges_list = edges_list[edges_list.columns[1:-1]]\n",
    "    \n",
    "                except:\n",
    "                    print('Not a DAG! ',file_num)\n",
    "                    continue\n",
    "    \n",
    "                X1_coef = edges_list.to_numpy()\n",
    "                KMean_coef = KMeans(n_clusters=10)\n",
    "                KMean_coef.fit(X1_coef)\n",
    "                \n",
    "                mitigator_final_EOD=[]\n",
    "                None_model_final_EOD = []\n",
    "                mitigator_final_AOD=[]\n",
    "                None_model_final_AOD = []\n",
    "    \n",
    "                for i in range(KMean_coef.n_clusters):\n",
    "                    #print('Coef ',i)\n",
    "                    weights_ind = np.random.choice(np.where(KMean_coef.labels_==i)[0])\n",
    "                    edges = edges_list.iloc[weights_ind]\n",
    "    \n",
    "                    if dataset=='Law':\n",
    "                        edges['first_pf0']+=1\n",
    "                    elif dataset=='Bank':\n",
    "                        edges['label0']+=1\n",
    "                    elif dataset=='Heart':\n",
    "                        edges['label0']+=1\n",
    "                    elif dataset=='Student':\n",
    "                        edges['G30']+=1\n",
    "                    else:\n",
    "                        edges['y0']+=1\n",
    "                    #print(file_num,weights_ind)\n",
    "                    mitigator_EOD=[]\n",
    "                    mitigator_AOD=[]\n",
    "                    None_model = []\n",
    "                    for j in range(10):\n",
    "                        final_df, status = generate_dataset_shift(df, graph, edges,sens_index, priv_group, unpriv_group)\n",
    "    \n",
    "                        if status==False:\n",
    "                            continue\n",
    "    \n",
    "                        X2 = final_df.to_numpy()[:,:-1]\n",
    "                        Y2 = final_df.to_numpy()[:,-1].astype(int)\n",
    "                        A2 = X2[:,sens_index]\n",
    "    \n",
    "                        if priv_group not in A2 or unpriv_group not in A2:\n",
    "                            print('No sens group')\n",
    "                            continue\n",
    "                        model = LogisticRegression()\n",
    "                        model.fit(X2,Y2)\n",
    "                        \n",
    "                        preds_None = model.predict(X2)\n",
    "                        acc_None = accuracy_score(Y2,preds_None)\n",
    "                        f1_None = f1_score(Y2,preds_None)\n",
    "                        AOD_None = round(average_odds_difference(Y2, preds_None,prot_attr=A2,priv_group=priv_group ),3)\n",
    "                        EOD_None = eod(Y2, preds_None,sens=A2, priv=priv_group, unpriv=unpriv_group)        \n",
    "                        None_model.append([EOD_None,acc_None,f1_None])\n",
    "                        y_prob = model.predict_proba(X2)\n",
    "                        if Practice == 'TO':\n",
    "                        #print('Success rate DAG', file_num, succ_rate)\n",
    "                            mitigator = ThresholdOptimizer(\n",
    "                                estimator=model,\n",
    "                                constraints=\"equalized_odds\",   # or \"demographic_parity\"\n",
    "                                prefit=True,\n",
    "                                predict_method=\"predict_proba\")\n",
    "                            mitigator.fit(X2,Y2, sensitive_features=A2)\n",
    "                            preds_new = mitigator.predict(X2,sensitive_features=A2)\n",
    "                        if Practice == 'CEO':\n",
    "                            mitigator = CalibratedEqualizedOdds(prot_attr=A2, cost_constraint='fpr')\n",
    "                            mitigator.fit(y_prob,Y2)\n",
    "                            preds_new = mitigator.predict(y_prob)\n",
    "                        acc_temp = accuracy_score(Y2,preds_new)\n",
    "                        f1_temp = f1_score(Y2,preds_new)\n",
    "                        AOD = round(average_odds_difference(Y2, preds_new,prot_attr=A2,priv_group=priv_group ),3)\n",
    "                        EOD = eod(Y2, preds_new,sens=A2, priv=priv_group, unpriv=unpriv_group)\n",
    "                        acc_diff = round(acc_temp - acc_None,2)\n",
    "                        f1_diff = round(f1_temp- f1_None,2)\n",
    "                        EOD_diff = EOD - EOD_None\n",
    "                        AOD_diff = AOD - AOD_None\n",
    "                        EOD_diff_abs = abs(EOD) - abs(EOD_None)\n",
    "                        AOD_diff_abs = abs(AOD) - abs(AOD_None)\n",
    "    \n",
    "                        input(EOD_diff)\n",
    "                        \n",
    "                        mitigator_EOD.append([ EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                        mitigator_AOD.append([ AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "    \n",
    "    \n",
    "    \n",
    "                    mitigator_final_EOD.append(np.mean(mitigator_EOD,axis=0))\n",
    "                    mitigator_final_AOD.append(np.mean(mitigator_AOD,axis=0)) \n",
    "                    \n",
    "                np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_'+Practice +'_'+ str(file_num)+'.npy',mitigator_final_EOD)\n",
    "                np.save('./'+dataset+'_Analysis/RQ4/'+Algorithm+'_'+Practice +'_' + str(file_num)+'.npy',mitigator_final_AOD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if Practice in ['HP']:\n",
    "    import sys,os\n",
    "    sys.path.append(\"./subjects/\")\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import time\n",
    "    import re, random\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import ElasticNetCV\n",
    "    from sklearn.cluster import KMeans\n",
    "    import shap\n",
    "    import xml_parser\n",
    "    import xml_parser_domains\n",
    "    from mutation import mutate, clip_LR\n",
    "    from fairlearn.metrics import equalized_odds_difference\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from Utils_Functions import generate_dataset, eod\n",
    "    import copy, pickle, glob\n",
    "    from sklearn.metrics.pairwise import  euclidean_distances\n",
    "    import argparse\n",
    "    \n",
    "    \n",
    "    \n",
    "    def HP_search_population(X, Y, A,X_shap, HP_population, program_name, max_iter, \n",
    "                   sensitive_param, unpriviliged_group, priviliged_group, sensitive_name, start_time):\n",
    "        time1 = time.time()\n",
    "        df_input = pd.DataFrame()\n",
    "        \n",
    "        if(program_name == \"LogisticRegression\"):\n",
    "            import Logistic_Regression_Mitigation\n",
    "            original_program = Logistic_Regression_Mitigation.logisticRegressionOriginal\n",
    "            input_program_tree = 'logistic_regression_Params.xml'\n",
    "            num_args = 15\n",
    "        \n",
    "    \n",
    "        for arr_clip in HP_population:\n",
    "    \n",
    "            res1, LR, inp_valid1, score_org, pred_org = original_program(arr_clip, X, X, Y, Y)\n",
    "    \n",
    "    \n",
    "            EOD_A = eod(Y, pred_org,sens=A, priv=priviliged_group, unpriv=unpriviliged_group)\n",
    "            \n",
    "            \n",
    "            df_input = df_input.append([EOD_A])\n",
    "            #print(\"---------------------------------------------------------\")                 \n",
    "            \n",
    "            #print('Iteration took ', time.time() - time2)\n",
    "    \n",
    "        \n",
    "    \n",
    "        df_input.columns=['EOD']\n",
    "        #label = np.digitize(df_input['EOD'],bins=bins)\n",
    "        Y_shap = df_input['EOD']\n",
    "        ENCV =  ElasticNetCV(cv=10).fit(X_shap,Y_shap) \n",
    "        explainer = shap.LinearExplainer(ENCV, X_shap)\n",
    "        shap_values = explainer.shap_values(X_shap)\n",
    "        HP_importance = X_shap.columns[np.argsort(np.abs(shap_values).mean(0))[::-1]]\n",
    "        return HP_importance, df_input, shap_values\n",
    "    \n",
    "    \n",
    "    def HP_search(X, Y, A, dataset, program_name, max_iter, \n",
    "                   sensitive_param, unpriviliged_group, priviliged_group, sensitive_name, start_time):\n",
    "        time1 = time.time()\n",
    "        df_input = pd.DataFrame()\n",
    "        default_acc = 0.0\n",
    "        HP_population = []\n",
    "        \n",
    "          \n",
    "        if(program_name == \"LogisticRegression\"):\n",
    "            import Logistic_Regression_Mitigation\n",
    "            original_program = Logistic_Regression_Mitigation.logisticRegressionOriginal\n",
    "            input_program_tree = 'logistic_regression_Params.xml'\n",
    "            num_args = 15\n",
    "        \n",
    "    \n",
    "        arr_min, arr_max, arr_type, arr_default = xml_parser_domains.xml_parser_domains(input_program_tree, num_args)\n",
    "    \n",
    "        seeds = []\n",
    "        promising_inputs_EOD = []\n",
    "        promising_metric_EOD = []\n",
    "    \n",
    "       \n",
    "        EOD_max = 0\n",
    "        for counter in range(max_iter):\n",
    "            time2 = time.time()\n",
    "            #print('counter',counter)  \n",
    "    \n",
    "            inp = mutate( arr_max, arr_min, arr_type, arr_default, promising_inputs_EOD, counter)\n",
    "    \n",
    "            if re.match(r'Logistic', program_name):\n",
    "                arr_clip, features = clip_LR(inp)\n",
    "            \n",
    "            if arr_clip in seeds:\n",
    "                #print('Duplicated Seed!')\n",
    "                continue\n",
    "            else:\n",
    "                seeds.append(arr_clip)\n",
    "            EOD_avg = []\n",
    "    \n",
    "            \n",
    "            X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(X, Y, A)\n",
    "            res1, LR, inp_valid1, score_org, pred_org = original_program(arr_clip, X_train, X_test, \n",
    "                                                                                y_train, y_test)\n",
    "            if not res1:\n",
    "                #print('training failed')\n",
    "                continue\n",
    "            else:\n",
    "                failed_flag = False\n",
    "    \n",
    "            # tolerate at most 2% of accuracy loss        \n",
    "            if counter == 0:\n",
    "                default_acc = score_org\n",
    "    \n",
    "            if (score_org < (default_acc - 0.05)) :\n",
    "                #print('Acc low')\n",
    "                continue\n",
    "            res1, LR1, inp_valid1, score_org, pred_org = original_program(arr_clip, X, X, Y, Y)\n",
    "            EOD_A = eod(Y, pred_org,sens=A, priv=priviliged_group, unpriv=unpriviliged_group)\n",
    "    #         pred = LR.predict(X)\n",
    "    #         # A original model , B mitigated mode   \n",
    "    #         EOD_avg.append(eod(Y, pred,sens=A, priv=priviliged_group, unpriv=unpriviliged_group))\n",
    "    #         print(EOD_avg[-1])\n",
    "    #         input()\n",
    "    #         EOD_A = np.mean(EOD_avg)\n",
    "    #         print(EOD_A)\n",
    "    #         print('------')\n",
    "            #print(eod_test_A, EOD_A)\n",
    "            if abs(EOD_A) > EOD_max:\n",
    "                EOD_max = abs(EOD_A)\n",
    "                #print('Intresting EOD')\n",
    "                intresting_EOD = 1\n",
    "                promising_inputs_EOD.append(inp)\n",
    "            elif abs(EOD_A) > 0.1:\n",
    "                #print('Intresting EOD')\n",
    "                intresting_EOD = 1\n",
    "                promising_inputs_EOD.append(inp)\n",
    "            else:\n",
    "                intresting_EOD = 0        \n",
    "            df_input = pd.concat([df_input,pd.DataFrame(arr_clip + [EOD_A]).T] , sort=False)\n",
    "            HP_population.append(arr_clip)\n",
    "            #print(\"---------------------------------------------------------\")                 \n",
    "            if time.time() - time1 >= time_out:\n",
    "                break\n",
    "            #print('Iteration took ', time.time() - time2)\n",
    "    \n",
    "        \n",
    "        df_input.columns = ['solver', 'penalty', 'dual', 'tol', 'C', 'fit_intercept', 'intercept_scaling', \n",
    "                            'max_iteration', 'multi_class', 'l1_ratio', 'class_weight', 'random_state', 'verbose',\n",
    "                            'warm_start', 'n_jobs','EOD']\n",
    "        df_input.reset_index(drop=True,inplace=True)\n",
    "        df_input.to_csv('./'+dataset+'_Analysis/RQ3/'+ dataset + '_base_HP_search.csv', index=False)\n",
    "        ind_0 = df_input.sort_values(by='EOD').head(200).index.to_list()\n",
    "        ind_1 = df_input.sort_values(by='EOD').tail(200).index.to_list()\n",
    "        HP_population = np.array(HP_population)[ind_0 + ind_1]\n",
    "        np.save('./'+dataset+'_Analysis/RQ3/'+ dataset + '_HP_population.csv',HP_population)\n",
    "        df_inp = df_input.iloc[ind_0+ind_1]\n",
    "    #     df_inp.loc[ind_0,'EOD'] = 0\n",
    "    #     df_inp.loc[ind_1,'EOD'] = 1\n",
    "        df_inp.reset_index(drop=True,inplace=True)\n",
    "        le =  LabelEncoder()\n",
    "        cat_columns = ['solver', 'penalty', 'dual','fit_intercept', 'multi_class', 'warm_start']\n",
    "        for col in cat_columns:\n",
    "            df_inp[col] = le.fit_transform(df_inp[col])\n",
    "        columns = []\n",
    "        for col in df_inp.columns[:-1]:\n",
    "            if df_inp[col].unique().shape[0] > 1 :\n",
    "                columns.append(col)           \n",
    "        label = df_inp['EOD'].to_numpy()\n",
    "    #     bins = np.histogram(df_inp['EOD'], bins=3)[1][1:-1].tolist()\n",
    "    #     label = np.digitize(df_inp['EOD'],bins=bins)\n",
    "    \n",
    "        df_inp = df_inp[columns]\n",
    "        for ind in np.where(df_inp.isna().sum()>0):\n",
    "            df_inp[df_inp.columns[ind]+'_isna']=0\n",
    "            df_inp.loc[np.where(df_inp[df_inp.columns[ind]].isna()==True)[0],df_inp.columns[ind]+'_isna'] = 1\n",
    "            df_inp.loc[np.where(df_inp[df_inp.columns[ind]].isna()==True)[0], df_inp.columns[ind]] = 0\n",
    "        df_inp['label'] = label\n",
    "        X_shap = df_inp[df_inp.columns[:-1]]\n",
    "        Y_shap = df_inp['label']\n",
    "        features = X_shap.columns\n",
    "        cat_features = []\n",
    "        for cat in X_shap.select_dtypes(exclude=\"number\"):\n",
    "            cat_features.append(cat)\n",
    "            X_shap[cat] = X_shap[cat].astype(\"category\").cat.codes.astype(\"category\")\n",
    "        ENCV =  ElasticNetCV(cv=10).fit(X_shap,Y_shap) \n",
    "        explainer = shap.LinearExplainer(ENCV, X_shap)\n",
    "        shap_values = explainer.shap_values(X_shap)\n",
    "        HP_importance = X_shap.columns[np.argsort(np.abs(shap_values).mean(0))[::-1]]\n",
    "        return HP_importance, df_input, HP_population, X_shap\n",
    "    if __name__ == '__main__':\n",
    "        start_time = time.time()\n",
    "        time_out = 60 * 60\n",
    "        original_model = True\n",
    "        save_model = False\n",
    "        num_iteration =  1000000\n",
    "        program_name=\"LogisticRegression\"\n",
    "        \n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--dataset', type=str, help='Select the dataset')\n",
    "        args = parser.parse_args()\n",
    "        dataset=args.dataset\n",
    "        \n",
    "        for dataset in [dataset]:\n",
    "            HP_importance_results = []\n",
    "            final_shap_list = []\n",
    "            #data_config.keys(): \n",
    "            if dataset == 'Adult':\n",
    "                sensitive_param = 7\n",
    "                sensitive_name = 'gender'\n",
    "                priviliged_group = 1  #male\n",
    "                unpriviliged_group = 0#female\n",
    "                favorable_label  = 1.0\n",
    "                unfavorable_label = 0.0\n",
    "                data_file_name = 'adult_org-Copy1.csv'\n",
    "                graph_base_filename ='./Adult_Analysis/ges/DAGs/Adult_ges_DAG_1.csv'\n",
    "                edge_list_base_filename = './Adult_Analysis/ges/PP/Adult_ges_pp_1.csv'\n",
    "                alg_list = ['ges','simy']\n",
    "            if dataset == 'Compas':\n",
    "                sensitive_param = 1\n",
    "                sensitive_name = 'race'\n",
    "                priviliged_group = 1  #male\n",
    "                unpriviliged_group = 0#female\n",
    "                favorable_label  = 1.0\n",
    "                unfavorable_label = 0.0\n",
    "                data_file_name = 'compas-Copy1'\n",
    "                graph_base_filename ='./Compas_Analysis/pc/DAGs/Compas_pc_DAG_13.csv'\n",
    "                edge_list_base_filename = './Compas_Analysis/pc/PP/Compas_pc_pp_13.csv'\n",
    "                alg_list = ['ges','pc']\n",
    "            if dataset == 'Bank':\n",
    "                sensitive_param = 0\n",
    "                sensitive_name = 'age'\n",
    "                priviliged_group = 5  #male\n",
    "                unpriviliged_group = 3#female\n",
    "                favorable_label  = 1.0\n",
    "                unfavorable_label = 0.0\n",
    "                data_file_name = 'bank'\n",
    "                graph_base_filename ='./Bank_Analysis/ges/DAGs/Bank_ges_DAG_8.csv'\n",
    "                edge_list_base_filename = './Bank_Analysis/ges/PP/Bank_ges_pp_8.csv'\n",
    "                alg_list = ['ges']\n",
    "            if dataset == 'Law':\n",
    "                sensitive_param = 0\n",
    "                sensitive_name = 'sex'\n",
    "                priviliged_group = 1  #male\n",
    "                unpriviliged_group = 0#female\n",
    "                favorable_label  = 1.0\n",
    "                unfavorable_label = 0.0\n",
    "                data_file_name = 'law.csv'\n",
    "                graph_base_filename ='./Law_Analysis/ges/DAGs/Law_ges_DAG_15.csv'\n",
    "                edge_list_base_filename = './Law_Analysis/ges/PP/Law_ges_pp_15.csv'\n",
    "                alg_list = ['ges','simy']\n",
    "            if dataset == 'Student':\n",
    "                sensitive_param = 0\n",
    "                sensitive_name = 'sex'\n",
    "                priviliged_group = 1  #male\n",
    "                unpriviliged_group = 0#female\n",
    "                favorable_label  = 1.0\n",
    "                unfavorable_label = 0.0\n",
    "                data_file_name = 'students-processed_2'\n",
    "                graph_base_filename ='./Student_Analysis/pc/DAGs/Student_pc_DAG_1.csv'\n",
    "                edge_list_base_filename = './Student_Analysis/pc/PP/Student_pc_pp_1.csv'\n",
    "                alg_list = ['simy','pc']\n",
    "            if dataset == 'Heart':\n",
    "                sensitive_param = 0\n",
    "                sensitive_name = 'sex'\n",
    "                priviliged_group = 1  #male\n",
    "                unpriviliged_group = 0#female\n",
    "                favorable_label  = 1.0\n",
    "                unfavorable_label = 0.0\n",
    "                data_file_name = 'heart_processed_1'\n",
    "                graph_base_filename ='./Heart_Analysis/ges/DAGs/Heart_ges_DAG_4.csv'\n",
    "                edge_list_base_filename = './Heart_Analysis/ges/PP/Heart_ges_pp_4.csv'\n",
    "                alg_list = ['ges']\n",
    "            df = pd.read_csv('./subjects/datasets/'+data_file_name)\n",
    "    \n",
    "            df = df.drop_duplicates().reset_index(drop=True)\n",
    "            A = df[sensitive_name].to_numpy()\n",
    "            X = df.to_numpy()[:,:-1]\n",
    "            Y = df.to_numpy()[:,-1].astype(int)\n",
    "    \n",
    "            num_cluster = 100\n",
    "            try :\n",
    "                with open('./'+dataset+'_Analysis/Kmean/KMean_{clus}.pkl'.format(clus=num_cluster), 'rb') as f:\n",
    "                    KMean = pickle.load(f)\n",
    "            except:\n",
    "                KMean = KMeans(n_clusters=num_cluster)\n",
    "                KMean.fit(X)\n",
    "                with open('./'+dataset+'_Analysis/Kmean/KMean_{clus}.pkl'.format(clus=num_cluster),'wb') as f:\n",
    "                    pickle.dump(KMean,f)\n",
    "    \n",
    "            ave_dist =[] \n",
    "            for i in range(KMean.n_clusters):\n",
    "                mean_dist = euclidean_distances(X[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).mean()\n",
    "                std_dist = euclidean_distances(X[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).std()\n",
    "                ave_dist.append(mean_dist + (2 * std_dist))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            graph_base = pd.read_csv(graph_base_filename)\n",
    "            edges_list_base = pd.read_csv(edge_list_base_filename)\n",
    "    \n",
    "            edges_base = edges_list_base[edges_list_base.columns[1:-1]].mean()\n",
    "            final_df_base, succ_rate_base = generate_dataset(df, graph_base, edges_base, ave_dist, KMean.cluster_centers_ ,sensitive_param, priviliged_group, unpriviliged_group)\n",
    "                                \n",
    "            A_base = final_df_base[sensitive_name].to_numpy()\n",
    "            X_base = final_df_base.to_numpy()[:,:-1]\n",
    "            Y_base = final_df_base.to_numpy()[:,-1].astype(int)\n",
    "            HP_importance_org , df_HP_org, HP_population, X_shap_org = HP_search(X_base, Y_base, A_base, dataset, program_name, num_iteration, \n",
    "                       sensitive_param, unpriviliged_group, priviliged_group, sensitive_name, \n",
    "                       start_time)\n",
    "    \n",
    "            HP_importance_results.append([dataset,'original', 'original', 'original'] + HP_importance_org.to_list())\n",
    "            \n",
    "            final_shap_list.append(X_shap_org)\n",
    "            for Algorithm in alg_list:\n",
    "                print('Algorithm', Algorithm)\n",
    "                for edge_list_filename in glob.glob('./'+dataset+'_Analysis/'+Algorithm+'/PP/*.csv'):\n",
    "                    print(edge_list_filename)\n",
    "                    file_num = int(re.findall(r'\\d+', edge_list_filename)[0])\n",
    "                    RQ1_res = np.load('./'+dataset+'_Analysis/RQ1/'+dataset+'_'+Algorithm+'_RQ1_results.npy')\n",
    "                    if RQ1_res[np.where(RQ1_res[:,1].astype(int)==file_num)[0]][0][2].astype(float)==0.0:\n",
    "                        print('No',file_num)\n",
    "                        continue\n",
    "                    try:\n",
    "                        graph_filename = './'+dataset+'_Analysis/'+Algorithm+'/DAGs/'+dataset+'_'+Algorithm+'_DAG_{file_num}.csv'.format(file_num=file_num)\n",
    "                        graph = pd.read_csv(graph_filename)\n",
    "    \n",
    "                        edges_list = pd.read_csv(edge_list_filename)\n",
    "                        if dataset=='Bank' and Algorithm=='simy':\n",
    "                            graph.columns = [i.replace('1','') for i in graph.columns]\n",
    "                            graph[graph.columns[0]] = [i.replace('1','') for i in graph[graph.columns[0]]]\n",
    "                            edges_list.columns = [i.replace('1','') for i in edges_list.columns]\n",
    "                        edges_list = edges_list[edges_list.columns[1:-1]]\n",
    "    \n",
    "                    except:\n",
    "                        print('Not a DAG! ',file_num)\n",
    "                        continue\n",
    "    \n",
    "    \n",
    "    \n",
    "                    X1_coef = edges_list.to_numpy()\n",
    "                    KMean_coef = KMeans(n_clusters=10)\n",
    "                    KMean_coef.fit(X1_coef)\n",
    "    \n",
    "                    for weights_num in range(KMean_coef.n_clusters):\n",
    "    \n",
    "                        #print('Coef ',i)\n",
    "                        weights_ind = np.random.choice(np.where(KMean_coef.labels_==weights_num)[0])\n",
    "                        edges = edges_list.iloc[weights_ind]\n",
    "                        \n",
    "                       \n",
    "                        final_df, succ_rate = generate_dataset(df, graph, edges, ave_dist, KMean.cluster_centers_ ,sensitive_param, priviliged_group, unpriviliged_group)\n",
    "                        if succ_rate!=0.0:\n",
    "                            A1 = final_df[sensitive_name].to_numpy()\n",
    "                            X1 = final_df.to_numpy()[:,:-1]\n",
    "                            Y1 = final_df.to_numpy()[:,-1].astype(int)\n",
    "    \n",
    "    \n",
    "                            HP_importance_pert, df_HP_pert ,shap_values= HP_search_population(X1, Y1, A1, X_shap_org, HP_population, program_name, num_iteration, \n",
    "                                       sensitive_param, unpriviliged_group, priviliged_group, sensitive_name, \n",
    "                                       start_time)\n",
    "    \n",
    "                            final_shap_list.append(shap_values)\n",
    "                            HP_importance_results.append([dataset,Algorithm, file_num, weights_ind] + HP_importance_pert.to_list())\n",
    "                        else:\n",
    "                            print('Zero succ')\n",
    "    \n",
    "    \n",
    "            np.save('./'+dataset+'_Analysis/RQ3/'+ dataset + '_RQ3.npy', HP_importance_results)\n",
    "            np.save('./'+dataset+'_Analysis/RQ3/'+ dataset + '_RQ3_shap_values.npy', final_shap_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f1661-be9d-4d86-8a8b-33eac3b6aeaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
