{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7240f8-439b-4320-ba33-7f221b42ccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm pc\n",
      "../Adult_Analysis/pc/PP\\Adult_pc_pp_107.csv\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys,os\n",
    "sys.path.append(\"./subjects/\")\n",
    "subject_path = os.path.join(os.getcwd(), '..', 'subjects')\n",
    "sys.path.append(os.path.abspath(subject_path))\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random, math\n",
    "import tensorflow_probability as tfb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import  euclidean_distances\n",
    "from Utils_Functions import KLdivergence\n",
    "from sklearn.feature_selection import SelectKBest, SelectFpr,SelectPercentile \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from aif360.sklearn.metrics import equal_opportunity_difference,average_odds_difference\n",
    "import glob\n",
    "import re\n",
    "import argparse\n",
    "    \n",
    "def generate_dataset_I(data, graph, edges, ave_dist, centroids,sens_index, priv_group, unpriv_group):\n",
    "\n",
    "    dataset_types = [str(data[i].dtype) for i in data.columns]\n",
    "    succ_generated = 0\n",
    "    generation_coef = 10\n",
    "    graph_dic ={}\n",
    "    for i in graph.columns[1:]:\n",
    "        if np.where(graph[i])[0].shape[0]==0:\n",
    "            graph_dic[i]=None\n",
    "        else:\n",
    "            graph_dic[i]= graph['Unnamed: 0'][np.where(graph[i])[0]].values\n",
    "\n",
    "    final_df = pd.DataFrame(columns = data.columns) \n",
    "    trial = 0\n",
    "    not_interesting = False\n",
    "    while final_df.shape[0]<data.shape[0]:\n",
    "\n",
    "        if trial > 20:\n",
    "            not_interesting = True\n",
    "            return None , 0.0#succ_generated/( trial *  data.shape[0]*generation_coef) \n",
    "        df_new_dic ={}\n",
    "        for edge in graph.sum().index[np.where(graph.sum()==0)[0]]:\n",
    "            df_new_dic[edge] = np.random.choice(np.unique(data[edge]), size = data.shape[0]*generation_coef) \n",
    "\n",
    "        statring_atts = graph.sum().index[np.where(graph.sum()==0)[0]]\n",
    "        \n",
    "        while statring_atts.shape[0] != graph['Unnamed: 0'].shape[0]:\n",
    "            \n",
    "            for att in graph_dic.keys():\n",
    "                if att not in statring_atts:\n",
    "                    if 0 in  [1 if graph_dic[att][i] in statring_atts else 0 for i in range(graph_dic[att].shape[0])]:\n",
    "\n",
    "                        continue\n",
    "                    else:\n",
    "                        edge_logits = 0\n",
    "                        \n",
    "                        for cause in graph_dic[att]:\n",
    "                            edge_logits += (edges[cause+att] * df_new_dic[cause])\n",
    "                        if np.unique(data[att]).shape[0]==2:\n",
    "                            df_new_dic[att] =  tfb.distributions.Bernoulli(logits=edge_logits + edges[att+'0'] ).sample().numpy()\n",
    "                        elif 'float' in dataset_types[np.where(data.columns==att)[0][0]]:\n",
    "\n",
    "                            df_new_dic[att] =  tfb.distributions.Normal(loc=(edge_logits+ edges[att+'0']), scale= edges['sigma_h']).sample().numpy()\n",
    "                        \n",
    "                        else:    \n",
    "                            df_new_dic[att] =  tfb.distributions.Poisson(rate=tf.exp(edge_logits+ edges[att+'0']) ).sample().numpy()  \n",
    "\n",
    "                        statring_atts = np.append(statring_atts,att) \n",
    "\n",
    "        new_df = pd.DataFrame(columns = data.columns)\n",
    "        for col in new_df.columns:\n",
    "            new_df[col] = df_new_dic[col]\n",
    "        \n",
    "        ind_inf = np.unique(np.where(new_df>data.max())[0])\n",
    "        new_df.drop(ind_inf,axis=0,inplace=True)\n",
    "        for col in range(new_df.columns.shape[0]):\n",
    "            new_df[new_df.columns[col]]=new_df[new_df.columns[col]].astype(dataset_types[col])\n",
    "        if new_df.shape[0]<1:\n",
    "            return None , 0.0\n",
    "        X2 = new_df.to_numpy()[:,:-1]\n",
    "        Y2 = new_df.to_numpy()[:,-1]\n",
    "        dist = euclidean_distances(X2, centroids)\n",
    "        succ_generated += new_df.iloc[np.where((ave_dist>=dist).sum(1)>0)].shape[0]\n",
    "        final_df = pd.concat([final_df,new_df.iloc[np.where((ave_dist>=dist).sum(1)>0)[0]]]).reset_index(drop=True)\n",
    "        final_df = final_df.drop_duplicates()\n",
    "\n",
    "        #print(succ_generated,trial)\n",
    "        if succ_generated<10:\n",
    "            return None, 0.0\n",
    "            \n",
    "        trial += 1\n",
    "   \n",
    "    #final_df = final_df.astype(int)\n",
    "    succ_rate = succ_generated/( trial *  data.shape[0]*generation_coef) \n",
    "    final_df = final_df.sample(n= data.shape[0])\n",
    "    Y2 = final_df.to_numpy()[:,-1]\n",
    "    if (Y2.sum()/Y2.shape[0]< 0.06) or (Y2.sum()/Y2.shape[0]> 0.95):\n",
    "        return None, 0.0#succ_generated/( trial *  data.shape[0]*generation_coef)   \n",
    "    if priv_group not in final_df[final_df.columns[sens_index]].values or unpriv_group not in final_df[final_df.columns[sens_index]].values:\n",
    "        return None, 0.0\n",
    "\n",
    "    return final_df, succ_rate\n",
    "\n",
    "def eod(y_true, y_pred, sens, priv, unpriv):\n",
    "    \n",
    "    ind_priv = np.where(sens==priv)[0]\n",
    "    ytru_rate_priv = np.where(y_true[ind_priv]==1)[0].shape[0]\n",
    "    pred_rate_priv = np.where(y_pred[ind_priv]==1)[0].shape[0]\n",
    "    tpr_priv = ytru_rate_priv/ind_priv.shape[0] - pred_rate_priv/ind_priv.shape[0]\n",
    "    ind_unpriv = np.where(sens==unpriv)[0]\n",
    "    ytru_rate_unpriv = np.where(y_true[ind_unpriv]==1)[0].shape[0]\n",
    "    pred_rate_unpriv = np.where(y_pred[ind_unpriv]==1)[0].shape[0]\n",
    "    tpr_unpriv = ytru_rate_unpriv/ind_unpriv.shape[0] - pred_rate_unpriv/ind_unpriv.shape[0]\n",
    "    return round(tpr_priv - tpr_unpriv , 3)\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset', type=str, help='Select the dataset')\n",
    "# args = parser.parse_args()\n",
    "# dataset=args.dataset\n",
    "dataset ='Adult'\n",
    "for dataset in [dataset]:\n",
    "    if dataset == 'Adult':\n",
    "        sens_index = 7\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0\n",
    "        data_file_name = 'adult_org-Copy1.csv'\n",
    "    if dataset == 'Compas':\n",
    "        sens_index = 1\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0\n",
    "        data_file_name = 'compas-Copy1'\n",
    "    if dataset == 'Bank':\n",
    "        sens_index = 0\n",
    "        priv_group = 5\n",
    "        unpriv_group = 3\n",
    "        data_file_name = 'bank'\n",
    "    if dataset == 'Heart':\n",
    "        sens_index = 0\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0 \n",
    "        data_file_name = 'heart_processed_1'\n",
    "    if dataset == 'Law':\n",
    "        sens_index = 1\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0\n",
    "        data_file_name = 'law.csv'\n",
    "\n",
    "    if dataset == 'Student':\n",
    "        sens_index = 0\n",
    "        priv_group = 1\n",
    "        unpriv_group = 0  \n",
    "        data_file_name = 'students-processed_2'\n",
    "    df = pd.read_csv('../subjects/datasets/'+data_file_name)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    X1 = df.to_numpy()[:,:-1]\n",
    "    Y1 = df.to_numpy()[:,-1].astype(int)\n",
    "    num_cluster = 100\n",
    "    try :\n",
    "        with open('../'+dataset+'_Analysis/Kmean/KMean_{clus}.pkl'.format(clus=num_cluster), 'rb') as f:\n",
    "            KMean = pickle.load(f)\n",
    "    except:\n",
    "        KMean = KMeans(n_clusters=num_cluster)\n",
    "        KMean.fit(X1)\n",
    "        with open('../'+dataset+'_Analysis/Kmean/KMean_{clus}.pkl'.format(clus=num_cluster),'wb') as f:\n",
    "            pickle.dump(KMean,f)\n",
    "\n",
    "    ave_dist =[] \n",
    "    for i in range(KMean.n_clusters):\n",
    "        mean_dist = euclidean_distances(X1[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).mean()\n",
    "        std_dist = euclidean_distances(X1[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).std()\n",
    "        ave_dist.append(mean_dist + (2 * std_dist))\n",
    "        #ave_dist.append(euclidean_distances(X1[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).max())\n",
    "\n",
    "    for Algorithm in ['pc','ges','simy']:\n",
    "        print('Algorithm', Algorithm)\n",
    "        for edge_list_filename in glob.glob('../'+dataset+'_Analysis/'+Algorithm+'/PP/*.csv'):\n",
    "            print(edge_list_filename)\n",
    "            file_num = int(re.findall(r'\\d+', edge_list_filename)[0])\n",
    "            RQ1_res = np.load('../'+dataset+'_Analysis/RQ1/'+dataset+'_'+Algorithm+'_RQ1_results.npy')\n",
    "            if RQ1_res[np.where(RQ1_res[:,1].astype(int)==file_num)[0]][0][2].astype(float)==0.0:\n",
    "                continue\n",
    "            try:\n",
    "                graph_filename = '../'+dataset+'_Analysis/'+Algorithm+'/DAGs/'+dataset+'_'+Algorithm+'_DAG_{file_num}.csv'.format(file_num=file_num)\n",
    "                graph = pd.read_csv(graph_filename)\n",
    "                #edge_list_filename = './'+dataset+'_Analysis/'+Algorithm+'/PP/'+dataset+'_'+Algorithm+'_pp_{file_num}.csv'.format(file_num=file_num)\n",
    "                edges_list = pd.read_csv(edge_list_filename)\n",
    "                if dataset=='Bank' and Algorithm=='simy':\n",
    "                    graph.columns = [i.replace('1','') for i in graph.columns]\n",
    "                    graph[graph.columns[0]] = [i.replace('1','') for i in graph[graph.columns[0]]]\n",
    "                    edges_list.columns = [i.replace('1','') for i in edges_list.columns]\n",
    "                edges_list = edges_list[edges_list.columns[1:-1]]\n",
    "\n",
    "            except:\n",
    "                print('Not a DAG! ',file_num)\n",
    "                continue\n",
    "\n",
    "            X1_coef = edges_list.to_numpy()\n",
    "            KMean_coef = KMeans(n_clusters=10)\n",
    "            KMean_coef.fit(X1_coef)\n",
    "            SelectKBest_final_EOD=[]\n",
    "            SelectFpr_final_EOD=[]\n",
    "            SelectPercentile_final_EOD=[]\n",
    "            None_model_final_EOD = []\n",
    "            SelectKBest_final_AOD=[]\n",
    "            SelectFpr_final_AOD=[]\n",
    "            SelectPercentile_final_AOD=[]\n",
    "            None_model_final_AOD = []\n",
    "            drop_final_EOD = []\n",
    "            drop_final_AOD = []\n",
    "\n",
    "            for i in range(KMean_coef.n_clusters):\n",
    "                #print('Coef ',i)\n",
    "                edges = edges_list.iloc[np.random.choice(np.where(KMean_coef.labels_==i)[0])]\n",
    "                SelectKBest_EOD=[]\n",
    "                SelectFpr_EOD=[]\n",
    "                SelectPercentile_EOD=[]\n",
    "                SelectKBest_AOD=[]\n",
    "                SelectFpr_AOD=[]\n",
    "                SelectPercentile_AOD=[]\n",
    "                drop_EOD = []\n",
    "                drop_AOD = []\n",
    "                None_model = []\n",
    "                for j in range(10):\n",
    "                    final_df, succ_rate = generate_dataset_I(df, graph, edges, ave_dist, KMean.cluster_centers_ ,sens_index, priv_group, unpriv_group)\n",
    "                    if succ_rate==0.0 :\n",
    "                        continue\n",
    "\n",
    "                    X2 = final_df.to_numpy()[:,:-1]\n",
    "                    Y2 = final_df.to_numpy()[:,-1].astype(int)\n",
    "                    A2 = X2[:,sens_index]\n",
    "\n",
    "                    if priv_group not in A2 or unpriv_group not in A2:\n",
    "                        print('No sens group')\n",
    "                        continue\n",
    "                    model = LogisticRegression()\n",
    "                    model.fit(X2,Y2)\n",
    "                    \n",
    "                    preds_None = model.predict(X2)\n",
    "                    acc_None = accuracy_score(Y2,preds_None)\n",
    "                    f1_None = f1_score(Y2,preds_None)\n",
    "                    AOD_None = round(average_odds_difference(Y2, preds_None,prot_attr=A2,priv_group=priv_group ),3)\n",
    "                    EOD_None = eod(Y2, preds_None,sens=A2, priv=priv_group, unpriv=unpriv_group)        \n",
    "                    None_model.append([EOD_None,acc_None,f1_None])\n",
    "                    \n",
    "\n",
    "                    #print('Success rate DAG', file_num, succ_rate)\n",
    "                    for transformer in ['SelectKBest', 'SelectFpr','SelectPercentile' ,'drop']:\n",
    "\n",
    "\n",
    "                        if transformer == 'SelectKBest':\n",
    "                            Kbest = SelectKBest( k=int(X2.shape[1]/2))\n",
    "                            X_new = Kbest.fit_transform(X2, Y2)\n",
    "                        elif transformer == 'SelectFpr':\n",
    "                            sfpr =  SelectFpr(alpha=0.01)\n",
    "                            X_new = sfpr.fit_transform(X2, Y2)\n",
    "\n",
    "                        elif transformer == 'SelectPercentile':\n",
    "                            percentile =  SelectPercentile(percentile=10)\n",
    "                            X_new = percentile.fit_transform(X2, Y2)\n",
    "                        elif transformer == 'drop':                        \n",
    "                            X_new =  np.delete(X2,sens_index,axis=1)\n",
    "\n",
    "\n",
    "                        model = LogisticRegression()\n",
    "                        model.fit(X_new,Y2)\n",
    "                        preds_new = model.predict(X_new)\n",
    "                        acc_temp = accuracy_score(Y2,preds_new)\n",
    "                        f1_temp = f1_score(Y2,preds_new)\n",
    "                        AOD = round(average_odds_difference(Y2, preds_new,prot_attr=A2,priv_group=priv_group ),3)\n",
    "                        EOD = eod(Y2, preds_new,sens=A2, priv=priv_group, unpriv=unpriv_group)\n",
    "                        acc_diff = round(acc_temp - acc_None,2)\n",
    "                        f1_diff = round(f1_temp- f1_None,2)\n",
    "                        EOD_diff = EOD - EOD_None\n",
    "                        AOD_diff = AOD - AOD_None\n",
    "                        EOD_diff_abs = abs(EOD) - abs(EOD_None)\n",
    "                        AOD_diff_abs = abs(AOD) - abs(AOD_None)\n",
    "                        #print([ EOD_diff, acc_diff, f1_diff])\n",
    "                        if transformer == 'SelectKBest':\n",
    "                            SelectKBest_EOD.append([ EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                            SelectKBest_AOD.append([ AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "\n",
    "                        elif transformer == 'SelectFpr':\n",
    "                            SelectFpr_EOD.append([EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                            SelectFpr_AOD.append([AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "\n",
    "                        elif transformer == 'SelectPercentile':\n",
    "                            SelectPercentile_EOD.append([EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                            SelectPercentile_AOD.append([AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "\n",
    "                        elif transformer == 'drop':\n",
    "                            drop_EOD.append([EOD,EOD_None,EOD_diff,EOD_diff_abs, acc_diff, f1_diff])\n",
    "                            drop_AOD.append([AOD,AOD_None,AOD_diff,AOD_diff_abs, acc_diff, f1_diff])\n",
    "\n",
    "                if len(SelectKBest_EOD)>0:\n",
    "                    SelectKBest_final_EOD.append(np.mean(SelectKBest_EOD,axis=0))\n",
    "                if len(SelectKBest_AOD)>0:\n",
    "                    SelectKBest_final_AOD.append(np.mean(SelectKBest_AOD,axis=0)) \n",
    "\n",
    "                if len(SelectFpr_EOD)>0:\n",
    "                    SelectFpr_final_EOD.append(np.mean(SelectFpr_EOD,axis=0))\n",
    "                if len(SelectFpr_AOD)>0:\n",
    "                    SelectFpr_final_AOD.append(np.mean(SelectFpr_AOD,axis=0))\n",
    "\n",
    "                if len(SelectPercentile_EOD)>0:    \n",
    "                    SelectPercentile_final_EOD.append(np.mean(SelectPercentile_EOD,axis=0))\n",
    "                if len(SelectPercentile_AOD)>0:    \n",
    "                    SelectPercentile_final_AOD.append(np.mean(SelectPercentile_AOD,axis=0))\n",
    "\n",
    "                if len(drop_EOD)>0:    \n",
    "                    drop_final_EOD.append(np.mean(drop_EOD,axis=0))\n",
    "                if len(drop_AOD)>0:    \n",
    "                    drop_final_AOD.append(np.mean(drop_AOD,axis=0))\n",
    "\n",
    "    #         print(np.array(SelectKBest_final_EOD).min(),np.array(SelectKBest_final_EOD).max())\n",
    "    #         print(np.array(SelectKBest_final_AOD).min(),np.array(SelectKBest_final_AOD).max())\n",
    "\n",
    "    #         print(np.array(SelectFpr_final_EOD).min(),np.array(SelectFpr_final_EOD).max())\n",
    "    #         print(np.array(SelectFpr_final_AOD).min(),np.array(SelectFpr_final_AOD).max())\n",
    "\n",
    "    #         print(np.array(SelectPercentile_final_EOD).min(),np.array(SelectPercentile_final_EOD).max())\n",
    "    #         print(np.array(SelectPercentile_final_AOD).min(),np.array(SelectPercentile_final_AOD).max())\n",
    "            np.save('../'+dataset+'_Analysis/RQ2/'+Algorithm+'_SelectKbest_EOD_' + str(file_num)+'.npy',SelectKBest_final_EOD)\n",
    "            np.save('../'+dataset+'_Analysis/RQ2/'+Algorithm+'_SelectKbest_AOD_' + str(file_num)+'.npy',SelectKBest_final_AOD)\n",
    "\n",
    "            np.save('../'+dataset+'_Analysis/RQ2/'+Algorithm+'_SelectFpr_EOD_' + str(file_num)+'.npy',SelectFpr_final_EOD)\n",
    "            np.save('../'+dataset+'_Analysis/RQ2/'+Algorithm+'_SelectFpr_AOD_' + str(file_num)+'.npy',SelectFpr_final_AOD)\n",
    "\n",
    "            np.save('../'+dataset+'_Analysis/RQ2/'+Algorithm+'_SelectPercentile_EOD_' + str(file_num)+'.npy',SelectPercentile_final_EOD)\n",
    "            np.save('../'+dataset+'_Analysis/RQ2/'+Algorithm+'_SelectPercentile_AOD_' + str(file_num)+'.npy',SelectPercentile_final_AOD)\n",
    "\n",
    "            np.save('../'+dataset+'_Analysis/RQ2/'+Algorithm+'_drop_EOD_' + str(file_num)+'.npy',drop_final_EOD)\n",
    "            np.save('../'+dataset+'_Analysis/RQ2/'+Algorithm+'_drop_AOD_' + str(file_num)+'.npy',drop_final_AOD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dce894-1e3f-4fb8-b4de-dc6f192d24b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
