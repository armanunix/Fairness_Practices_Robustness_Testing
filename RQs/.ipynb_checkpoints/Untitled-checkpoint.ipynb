{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7240f8-439b-4320-ba33-7f221b42ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"./subjects/\")\n",
    "subject_path = os.path.join(os.getcwd(), '..', 'subjects')\n",
    "sys.path.append(os.path.abspath(subject_path))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re, random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.cluster import KMeans\n",
    "import shap\n",
    "import xml_parser\n",
    "import xml_parser_domains\n",
    "from mutation import mutate, clip_LR\n",
    "from fairlearn.metrics import equalized_odds_difference\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from Utils_Functions import generate_dataset, eod\n",
    "import copy, pickle, glob\n",
    "from sklearn.metrics.pairwise import  euclidean_distances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def HP_search_population(X, Y, A,X_shap, HP_population, program_name, max_iter, \n",
    "               sensitive_param, unpriviliged_group, priviliged_group, sensitive_name, start_time):\n",
    "    time1 = time.time()\n",
    "    df_input = pd.DataFrame()\n",
    "    \n",
    "    if(program_name == \"LogisticRegression\"):\n",
    "        import Logistic_Regression_Mitigation\n",
    "        original_program = Logistic_Regression_Mitigation.logisticRegressionOriginal\n",
    "        input_program_tree = 'logistic_regression_Params.xml'\n",
    "        num_args = 15\n",
    "    \n",
    "\n",
    "    for arr_clip in HP_population:\n",
    "\n",
    "        res1, LR, inp_valid1, score_org, pred_org = original_program(arr_clip, X, X, Y, Y)\n",
    "\n",
    "\n",
    "        EOD_A = eod(Y, pred_org,sens=A, priv=priviliged_group, unpriv=unpriviliged_group)\n",
    "        \n",
    "        \n",
    "        df_input = df_input.append([EOD_A])\n",
    "        #print(\"---------------------------------------------------------\")                 \n",
    "        \n",
    "        #print('Iteration took ', time.time() - time2)\n",
    "\n",
    "    \n",
    "\n",
    "    df_input.columns=['EOD']\n",
    "    #label = np.digitize(df_input['EOD'],bins=bins)\n",
    "    Y_shap = df_input['EOD']\n",
    "    ENCV =  ElasticNetCV(cv=10).fit(X_shap,Y_shap) \n",
    "    explainer = shap.LinearExplainer(ENCV, X_shap)\n",
    "    shap_values = explainer.shap_values(X_shap)\n",
    "    HP_importance = X_shap.columns[np.argsort(np.abs(shap_values).mean(0))[::-1]]\n",
    "    return HP_importance, df_input, shap_values\n",
    "\n",
    "\n",
    "def HP_search(X, Y, A, dataset, program_name, max_iter, \n",
    "               sensitive_param, unpriviliged_group, priviliged_group, sensitive_name, start_time):\n",
    "    time1 = time.time()\n",
    "    df_input = pd.DataFrame()\n",
    "    default_acc = 0.0\n",
    "    HP_population = []\n",
    "    \n",
    "      \n",
    "    if(program_name == \"LogisticRegression\"):\n",
    "        import Logistic_Regression_Mitigation\n",
    "        original_program = Logistic_Regression_Mitigation.logisticRegressionOriginal\n",
    "        input_program_tree = 'logistic_regression_Params.xml'\n",
    "        num_args = 15\n",
    "    \n",
    "\n",
    "    arr_min, arr_max, arr_type, arr_default = xml_parser_domains.xml_parser_domains(input_program_tree, num_args)\n",
    "\n",
    "    seeds = []\n",
    "    promising_inputs_EOD = []\n",
    "    promising_metric_EOD = []\n",
    "\n",
    "   \n",
    "    EOD_max = 0\n",
    "    for counter in range(max_iter):\n",
    "        time2 = time.time()\n",
    "        #print('counter',counter)  \n",
    "\n",
    "        inp = mutate( arr_max, arr_min, arr_type, arr_default, promising_inputs_EOD, counter)\n",
    "\n",
    "        if re.match(r'Logistic', program_name):\n",
    "            arr_clip, features = clip_LR(inp)\n",
    "        \n",
    "        if arr_clip in seeds:\n",
    "            #print('Duplicated Seed!')\n",
    "            continue\n",
    "        else:\n",
    "            seeds.append(arr_clip)\n",
    "        EOD_avg = []\n",
    "\n",
    "        \n",
    "        X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(X, Y, A)\n",
    "        res1, LR, inp_valid1, score_org, pred_org = original_program(arr_clip, X_train, X_test, \n",
    "                                                                            y_train, y_test)\n",
    "        if not res1:\n",
    "            #print('training failed')\n",
    "            continue\n",
    "        else:\n",
    "            failed_flag = False\n",
    "\n",
    "        # tolerate at most 2% of accuracy loss        \n",
    "        if counter == 0:\n",
    "            default_acc = score_org\n",
    "\n",
    "        if (score_org < (default_acc - 0.05)) :\n",
    "            #print('Acc low')\n",
    "            continue\n",
    "        res1, LR1, inp_valid1, score_org, pred_org = original_program(arr_clip, X, X, Y, Y)\n",
    "        EOD_A = eod(Y, pred_org,sens=A, priv=priviliged_group, unpriv=unpriviliged_group)\n",
    "#         pred = LR.predict(X)\n",
    "#         # A original model , B mitigated mode   \n",
    "#         EOD_avg.append(eod(Y, pred,sens=A, priv=priviliged_group, unpriv=unpriviliged_group))\n",
    "#         print(EOD_avg[-1])\n",
    "#         input()\n",
    "#         EOD_A = np.mean(EOD_avg)\n",
    "#         print(EOD_A)\n",
    "#         print('------')\n",
    "        #print(eod_test_A, EOD_A)\n",
    "        if abs(EOD_A) > EOD_max:\n",
    "            EOD_max = abs(EOD_A)\n",
    "            #print('Intresting EOD')\n",
    "            intresting_EOD = 1\n",
    "            promising_inputs_EOD.append(inp)\n",
    "        elif abs(EOD_A) > 0.1:\n",
    "            #print('Intresting EOD')\n",
    "            intresting_EOD = 1\n",
    "            promising_inputs_EOD.append(inp)\n",
    "        else:\n",
    "            intresting_EOD = 0        \n",
    "        df_input = pd.concat([df_input,pd.DataFrame(arr_clip + [EOD_A]).T] , sort=False)\n",
    "        HP_population.append(arr_clip)\n",
    "        #print(\"---------------------------------------------------------\")                 \n",
    "        if time.time() - time1 >= time_out:\n",
    "            break\n",
    "        #print('Iteration took ', time.time() - time2)\n",
    "\n",
    "    \n",
    "    df_input.columns = ['solver', 'penalty', 'dual', 'tol', 'C', 'fit_intercept', 'intercept_scaling', \n",
    "                        'max_iteration', 'multi_class', 'l1_ratio', 'class_weight', 'random_state', 'verbose',\n",
    "                        'warm_start', 'n_jobs','EOD']\n",
    "    df_input.reset_index(drop=True,inplace=True)\n",
    "    df_input.to_csv('./'+dataset+'_Analysis/RQ3/'+ dataset + '_base_HP_search.csv', index=False)\n",
    "    ind_0 = df_input.sort_values(by='EOD').head(200).index.to_list()\n",
    "    ind_1 = df_input.sort_values(by='EOD').tail(200).index.to_list()\n",
    "    HP_population = np.array(HP_population)[ind_0 + ind_1]\n",
    "    np.save('./'+dataset+'_Analysis/RQ3/'+ dataset + '_HP_population.csv',HP_population)\n",
    "    df_inp = df_input.iloc[ind_0+ind_1]\n",
    "#     df_inp.loc[ind_0,'EOD'] = 0\n",
    "#     df_inp.loc[ind_1,'EOD'] = 1\n",
    "    df_inp.reset_index(drop=True,inplace=True)\n",
    "    le =  LabelEncoder()\n",
    "    cat_columns = ['solver', 'penalty', 'dual','fit_intercept', 'multi_class', 'warm_start']\n",
    "    for col in cat_columns:\n",
    "        df_inp[col] = le.fit_transform(df_inp[col])\n",
    "    columns = []\n",
    "    for col in df_inp.columns[:-1]:\n",
    "        if df_inp[col].unique().shape[0] > 1 :\n",
    "            columns.append(col)           \n",
    "    label = df_inp['EOD'].to_numpy()\n",
    "#     bins = np.histogram(df_inp['EOD'], bins=3)[1][1:-1].tolist()\n",
    "#     label = np.digitize(df_inp['EOD'],bins=bins)\n",
    "\n",
    "    df_inp = df_inp[columns]\n",
    "    for ind in np.where(df_inp.isna().sum()>0):\n",
    "        df_inp[df_inp.columns[ind]+'_isna']=0\n",
    "        df_inp.loc[np.where(df_inp[df_inp.columns[ind]].isna()==True)[0],df_inp.columns[ind]+'_isna'] = 1\n",
    "        df_inp.loc[np.where(df_inp[df_inp.columns[ind]].isna()==True)[0], df_inp.columns[ind]] = 0\n",
    "    df_inp['label'] = label\n",
    "    X_shap = df_inp[df_inp.columns[:-1]]\n",
    "    Y_shap = df_inp['label']\n",
    "    features = X_shap.columns\n",
    "    cat_features = []\n",
    "    for cat in X_shap.select_dtypes(exclude=\"number\"):\n",
    "        cat_features.append(cat)\n",
    "        X_shap[cat] = X_shap[cat].astype(\"category\").cat.codes.astype(\"category\")\n",
    "    ENCV =  ElasticNetCV(cv=10).fit(X_shap,Y_shap) \n",
    "    explainer = shap.LinearExplainer(ENCV, X_shap)\n",
    "    shap_values = explainer.shap_values(X_shap)\n",
    "    HP_importance = X_shap.columns[np.argsort(np.abs(shap_values).mean(0))[::-1]]\n",
    "    return HP_importance, df_input, HP_population, X_shap\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    time_out = 60 * 60\n",
    "    original_model = True\n",
    "    save_model = False\n",
    "    num_iteration =  1000000\n",
    "    program_name=\"LogisticRegression\"\n",
    "\n",
    " \n",
    "    for dataset in ['Adult']:\n",
    "        HP_importance_results = []\n",
    "        final_shap_list = []\n",
    "        #data_config.keys(): \n",
    "        if dataset == 'Adult':\n",
    "            sensitive_param = 7\n",
    "            sensitive_name = 'gender'\n",
    "            priviliged_group = 1  #male\n",
    "            unpriviliged_group = 0#female\n",
    "            favorable_label  = 1.0\n",
    "            unfavorable_label = 0.0\n",
    "            data_file_name = 'adult_org-Copy1.csv'\n",
    "            graph_base_filename ='../Adult_Analysis/ges/DAGs/Adult_ges_DAG_1.csv'\n",
    "            edge_list_base_filename = '../Adult_Analysis/ges/PP/Adult_ges_pp_1.csv'\n",
    "            alg_list = ['ges','simy']\n",
    "        if dataset == 'Compas':\n",
    "            sensitive_param = 1\n",
    "            sensitive_name = 'race'\n",
    "            priviliged_group = 1  #male\n",
    "            unpriviliged_group = 0#female\n",
    "            favorable_label  = 1.0\n",
    "            unfavorable_label = 0.0\n",
    "            data_file_name = 'compas-Copy1'\n",
    "            graph_base_filename ='../Compas_Analysis/pc/DAGs/Compas_pc_DAG_13.csv'\n",
    "            edge_list_base_filename = '../Compas_Analysis/pc/PP/Compas_pc_pp_13.csv'\n",
    "            alg_list = ['ges','pc']\n",
    "        if dataset == 'Bank':\n",
    "            sensitive_param = 0\n",
    "            sensitive_name = 'age'\n",
    "            priviliged_group = 5  #male\n",
    "            unpriviliged_group = 3#female\n",
    "            favorable_label  = 1.0\n",
    "            unfavorable_label = 0.0\n",
    "            data_file_name = 'bank'\n",
    "            graph_base_filename ='../Bank_Analysis/ges/DAGs/Bank_ges_DAG_8.csv'\n",
    "            edge_list_base_filename = '../Bank_Analysis/ges/PP/Bank_ges_pp_8.csv'\n",
    "            alg_list = ['ges']\n",
    "        if dataset == 'Law':\n",
    "            sensitive_param = 0\n",
    "            sensitive_name = 'sex'\n",
    "            priviliged_group = 1  #male\n",
    "            unpriviliged_group = 0#female\n",
    "            favorable_label  = 1.0\n",
    "            unfavorable_label = 0.0\n",
    "            data_file_name = 'law.csv'\n",
    "            graph_base_filename ='../Law_Analysis/ges/DAGs/Law_ges_DAG_15.csv'\n",
    "            edge_list_base_filename = '../Law_Analysis/ges/PP/Law_ges_pp_15.csv'\n",
    "            alg_list = ['ges','simy']\n",
    "        if dataset == 'Student':\n",
    "            sensitive_param = 0\n",
    "            sensitive_name = 'sex'\n",
    "            priviliged_group = 1  #male\n",
    "            unpriviliged_group = 0#female\n",
    "            favorable_label  = 1.0\n",
    "            unfavorable_label = 0.0\n",
    "            data_file_name = 'students-processed_2'\n",
    "            graph_base_filename ='../Student_Analysis/pc/DAGs/Student_pc_DAG_1.csv'\n",
    "            edge_list_base_filename = '../Student_Analysis/pc/PP/Student_pc_pp_1.csv'\n",
    "            alg_list = ['simy','pc']\n",
    "        if dataset == 'Heart':\n",
    "            sensitive_param = 0\n",
    "            sensitive_name = 'sex'\n",
    "            priviliged_group = 1  #male\n",
    "            unpriviliged_group = 0#female\n",
    "            favorable_label  = 1.0\n",
    "            unfavorable_label = 0.0\n",
    "            data_file_name = 'heart_processed_1'\n",
    "            graph_base_filename ='../Heart_Analysis/ges/DAGs/Heart_ges_DAG_4.csv'\n",
    "            edge_list_base_filename = '../Heart_Analysis/ges/PP/Heart_ges_pp_4.csv'\n",
    "            alg_list = ['ges']\n",
    "        df = pd.read_csv('../subjects/datasets/'+data_file_name)\n",
    "\n",
    "        df = df.drop_duplicates().reset_index(drop=True)\n",
    "        A = df[sensitive_name].to_numpy()\n",
    "        X = df.to_numpy()[:,:-1]\n",
    "        Y = df.to_numpy()[:,-1].astype(int)\n",
    "\n",
    "        num_cluster = 100\n",
    "        try :\n",
    "            with open('../'+dataset+'_Analysis/Kmean/KMean_{clus}.pkl'.format(clus=num_cluster), 'rb') as f:\n",
    "                KMean = pickle.load(f)\n",
    "        except:\n",
    "            KMean = KMeans(n_clusters=num_cluster)\n",
    "            KMean.fit(X)\n",
    "            with open('../'+dataset+'_Analysis/Kmean/KMean_{clus}.pkl'.format(clus=num_cluster),'wb') as f:\n",
    "                pickle.dump(KMean,f)\n",
    "\n",
    "        ave_dist =[] \n",
    "        for i in range(KMean.n_clusters):\n",
    "            mean_dist = euclidean_distances(X[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).mean()\n",
    "            std_dist = euclidean_distances(X[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).std()\n",
    "            ave_dist.append(mean_dist + (2 * std_dist))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        graph_base = pd.read_csv(graph_base_filename)\n",
    "        edges_list_base = pd.read_csv(edge_list_base_filename)\n",
    "\n",
    "        edges_base = edges_list_base[edges_list_base.columns[1:-1]].mean()\n",
    "        final_df_base, succ_rate_base = generate_dataset(df, graph_base, edges_base, ave_dist, KMean.cluster_centers_ ,sensitive_param, priviliged_group, unpriviliged_group)\n",
    "                            \n",
    "        A_base = final_df_base[sensitive_name].to_numpy()\n",
    "        X_base = final_df_base.to_numpy()[:,:-1]\n",
    "        Y_base = final_df_base.to_numpy()[:,-1].astype(int)\n",
    "        HP_importance_org , df_HP_org, HP_population, X_shap_org = HP_search(X_base, Y_base, A_base, dataset, program_name, num_iteration, \n",
    "                   sensitive_param, unpriviliged_group, priviliged_group, sensitive_name, \n",
    "                   start_time)\n",
    "\n",
    "        HP_importance_results.append([dataset,'original', 'original', 'original'] + HP_importance_org.to_list())\n",
    "        \n",
    "        final_shap_list.append(X_shap_org)\n",
    "        for Algorithm in alg_list:\n",
    "            print('Algorithm', Algorithm)\n",
    "            for edge_list_filename in glob.glob('../'+dataset+'_Analysis/'+Algorithm+'/PP/*.csv'):\n",
    "                print(edge_list_filename)\n",
    "                file_num = int(re.findall(r'\\d+', edge_list_filename)[0])\n",
    "                RQ1_res = np.load('../'+dataset+'_Analysis/RQ1/'+dataset+'_'+Algorithm+'_RQ1_results.npy')\n",
    "                if RQ1_res[np.where(RQ1_res[:,1].astype(int)==file_num)[0]][0][2].astype(float)==0.0:\n",
    "                    print('No',file_num)\n",
    "                    continue\n",
    "                try:\n",
    "                    graph_filename = '../'+dataset+'_Analysis/'+Algorithm+'/DAGs/'+dataset+'_'+Algorithm+'_DAG_{file_num}.csv'.format(file_num=file_num)\n",
    "                    graph = pd.read_csv(graph_filename)\n",
    "\n",
    "                    edges_list = pd.read_csv(edge_list_filename)\n",
    "                    if dataset=='Bank' and Algorithm=='simy':\n",
    "                        graph.columns = [i.replace('1','') for i in graph.columns]\n",
    "                        graph[graph.columns[0]] = [i.replace('1','') for i in graph[graph.columns[0]]]\n",
    "                        edges_list.columns = [i.replace('1','') for i in edges_list.columns]\n",
    "                    edges_list = edges_list[edges_list.columns[1:-1]]\n",
    "\n",
    "                except:\n",
    "                    print('Not a DAG! ',file_num)\n",
    "                    continue\n",
    "\n",
    "\n",
    "\n",
    "                X1_coef = edges_list.to_numpy()\n",
    "                KMean_coef = KMeans(n_clusters=10)\n",
    "                KMean_coef.fit(X1_coef)\n",
    "\n",
    "                for weights_num in range(KMean_coef.n_clusters):\n",
    "\n",
    "                    #print('Coef ',i)\n",
    "                    weights_ind = np.random.choice(np.where(KMean_coef.labels_==weights_num)[0])\n",
    "                    edges = edges_list.iloc[weights_ind]\n",
    "                    \n",
    "                   \n",
    "                    final_df, succ_rate = generate_dataset(df, graph, edges, ave_dist, KMean.cluster_centers_ ,sensitive_param, priviliged_group, unpriviliged_group)\n",
    "                    if succ_rate!=0.0:\n",
    "                        A1 = final_df[sensitive_name].to_numpy()\n",
    "                        X1 = final_df.to_numpy()[:,:-1]\n",
    "                        Y1 = final_df.to_numpy()[:,-1].astype(int)\n",
    "\n",
    "\n",
    "                        HP_importance_pert, df_HP_pert ,shap_values= HP_search_population(X1, Y1, A1, X_shap_org, HP_population, program_name, num_iteration, \n",
    "                                   sensitive_param, unpriviliged_group, priviliged_group, sensitive_name, \n",
    "                                   start_time)\n",
    "\n",
    "                        final_shap_list.append(shap_values)\n",
    "                        HP_importance_results.append([dataset,Algorithm, file_num, weights_ind] + HP_importance_pert.to_list())\n",
    "                    else:\n",
    "                        print('Zero succ')\n",
    "\n",
    "\n",
    "        np.save('../'+dataset+'_Analysis/RQ3/'+ dataset + '_RQ3.npy', HP_importance_results)\n",
    "        np.save('../'+dataset+'_Analysis/RQ3/'+ dataset + '_RQ3_shap_values.npy', final_shap_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dce894-1e3f-4fb8-b4de-dc6f192d24b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
