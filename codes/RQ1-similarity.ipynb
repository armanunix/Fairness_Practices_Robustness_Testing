{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c555ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.append(\"./subjects/\")\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random, math\n",
    "import tensorflow_probability as tfb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import  euclidean_distances\n",
    "import glob\n",
    "import re\n",
    "from sklearn.neighbors import KDTree\n",
    "from Utils_Functions import generate_dataset, eod\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdmetrics.single_table import LogisticDetection, SVCDetection\n",
    "import pandas as pd\n",
    "from sdv.sampling import Condition\n",
    "\n",
    "import numpy as np\n",
    "from sdmetrics.column_pairs import DiscreteKLDivergence as metric\n",
    "from sdmetrics.single_table import BinaryAdaBoostClassifier, BinaryDecisionTreeClassifier, BinaryLogisticRegression, BinaryMLPClassifier    \n",
    "\n",
    "import numpy\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy.random import random\n",
    "from scipy.linalg import sqrtm\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,normalize\n",
    "\n",
    "def generate_dataset(data, graph, edges, sens_index, priv_group, unpriv_group):\n",
    "    #global  edge,df_new_dic\n",
    "    dataset_types = [str(data[i].dtype) for i in data.columns]\n",
    "    succ_generated = 0\n",
    "    generation_coef = 10\n",
    "    \n",
    "    graph_dic ={}\n",
    "    for i in graph.columns[1:]:\n",
    "        if np.where(graph[i])[0].shape[0]==0:\n",
    "            graph_dic[i]=None\n",
    "        else:\n",
    "            graph_dic[i]= graph['Unnamed: 0'][np.where(graph[i])[0]].values\n",
    "\n",
    "    final_df = pd.DataFrame(columns = data.columns) \n",
    "    trial = 0\n",
    "    not_interesting = False\n",
    "    while final_df.shape[0]<data.shape[0]:\n",
    "\n",
    "        if trial > 20:\n",
    "            not_interesting = True\n",
    "            return None #succ_generated/( trial *  data.shape[0]*generation_coef) \n",
    "        df_new_dic ={}\n",
    "        for edge in graph.sum().index[np.where(graph.sum()==0)[0]]:\n",
    "            df_new_dic[edge] = np.random.choice(np.unique(data[edge]), size = data.shape[0]*generation_coef) \n",
    "\n",
    "        statring_atts = graph.sum().index[np.where(graph.sum()==0)[0]]\n",
    "        \n",
    "        while statring_atts.shape[0] != graph['Unnamed: 0'].shape[0]:\n",
    "            \n",
    "            for att in graph_dic.keys():\n",
    "                if att not in statring_atts:\n",
    "                    if 0 in  [1 if graph_dic[att][i] in statring_atts else 0 for i in range(graph_dic[att].shape[0])]:\n",
    "\n",
    "                        continue\n",
    "                    else:\n",
    "                        edge_logits = 0\n",
    "                        \n",
    "                        for cause in graph_dic[att]:\n",
    "                            edge_logits += (edges[cause+att] * df_new_dic[cause])\n",
    "                        if np.unique(data[att]).shape[0]==2:\n",
    "                            df_new_dic[att] =  tfb.distributions.Bernoulli(logits=edge_logits + edges[att+'0'] ).sample().numpy()\n",
    "                        elif 'float' in dataset_types[np.where(data.columns==att)[0][0]]:\n",
    "\n",
    "                            df_new_dic[att] =  tfb.distributions.Normal(loc=(edge_logits+ edges[att+'0']), scale= edges['sigma_h']).sample().numpy()\n",
    "                        \n",
    "                        else:    \n",
    "                            df_new_dic[att] =  tfb.distributions.Poisson(rate=tf.exp(edge_logits+ edges[att+'0']) ).sample().numpy()  \n",
    "\n",
    "                        statring_atts = np.append(statring_atts,att) \n",
    "\n",
    "         \n",
    "        new_df = pd.DataFrame(columns = data.columns)\n",
    "        for col in new_df.columns:\n",
    "            new_df[col] = df_new_dic[col].astype(str(data[col].dtype))\n",
    "        \n",
    "\n",
    "        final_df = pd.concat([final_df,new_df]).reset_index(drop=True)\n",
    "        final_df = final_df.drop_duplicates()\n",
    "\n",
    "        #print(succ_generated,trial)\n",
    "        trial += 1\n",
    "\n",
    "    \n",
    "    min_input ={}\n",
    "    max_input= {}\n",
    "    for col in data.columns:\n",
    "        min_input[col] = data[col].min()\n",
    "        max_input[col] = data[col].max()\n",
    "    for col in final_df.columns:\n",
    "        final_df[col] = final_df[col].apply(lambda x: x if x >=min_input[col] and x <=max_input[col] else None)\n",
    "\n",
    "    final_df.dropna(inplace=True)\n",
    "    final_df.reset_index(drop=True, inplace=True)\n",
    "    for col in final_df.columns:\n",
    "        final_df[col] = final_df[col].astype(str(data[col].dtype))\n",
    "#     probs={}\n",
    "#     for val in data[data.columns[sens_index]].unique():\n",
    "#         probs[val] = [data[(data[data.columns[sens_index]]==val) & (data['y']==1)].shape[0], data[(data[data.columns[sens_index]]==val) & (data['y']==0)].shape[0]]\n",
    "#     #print(probs)\n",
    "#     #for val in df['age'].unique():\n",
    "\n",
    "#     syn_df = pd.DataFrame(columns = data.columns)\n",
    "#     for val in probs.keys():\n",
    "\n",
    "#         syn_df  = pd.concat([syn_df,final_df[(final_df[data.columns[sens_index]]==val) & (final_df['y']==1)].sample(n=probs[val][0])])\n",
    "#         syn_df  = pd.concat([syn_df,final_df[(final_df[data.columns[sens_index]]==val) & (final_df['y']==0)].sample(n=probs[val][1])])\n",
    "\n",
    "#     for col in syn_df.columns:\n",
    "#         syn_df[col] = syn_df[col].astype(str(data[col].dtype))\n",
    "    return final_df\n",
    "dataset ='Adult'\n",
    "\n",
    "if dataset == 'Adult':\n",
    "    sens_index = 7\n",
    "    priv_group = 1\n",
    "    unpriv_group = 0\n",
    "    data_file_name = 'adult_org-Copy1.csv'\n",
    "if dataset == 'Compas':\n",
    "    sens_index = 1\n",
    "    priv_group = 1\n",
    "    unpriv_group = 0\n",
    "    data_file_name = 'compas-Copy1'\n",
    "if dataset == 'Bank':\n",
    "    sens_index = 0\n",
    "    priv_group = 5\n",
    "    unpriv_group = 3\n",
    "    data_file_name = 'bank'\n",
    "if dataset == 'Heart':\n",
    "    sens_index = 0\n",
    "    priv_group = 1\n",
    "    unpriv_group = 0 \n",
    "    data_file_name = 'heart_processed_1'\n",
    "if dataset == 'Law':\n",
    "    sens_index = 1\n",
    "    priv_group = 1\n",
    "    unpriv_group = 0\n",
    "    data_file_name = 'law.csv'\n",
    "\n",
    "if dataset == 'Student':\n",
    "    sens_index = 0\n",
    "    priv_group = 1\n",
    "    unpriv_group = 0  \n",
    "    data_file_name = 'students-processed_2'\n",
    "df = pd.read_csv('./subjects/datasets/'+data_file_name)\n",
    "df = df.drop_duplicates()\n",
    "X1 = df.to_numpy()[:,:-1]\n",
    "Y1 = df.to_numpy()[:,-1]\n",
    "\n",
    "    \n",
    "for Algorithm in ['pc','ges','simy']:\n",
    "    print('Algorithm', Algorithm)\n",
    "    succ_rate_list =[]\n",
    "    for edge_list_filename in glob.glob('./'+dataset+'_Analysis/'+Algorithm+'/PP/*.csv'):\n",
    "        #print(edge_list_filename)\n",
    "        file_num = int(re.findall(r'\\d+', edge_list_filename)[0])\n",
    "        \n",
    "    #     if file_num in [5,7,10,12]:\n",
    "    #         continue\n",
    "\n",
    "        try:\n",
    "            graph_filename = './'+dataset+'_Analysis/'+Algorithm+'/DAGs/'+dataset+'_'+Algorithm+'_DAG_{file_num}.csv'.format(file_num=file_num)\n",
    "            graph = pd.read_csv(graph_filename)\n",
    "            \n",
    "\n",
    "            #edge_list_filename = './'+dataset+'_Analysis/'+Algorithm+'/PP/'+dataset+'_'+Algorithm+'_pp_{file_num}.csv'.format(file_num=file_num)\n",
    "            edges_list = pd.read_csv(edge_list_filename)\n",
    "\n",
    "            if dataset=='Bank' and Algorithm=='simy':\n",
    "                graph.columns = [i.replace('1','') for i in graph.columns]\n",
    "                graph[graph.columns[0]] = [i.replace('1','') for i in graph[graph.columns[0]]]\n",
    "                edges_list.columns = [i.replace('1','') for i in edges_list.columns]\n",
    "            edges_list = edges_list[edges_list.columns[1:-1]].mean()\n",
    "\n",
    "        except:\n",
    "            print('Not a DAG! ',file_num)\n",
    "            continue\n",
    "        \n",
    "            #ave_dist.append(euclidean_distances(X1[np.where(KMean.labels_==[i])],[KMean.cluster_centers_[i]]).max())\n",
    "        for i in range(1):\n",
    "\n",
    "            syn_df = generate_dataset(df, graph, edges_list,sens_index, priv_group, unpriv_group)\n",
    "            #final_df= final_df.astype('int64')\n",
    "            input(syn_df.shape[0])\n",
    "            \n",
    "            metadata = SingleTableMetadata()\n",
    "            metadata.detect_from_dataframe(df)\n",
    "            \n",
    "            rnd_df = pd.DataFrame(columns = df.columns)\n",
    "            for col in rnd_df.columns:\n",
    "                if 'float' in str(df[col].dtype):\n",
    "                    rnd_df[col]  = np.random.uniform(low=df[col].min(),high=df[col].max(),size=df.shape[0])\n",
    "\n",
    "                else:\n",
    "                    rnd_df[col]  = np.random.choice(np.unique(df[col]),size=df.shape[0])\n",
    "\n",
    "\n",
    "            CTGAN_kl = round(metric.compute(real_data=df, synthetic_data=syn_df),2)\n",
    "            CTGAN_LD  = round(LogisticDetection.compute(real_data=df,synthetic_data=syn_df,metadata=metadata),2)\n",
    "\n",
    "            # example of calculating the frechet inception distance\n",
    "\n",
    "\n",
    "            # calculate frechet inception distance\n",
    "            def calculate_fid(act1, act2):\n",
    "             # calculate mean and covariance statistics\n",
    "                \n",
    "                se = StandardScaler()\n",
    "                act1 = se.fit_transform(act1.to_numpy())\n",
    "                act1 = normalize(act1)\n",
    "                act2 = se.fit_transform(act2.to_numpy())\n",
    "                act2 = normalize(act2)\n",
    "                mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    "                mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    "                # calculate sum squared difference between means\n",
    "                ssdiff = numpy.sum((mu1 - mu2)**2.0)\n",
    "                # calculate sqrt of product between cov\n",
    "                covmean = sqrtm(sigma1.dot(sigma2))\n",
    "                # check and correct imaginary numbers from sqrt\n",
    "                if iscomplexobj(covmean):\n",
    "                    covmean = covmean.real\n",
    "                # calculate score\n",
    "                fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "                return fid\n",
    "\n",
    "                # define two collections of activations\n",
    "\n",
    "            df_F1 = BinaryLogisticRegression.compute(test_data=df,train_data=df,target=df.columns[-1],metadata=metadata)\n",
    "            CTGAN_F1 = BinaryLogisticRegression.compute(test_data=df,train_data=syn_df,target=df.columns[-1],metadata=metadata)\n",
    "            CTGAN_F1_diff = round(df_F1 - CTGAN_F1,3)\n",
    "            CTGAN_fid = round(calculate_fid(df, syn_df),3)\n",
    "\n",
    "            #print('-------------------')\n",
    "            #print(dataset)\n",
    "            print(f'&FID {CTGAN_fid} & KL {CTGAN_kl}& LR_D {CTGAN_LD} &  F1_diff {CTGAN_F1_diff}&')\n",
    "\n",
    "            print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "045b2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioned =[]\n",
    "probs={}\n",
    "for val in df[df.columns[sens_index]].unique():\n",
    "    probs[val] = [df[(df[df.columns[sens_index]]==val) & (df[df.columns[-1]]==1)].shape[0], df[(df[df.columns[sens_index]]==val) & (df[df.columns[-1]]==0)].shape[0]]\n",
    "#print(probs)\n",
    "#for val in df['age'].unique():\n",
    "\n",
    "syn_df = pd.DataFrame(columns = df.columns)\n",
    "for val in probs.keys():\n",
    "    syn_df  = pd.concat([syn_df,final_df[(final_df[df.columns[sens_index]]==val) & (final_df[df.columns[-1]]==1)].sample(n=probs[val][0])])\n",
    "    syn_df  = pd.concat([syn_df,final_df[(final_df[df.columns[sens_index]]==val) & (final_df[df.columns[-1]]==0)].sample(n=probs[val][1])])\n",
    "\n",
    "for col in syn_df.columns:\n",
    "    syn_df[col] = syn_df[col].astype(str(df[col].dtype))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
