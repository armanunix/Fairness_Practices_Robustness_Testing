{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85 0.92\n",
      "\n",
      "0.84 0.91\n",
      "\n",
      "0.84 0.92\n",
      "\n",
      "0.85 0.92\n",
      "\n",
      "0.85 0.92\n",
      "\n",
      "0.85 0.92\n",
      "\n",
      "0.84 0.91\n",
      "\n",
      "0.84 0.91\n",
      "\n",
      "Bal 0.8508455780426948\n",
      "0.85 0.92\n",
      "\n",
      "0.83 0.91\n",
      "\n",
      "0.84 0.91\n",
      "\n",
      "0.85 0.92\n",
      "\n",
      "0.84 0.91\n",
      "\n",
      "0.84 0.91\n",
      "\n",
      "0.84 0.91\n",
      "\n",
      "0.85 0.92\n",
      "\n",
      "0.84 0.91\n",
      "\n",
      "0.84 0.92\n",
      "\n",
      "0.84 0.92\n",
      "\n",
      "0.84 0.91\n",
      "\n",
      "0.84 0.91\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 143\u001b[0m\n\u001b[1;32m    140\u001b[0m f1_temp \u001b[38;5;241m=\u001b[39m f1_score(y_test,preds)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mround\u001b[39m(acc_temp,\u001b[38;5;241m2\u001b[39m),\u001b[38;5;28mround\u001b[39m(f1_temp,\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mround\u001b[39m(acc_temp,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.9\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m  \u001b[38;5;28mround\u001b[39m(f1_temp,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.80\u001b[39m :\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m#if True:\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction balance = \u001b[39m\u001b[38;5;124m'\u001b[39m,preds\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(preds), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest balance = \u001b[39m\u001b[38;5;124m'\u001b[39m, y_test\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(y_test) )\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/ipykernel/kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/ipykernel/kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"./subjects/\")\n",
    "import time\n",
    "import random\n",
    "import tensorflow_probability as tfb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from adf_utils.config import compas\n",
    "from adf_data.compas import compas_data\n",
    "le = LabelEncoder()\n",
    "data = {'compas':compas_data}\n",
    "data_config = {'compas' :compas}\n",
    "\n",
    "\n",
    "dataset='compas'\n",
    "if dataset == \"redcar\":\n",
    "    sensitive_param =2\n",
    "elif  dataset == \"adult\":\n",
    "    sensitive_param =8\n",
    "elif  dataset == \"compas\":\n",
    "    sensitive_param =1\n",
    "\n",
    "if dataset == \"redcar\" and sensitive_param == 2:\n",
    "    sensitive_name = \"race\"\n",
    "    priviliged_group = [{sensitive_name: 1.0}]  #male\n",
    "    unpriviliged_group = [{sensitive_name: 0.0}]  #female\n",
    "    favorable_label  = 1.0\n",
    "    unfavorable_label = 0.0\n",
    "elif dataset == \"adult\" and sensitive_param == 8:\n",
    "    sensitive_name = \"gender\"\n",
    "    priviliged_group = [{sensitive_name: 1.0}]  #male\n",
    "    unpriviliged_group = [{sensitive_name: 0.0}]  #female\n",
    "    favorable_label  = 1.0\n",
    "    unfavorable_label = 0.0  \n",
    "elif dataset == \"compas\" and sensitive_param == 1:\n",
    "    sensitive_name = \"race\"\n",
    "    priviliged_group = [{sensitive_name: 1.0}]  #Caucasian\n",
    "    unpriviliged_group = [{sensitive_name: 0.0}]  #others\n",
    "    favorable_label  = 1.0\n",
    "    unfavorable_label = 0.0  \n",
    "X, Y, input_shape, nb_classes = data[dataset]()\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df.columns = data_config[dataset]().feature_name\n",
    "A = df[sensitive_name].to_numpy()\n",
    "X = df.to_numpy()\n",
    "\n",
    "df['label'] = Y\n",
    "\n",
    "X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(X, Y, A, random_state=0,stratify= Y)\n",
    "           \n",
    "len_df = df.shape[0]\n",
    "df_out=pd.read_csv('./Results/LogisticRegression/compas/race/compas_coef_NI_1000.csv')\n",
    "coef_list = ['ry', 'rd', 'rp', 'rj','sy', 'sd', 'sp', 'sj', 'ay', 'ad', 'ap', 'aj', \n",
    "             'jp', 'jd', 'jy', 'pd','dy']\n",
    "\n",
    "coef_mean = df_out[df_out.columns[8:]].mean()\n",
    "\n",
    "# step = -0.001\n",
    "# for i in range(500):\n",
    "#     direction = np.random.choice([2,-2],size=len(coef_list))\n",
    "edges = {}\n",
    "for i in range(len(coef_mean)):\n",
    "    #print(coef_mean[i])\n",
    "    edges[coef_list[i]] = coef_mean[i]# +  direction[i] * step\n",
    "#         #print(edges[coef_list[i]])\n",
    "\n",
    "#     #print('zie',edges['z1e'])\n",
    "remove_edge =None #'jy'\n",
    "\n",
    "num_samples =len_df\n",
    "\n",
    "\n",
    "models =[]\n",
    "dfs =[]\n",
    "acc =[]\n",
    "f1  =[]\n",
    "test_bal =[]\n",
    "pred_bal =[]\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    r_prob = df['race'].sum()/df.shape[0]\n",
    "    r = tfb.distributions.Bernoulli(r_prob).sample(num_samples).numpy()\n",
    "\n",
    "    s_prob = df['sex'].sum()/df.shape[0]\n",
    "    s = tfb.distributions.Bernoulli(s_prob).sample(num_samples).numpy()\n",
    "\n",
    "    a_pop = df['age_cat'].unique()\n",
    "    a_prob=[]\n",
    "    for age in a_pop:\n",
    "        a_prob.append(np.where(df['age_cat']==age)[0].shape[0]/df.shape[0])\n",
    "    a=np.array(random.choices(a_pop, weights=a_prob, k=num_samples))\n",
    "\n",
    "#     j[i] ~ poisson(exp((rj * r[i]) + (sj * s[i]) + (aj * a[i])) ); \n",
    "    j_rate = tf.exp((edges['rj'] * r) + (edges['sj'] * s) + (edges['aj'] * a) )\n",
    "    j =  tfb.distributions.Poisson(rate=j_rate).sample().numpy() \n",
    "    \n",
    "#     p[i] ~ poisson(exp((rp * r[i]) + (sp * s[i]) + (ap * a[i]) + (jp * j[i])));\n",
    "    p_rate = tf.exp((edges['rp'] * r) + (edges['sp'] * s) + (edges['ap'] * a) + (edges['jp'] * j))\n",
    "    p =  tfb.distributions.Poisson(rate=p_rate).sample().numpy() \n",
    "    \n",
    "#    d[i] ~ poisson(exp( (rd * r[i]) + (sd * s[i]) + (ad * a[i]) + (pd * p[i]) + (jd * j[i]) )); \n",
    "    d_rate = tf.exp((edges['rd'] * r) + (edges['sd'] * s) + (edges['ad'] * a) + (edges['jd'] * j) + (edges['pd'] * p) )\n",
    "    d =  tfb.distributions.Poisson(rate=d_rate).sample().numpy() \n",
    "    \n",
    "#    y[i] ~ bernoulli_logit( (ry * r[i]) + (sy * s[i]) + (ay * a[i]) + (dy * d[i]) + (jy * j[i]) );\n",
    "\n",
    "\n",
    "    y_logits = (edges['ry'] * r) + (edges['sy'] * s) + (edges['ay'] * a) + (edges['dy'] * d) + (edges['jy'] * j)\n",
    "    y = tfb.distributions.Bernoulli(logits = y_logits).sample().numpy()\n",
    "    \n",
    "    if y.sum()/len(y)<0.2 or y.sum()/len(y)>0.85:\n",
    "        print('Bal', y.sum()/len(y))\n",
    "        continue\n",
    "    new_df = pd.DataFrame(columns = df.columns)\n",
    "    new_df['sex'] = s\n",
    "    new_df['race'] = r\n",
    "    new_df['age_cat'] = a\n",
    "    new_df['priors_count'] = p\n",
    "    new_df['juv_fel_count'] = j\n",
    "    new_df['r_charge_degree'] = d\n",
    "    new_df['label'] = y\n",
    "    if new_df.isna().sum().sum()!=0 or np.isinf(new_df).sum().sum()!=0:\n",
    "        print('NA')\n",
    "        continue\n",
    "\n",
    "    X = new_df.to_numpy()[:,:-1]\n",
    "    Y = new_df.to_numpy()[:,-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y,stratify= Y)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    acc_temp = accuracy_score(y_test,preds)\n",
    "    f1_temp = f1_score(y_test,preds)\n",
    "\n",
    "    print(round(acc_temp,2),round(f1_temp,2))\n",
    "    input()\n",
    "    if round(acc_temp,2) >0.9 and  round(f1_temp,2)>0.80 :\n",
    "    #if True:\n",
    "        print('Prediction balance = ',preds.sum()/len(preds), 'test balance = ', y_test.sum()/len(y_test) )\n",
    "\n",
    "        print(i,acc_temp, f1_temp)\n",
    "\n",
    "        #print(model.score(X_test,y_test), accuracy_score(y_test,preds))\n",
    "        f1.append(round(f1_temp,2))\n",
    "        models.append(model)\n",
    "        dfs.append(new_df.copy())\n",
    "\n",
    "        test_bal.append(y_test.sum()/len(y_test))\n",
    "        pred_bal.append(preds.sum()/len(preds))\n",
    "\n",
    "\n",
    "# df_out_2 = pd.DataFrame([acc,f1,test_bal,pred_bal]).T\n",
    "#dfs[0].to_csv('./subjects/datasets/adult_pp.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnew_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_df' is not defined"
     ]
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"./Fairness-libraries-testing/\")\n",
    "import time\n",
    "import random\n",
    "import tensorflow_probability as tfb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "le = LabelEncoder()\n",
    "df = pd.read_csv('../subjects/datasets/adult_pp.csv')\n",
    "\n",
    "models =[]\n",
    "dfs =[]\n",
    "acc =[]\n",
    "f1  =[]\n",
    "test_bal =[]\n",
    "pred_bal =[]\n",
    "\n",
    "for col in [ 'workclass', 'educational-num', 'marital-status', 'occupation',\n",
    "       'relationship',  'hours-per-week', 'native-country',\n",
    "       'label']:\n",
    "    print(col)\n",
    "    for i in range(10):\n",
    "        df[col] = np.random.choice(df[col],size = df.shape[0])\n",
    "\n",
    "\n",
    "        X = df.to_numpy()[:,:-1]\n",
    "        Y = df.to_numpy()[:,-1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y,stratify= Y)\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train,y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        acc_temp = accuracy_score(y_test,preds)\n",
    "        f1_temp = f1_score(y_test,preds)\n",
    "        print(i,acc_temp, f1_temp)\n",
    "        if round(acc_temp,2) >=0.9 and  round(f1_temp,2)>0.85 :\n",
    "        #if True:\n",
    "            print('Prediction balance = ',preds.sum()/len(preds), 'test balance = ', y_test.sum()/len(y_test) )\n",
    "\n",
    "            print(i,acc_temp, f1_temp)\n",
    "\n",
    "            #print(model.score(X_test,y_test), accuracy_score(y_test,preds))\n",
    "            f1.append(round(f1_temp,2))\n",
    "            models.append(model)\n",
    "            dfs.append(new_df.copy())\n",
    "\n",
    "            test_bal.append(y_test.sum()/len(y_test))\n",
    "            pred_bal.append(preds.sum()/len(preds))\n",
    "            input()\n",
    "\n",
    "# df_out_2 = pd.DataFrame([acc,f1,test_bal,pred_bal]).T\n",
    "#dfs[0].to_csv('./subjects/datasets/adult_pp.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 161\u001b[0m\n\u001b[1;32m    159\u001b[0m e_rate \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexp(e0 \u001b[38;5;241m+\u001b[39m (z1e \u001b[38;5;241m*\u001b[39m z1) \u001b[38;5;241m+\u001b[39m (z2e \u001b[38;5;241m*\u001b[39m z2) \u001b[38;5;241m+\u001b[39m (ne \u001b[38;5;241m*\u001b[39m n) \u001b[38;5;241m+\u001b[39m (xe \u001b[38;5;241m*\u001b[39m x))\n\u001b[1;32m    160\u001b[0m e \u001b[38;5;241m=\u001b[39m  tfb\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mPoisson(rate\u001b[38;5;241m=\u001b[39me_rate)\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mnumpy() \n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m h_mean \u001b[38;5;241m=\u001b[39m h0 \u001b[38;5;241m+\u001b[39m (eh \u001b[38;5;241m*\u001b[39m e) \u001b[38;5;241m+\u001b[39m (z1h \u001b[38;5;241m*\u001b[39m z1) \u001b[38;5;241m+\u001b[39m (z2h \u001b[38;5;241m*\u001b[39m z2) \u001b[38;5;241m+\u001b[39m (nh \u001b[38;5;241m*\u001b[39m n) \u001b[38;5;241m+\u001b[39m (xh \u001b[38;5;241m*\u001b[39m x)\n\u001b[1;32m    163\u001b[0m h \u001b[38;5;241m=\u001b[39m tfb\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(loc\u001b[38;5;241m=\u001b[39mh_mean, scale\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/ipykernel/kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/ipykernel/kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# age, sex, education level, marital status, work-class and number\n",
    "# of working hours per week.\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import sys\n",
    "sys.path.append(\"./Fairness-libraries-testing/\")\n",
    "import time\n",
    "import random\n",
    "import tensorflow_probability as tfb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "# np.random.seed(0)\n",
    "# tf.random.set_seed(0)\n",
    "le = LabelEncoder()\n",
    "df = pd.read_csv('../subjects/datasets/adult_pp.csv')\n",
    "len_df = df.shape[0]\n",
    "num_samples = df.shape[0]\n",
    "z2 = df['age']\n",
    "x = df['gender']\n",
    "z1 = df['race']\n",
    "n = df['native-country']\n",
    "\n",
    "models =[]\n",
    "dfs =[]\n",
    "acc =[]\n",
    "f1  =[]\n",
    "test_bal =[]\n",
    "pred_bal =[]\n",
    "\n",
    "acc_max = 0\n",
    "for i in range(100000):\n",
    "    z1e = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    z2e = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    ne = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    xe = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    e0 = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    \n",
    "    eh = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    z1h = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    z2h = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    nh = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    xh = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    h0 = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    \n",
    "    ew = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    z2w = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    nw = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    hw = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    w0 = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    \n",
    "    \n",
    "    z1m = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    z2m = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    nm = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    xm = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    hm = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    wm = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    m0 = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    \n",
    "    z1o = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    z2o = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    eo = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    xo = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    mo = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    wo = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    o0 = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    \n",
    "    z2r = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    er = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    xr = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    mr = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    nr = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    r0 = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    \n",
    "    z1y = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    z2y = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    ny = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    ey = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    hy = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    wy = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    my = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    oy = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    ry = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    y0 = tfb.distributions.Normal(loc=0, scale=10).sample().numpy()\n",
    "    \n",
    "#     z1e = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     z2e = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     ne = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     xe = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "    \n",
    "#     eh = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     z1h = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     z2h = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     nh = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     xh = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "    \n",
    "#     ew = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     z2w =tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     nw =tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     hw = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "    \n",
    "    \n",
    "#     z1m = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     z2m = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     nm = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     xm =tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     hm = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     wm = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "    \n",
    "#     z1o = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     z2o = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     eo = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     xo = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     mo =tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     wo = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "    \n",
    "#     z2r = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     er = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     xr = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     mr =tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     nr = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "    \n",
    "#     z1y = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     z2y = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     ny = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     ey = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     hy = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     wy = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     my = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     oy = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "#     ry = tfp.distributions.Uniform(0,10).sample().numpy()\n",
    "    \n",
    "    x_prob = df['gender'].sum()/df.shape[0]\n",
    "    x = tfb.distributions.Bernoulli(x_prob).sample(num_samples).numpy()\n",
    "\n",
    "    z1_pop = df['race'].unique()\n",
    "    z1_prob=[]\n",
    "    for race in z1_pop:\n",
    "        z1_prob.append(np.where(df['race']==race)[0].shape[0]/df.shape[0])\n",
    "    z1=np.array(random.choices(z1_pop, weights=z1_prob, k=num_samples))\n",
    "\n",
    "    z2_pop = df['age'].unique()\n",
    "    z2_prob=[]\n",
    "    for age in z2_pop:\n",
    "        z2_prob.append(np.where(df['age']==age)[0].shape[0]/df.shape[0])\n",
    "    z2=np.array(random.choices(z2_pop, weights=z2_prob, k=num_samples))\n",
    "\n",
    "    n_pop = df['native-country'].unique()\n",
    "    n_prob=[]\n",
    "    for nc in n_pop:\n",
    "        n_prob.append(np.where(df['native-country']==nc)[0].shape[0]/df.shape[0])\n",
    "\n",
    "    n =np.array(random.choices(n_pop, weights=n_prob, k=num_samples))\n",
    "\n",
    "    e_rate = tf.exp(e0 + (z1e * z1) + (z2e * z2) + (ne * n) + (xe * x))\n",
    "    e =  tfb.distributions.Poisson(rate=e_rate).sample().numpy() \n",
    "    input()\n",
    "    h_mean = h0 + (eh * e) + (z1h * z1) + (z2h * z2) + (nh * n) + (xh * x)\n",
    "    h = tfb.distributions.Normal(loc=h_mean, scale= 1).sample().numpy()\n",
    "    \n",
    "    \n",
    "    w_rate = tf.exp(w0 + (ew * e) + (z2w * z2) + (nw * n) + (hw * h))\n",
    "    w =  tfb.distributions.Poisson(rate=w_rate).sample().numpy()\n",
    "    \n",
    "\n",
    "    m_rate = tf.exp(m0 + (z1m * z1) + (z2m * z2) + (nm * n) + (xm * x) + (hm * h) + (wm * w))\n",
    "    m =  tfb.distributions.Poisson(rate=m_rate).sample().numpy()\n",
    "    \n",
    "    o_rate = tf.exp(o0 + (z1o * z1) + (z2o * z2) + (eo * e) + (xo * x) + (mo * m) + (wo * w))\n",
    "    o =  tfb.distributions.Poisson(rate=o_rate).sample().numpy()\n",
    "    \n",
    "    r_rate = tf.exp(r0 + (z2r * z2) + (er * e) + (xr * x) + (mr * m) + (nr * n))\n",
    "    r =  tfb.distributions.Poisson(rate=r_rate).sample().numpy()\n",
    "\n",
    "    y_logits =y0 + (z1y * z1) + (z2y * z2) + (ny * n) + (ey * e) + (hy * h) + (wy * w) + (my * m) + (oy * o) + (ry * r)\n",
    "    y = tfb.distributions.Bernoulli(logits = y_logits).sample().numpy()\n",
    "    if y.sum()/len_df<0.2 or y.sum()/len_df>0.8:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    new_df = pd.DataFrame(columns = df.columns)\n",
    "    new_df['age'] = z2\n",
    "    new_df['workclass'] = w\n",
    "    new_df['educational-num'] = e\n",
    "    new_df['marital-status'] = m\n",
    "    new_df['occupation'] = o\n",
    "    new_df['relationship'] = r\n",
    "    new_df['race'] = z1\n",
    "    new_df['gender'] = x\n",
    "    new_df['hours-per-week'] = h\n",
    "    new_df['native-country'] = n\n",
    "    new_df['label'] = y\n",
    "    if new_df.isna().sum().sum()!=0 or np.isinf(new_df).sum().sum()!=0:\n",
    "        #print('NA')\n",
    "        continue\n",
    "    X = new_df.to_numpy()[:,:-1]\n",
    "    Y = new_df.to_numpy()[:,-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y,stratify= Y)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    acc_temp = accuracy_score(y_test,preds)\n",
    "    f1_temp = f1_score(y_test,preds)\n",
    "    if acc_temp > acc_max:\n",
    "        acc_max = acc_temp\n",
    "        print('Accuracy ', acc_temp,'  F1', f1_temp)\n",
    "    print('ACC , F1',acc_temp, f1_temp ,preds.sum()/preds.shape[0] )\n",
    "    if round(acc_temp,2) >0.89 and round(acc_temp,2) <0.94 and  round(f1_temp,2)>0.85  and round(f1_temp,2)<0.94 :\n",
    "    #if True:\n",
    "        print('Prediction balance = ',preds.sum()/len(preds), 'test balance = ', y_test.sum()/len(y_test) )\n",
    "\n",
    "        print(i,acc_temp, f1_temp)\n",
    "\n",
    "        #print(model.score(X_test,y_test), accuracy_score(y_test,preds))\n",
    "        f1.append(round(f1_temp,2))\n",
    "        models.append(model)\n",
    "        dfs.append(new_df.copy())\n",
    "\n",
    "        test_bal.append(y_test.sum()/len(y_test))\n",
    "        pred_bal.append(preds.sum()/len(preds))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmonjezi/.virtualenvs/RISC_LAB/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"./subjects/\")\n",
    "import time\n",
    "import random\n",
    "import tensorflow_probability as tfb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from adf_utils.config import census\n",
    "from adf_data.census import census_data\n",
    "X, Y, input_shape, nb_classes = census_data()\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df.columns = census().feature_name\n",
    "Y = np.argmax(Y,axis=1)\n",
    "df['label']= Y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,stratify= Y)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "preds = model.predict(X_test)\n",
    "acc_temp = accuracy_score(y_test,preds)\n",
    "f1_temp = f1_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8080088441223436, 0.4836471754212091)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.02202860e+00,  4.11660348e+00,  3.58804674e+00,\n",
       "         2.36702243e+00, -1.37195659e+00,  3.29604478e+00,\n",
       "        -2.13604152e+00,  5.85466355e+00,  8.92534306e+00,\n",
       "        -2.17482065e+00],\n",
       "       [ 2.20656076e+00,  5.50616718e+00,  1.66794070e+00,\n",
       "         5.95360909e-01, -2.57545698e+00,  1.49786432e+00,\n",
       "        -2.95452597e+00,  9.78623541e+00,  8.76360303e+00,\n",
       "        -2.76924393e+00],\n",
       "       [ 1.04278730e+00,  4.60625923e+00,  1.42094543e+00,\n",
       "         5.34922494e-01, -2.19936446e+00,  2.55832910e+00,\n",
       "        -2.06140206e+00,  6.10917741e+00,  9.45068135e+00,\n",
       "        -2.73295056e+00],\n",
       "       [ 2.47034915e+00,  4.09862906e+00,  2.36833522e+00,\n",
       "         4.35679206e-02, -4.07989383e+00,  3.57150086e+00,\n",
       "        -3.83819576e-01,  7.09329500e+00,  1.15430098e+01,\n",
       "        -3.78553530e+00],\n",
       "       [ 5.84500078e+00,  2.36376889e+00,  1.48780331e+00,\n",
       "         8.91392213e+00, -6.69612814e+00, -9.60517307e+00,\n",
       "        -1.08661170e+01,  7.62179362e+00,  4.39001161e+00,\n",
       "         9.34386415e+00],\n",
       "       [ 4.20430241e+00,  1.04068065e+00,  4.53592857e-01,\n",
       "         8.56387816e+00, -7.85018827e+00, -8.12843110e+00,\n",
       "        -8.45623137e+00,  5.41757109e+00,  5.96547666e+00,\n",
       "         6.71543287e+00],\n",
       "       [ 7.01328033e+00,  3.97973559e-01,  2.90138600e-01,\n",
       "         9.56638449e+00, -8.98245578e+00, -7.03496894e+00,\n",
       "        -9.38735707e+00,  7.62903595e+00,  5.91950142e+00,\n",
       "         8.10681613e+00],\n",
       "       [ 5.42088178e+00, -1.69556416e-01,  3.28383325e+00,\n",
       "         9.99244756e+00, -6.71171988e+00, -7.35136935e+00,\n",
       "        -1.04568577e+01,  8.56246186e+00,  5.29513165e+00,\n",
       "         8.20269936e+00],\n",
       "       [-2.76525282e-01,  5.08127768e+00,  4.41369674e-01,\n",
       "         6.84923380e-01, -2.42237057e+00,  3.30478476e+00,\n",
       "        -1.75906091e+00,  6.65482783e+00,  9.24507298e+00,\n",
       "        -1.90283775e+00],\n",
       "       [ 4.96370361e+00, -9.51269706e-04,  1.04933869e+00,\n",
       "         8.56809811e+00, -9.74442868e+00, -7.35658752e+00,\n",
       "        -9.12996961e+00,  5.11615322e+00,  7.05138721e+00,\n",
       "         9.29613214e+00]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'adf_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01madf_data\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madult\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adult_data\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01madf_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adult\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'adf_data'"
     ]
    }
   ],
   "source": [
    "from adf_data.adult import adult_data\n",
    "from adf_utils.config import adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
